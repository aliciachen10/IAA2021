
<!-- saved from url=(0049)https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><div class="line-gutter-backdrop"></div><table><tbody><tr><td colspan="2" class="line-wrap-cell"><form autocomplete="off"><label class="line-wrap-control">Line wrap<input type="checkbox"></label></form></td></tr><tr><td class="line-number" value="1"></td><td class="line-content"><span class="html-comment">&lt;!DOCTYP html&gt;</span></td></tr><tr><td class="line-number" value="2"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="3"></td><td class="line-content"><span class="html-tag">&lt;html <span class="html-attribute-name">lang</span>="<span class="html-attribute-value">en</span>"&gt;</span></td></tr><tr><td class="line-number" value="4"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="5"></td><td class="line-content"><span class="html-tag">&lt;head&gt;</span></td></tr><tr><td class="line-number" value="6"></td><td class="line-content"><span class="html-tag">&lt;meta <span class="html-attribute-name">http-equiv</span>="<span class="html-attribute-value">X-UA-Compatible</span>" <span class="html-attribute-name">content</span>="<span class="html-attribute-value">IE=Edge</span>" /&gt;</span></td></tr><tr><td class="line-number" value="7"></td><td class="line-content"><span class="html-tag">&lt;meta <span class="html-attribute-name">charset</span>="<span class="html-attribute-value">UTF-8</span>" /&gt;</span></td></tr><tr><td class="line-number" value="8"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="9"></td><td class="line-content"><span class="html-comment">&lt;!-- JQuery package --&gt;</span></td></tr><tr><td class="line-number" value="10"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="11"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>" <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/css/redmond/jquery-ui-1.10.2.css" rel="noreferrer noopener">../../css/redmond/jquery-ui-1.10.2.css</a>" <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" /&gt;</span></td></tr><tr><td class="line-number" value="12"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/javascript</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/js/jquery-1.9.1.min.js" rel="noreferrer noopener">../../js/jquery-1.9.1.min.js</a>"&gt;</span><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="13"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/javascript</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/js/jquery-ui-1.10.2.min.js" rel="noreferrer noopener">../../js/jquery-ui-1.10.2.min.js</a>"&gt;</span><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="14"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="15"></td><td class="line-content"><span class="html-comment">&lt;!-- Style header for jQuery accordions --&gt;</span></td></tr><tr><td class="line-number" value="16"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="17"></td><td class="line-content"><span class="html-tag">&lt;style&gt;</span></td></tr><tr><td class="line-number" value="18"></td><td class="line-content">.ui-accordion-header {</td></tr><tr><td class="line-number" value="19"></td><td class="line-content">  font-family: 'Droid Sans', sans-serif;</td></tr><tr><td class="line-number" value="20"></td><td class="line-content">  font-style: normal;</td></tr><tr><td class="line-number" value="21"></td><td class="line-content">}</td></tr><tr><td class="line-number" value="22"></td><td class="line-content"><span class="html-tag">&lt;/style&gt;</span></td></tr><tr><td class="line-number" value="23"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="24"></td><td class="line-content"><span class="html-comment">&lt;!-- Google Code hyphenator --&gt;</span></td></tr><tr><td class="line-number" value="25"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="26"></td><td class="line-content"><span class="html-comment">&lt;!--</span></td></tr><tr><td class="line-number" value="27"></td><td class="line-content"><span class="html-comment">&lt;script type="text/javascript" src="../../js/hyphenate.js"&gt;&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="28"></td><td class="line-content"><span class="html-comment">--&gt;</span></td></tr><tr><td class="line-number" value="29"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="30"></td><td class="line-content"><span class="html-comment">&lt;!-- Google fonts stuff --&gt;</span></td></tr><tr><td class="line-number" value="31"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="32"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>"</span></td></tr><tr><td class="line-number" value="33"></td><td class="line-content">  <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://fonts.googleapis.com/css?family=Noto+Serif:400" rel="noreferrer noopener">https://fonts.googleapis.com/css?family=Noto+Serif:400</a>" /&gt;</td></tr><tr><td class="line-number" value="34"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>"</span></td></tr><tr><td class="line-number" value="35"></td><td class="line-content">  <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://fonts.googleapis.com/css?family=Noto+Serif:400italic" rel="noreferrer noopener">https://fonts.googleapis.com/css?family=Noto+Serif:400italic</a>" /&gt;</td></tr><tr><td class="line-number" value="36"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>"</span></td></tr><tr><td class="line-number" value="37"></td><td class="line-content">  <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://fonts.googleapis.com/css?family=Droid+Sans:400" rel="noreferrer noopener">https://fonts.googleapis.com/css?family=Droid+Sans:400</a>" /&gt;</td></tr><tr><td class="line-number" value="38"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>"</span></td></tr><tr><td class="line-number" value="39"></td><td class="line-content">  <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://fonts.googleapis.com/css?family=Droid+Sans:700" rel="noreferrer noopener">https://fonts.googleapis.com/css?family=Droid+Sans:700</a>" /&gt;</td></tr><tr><td class="line-number" value="40"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="41"></td><td class="line-content"><span class="html-comment">&lt;!-- MathJax math formatting --&gt;</span></td></tr><tr><td class="line-number" value="42"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="43"></td><td class="line-content"><span class="html-tag">&lt;script&gt;</span></td></tr><tr><td class="line-number" value="44"></td><td class="line-content">MathJax = {</td></tr><tr><td class="line-number" value="45"></td><td class="line-content">  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}</td></tr><tr><td class="line-number" value="46"></td><td class="line-content">};</td></tr><tr><td class="line-number" value="47"></td><td class="line-content"><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="48"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="49"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/x-mathjax-config</span>"&gt;</span></td></tr><tr><td class="line-number" value="50"></td><td class="line-content">  MathJax.Hub.Config({</td></tr><tr><td class="line-number" value="51"></td><td class="line-content">  CommonHTML: {</td></tr><tr><td class="line-number" value="52"></td><td class="line-content">  scale: 90</td></tr><tr><td class="line-number" value="53"></td><td class="line-content">  },</td></tr><tr><td class="line-number" value="54"></td><td class="line-content">  "HTML-CSS": {</td></tr><tr><td class="line-number" value="55"></td><td class="line-content">  scale: 90</td></tr><tr><td class="line-number" value="56"></td><td class="line-content">  },</td></tr><tr><td class="line-number" value="57"></td><td class="line-content">  NativeMML: {</td></tr><tr><td class="line-number" value="58"></td><td class="line-content">  scale: 90</td></tr><tr><td class="line-number" value="59"></td><td class="line-content">  },</td></tr><tr><td class="line-number" value="60"></td><td class="line-content">  SVG: {</td></tr><tr><td class="line-number" value="61"></td><td class="line-content">  scale: 90</td></tr><tr><td class="line-number" value="62"></td><td class="line-content">  },</td></tr><tr><td class="line-number" value="63"></td><td class="line-content">  PreviewHTML: {</td></tr><tr><td class="line-number" value="64"></td><td class="line-content">  scale: 90</td></tr><tr><td class="line-number" value="65"></td><td class="line-content">  }</td></tr><tr><td class="line-number" value="66"></td><td class="line-content">  });</td></tr><tr><td class="line-number" value="67"></td><td class="line-content"><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="68"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="69"></td><td class="line-content"><span class="html-tag">&lt;style&gt;</span></td></tr><tr><td class="line-number" value="70"></td><td class="line-content">  .frac {</td></tr><tr><td class="line-number" value="71"></td><td class="line-content">    font-size: 1.25em;</td></tr><tr><td class="line-number" value="72"></td><td class="line-content">    line-height: 0%;</td></tr><tr><td class="line-number" value="73"></td><td class="line-content">  }</td></tr><tr><td class="line-number" value="74"></td><td class="line-content"><span class="html-tag">&lt;/style&gt;</span></td></tr><tr><td class="line-number" value="75"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="76"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/javascript</span>" <span class="html-attribute-name">id</span>="<span class="html-attribute-value">MathJax-script</span>" <span class="html-attribute-name">async</span></span></td></tr><tr><td class="line-number" value="77"></td><td class="line-content">	<span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" rel="noreferrer noopener">https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js</a>"&gt;</td></tr><tr><td class="line-number" value="78"></td><td class="line-content"><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="79"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="80"></td><td class="line-content"><span class="html-comment">&lt;!-- Course page CSS and JS --&gt;</span></td></tr><tr><td class="line-number" value="81"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="82"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>" <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/css/course.css" rel="noreferrer noopener">../../css/course.css</a>" <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" /&gt;</span></td></tr><tr><td class="line-number" value="83"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/javascript</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/js/mod-date.js" rel="noreferrer noopener">../../js/mod-date.js</a>"&gt;</span><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="84"></td><td class="line-content"><span class="html-tag">&lt;script <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/javascript</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/js/msa-dl.js" rel="noreferrer noopener">./js/msa-dl.js</a>"&gt;</span><span class="html-tag">&lt;/script&gt;</span></td></tr><tr><td class="line-number" value="85"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="86"></td><td class="line-content"><span class="html-comment">&lt;!-- Overrides for standard course CSS --&gt;</span></td></tr><tr><td class="line-number" value="87"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="88"></td><td class="line-content"><span class="html-tag">&lt;link <span class="html-attribute-name">type</span>="<span class="html-attribute-value">text/css</span>" <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/msa-dl.css" rel="noreferrer noopener">msa-dl.css</a>" <span class="html-attribute-name">rel</span>="<span class="html-attribute-value">stylesheet</span>" /&gt;</span></td></tr><tr><td class="line-number" value="89"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="90"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="91"></td><td class="line-content"><span class="html-tag">&lt;title&gt;</span>Deep Learning: Institute for Advanced Analytics<span class="html-tag">&lt;/title&gt;</span></td></tr><tr><td class="line-number" value="92"></td><td class="line-content"><span class="html-tag">&lt;/head&gt;</span></td></tr><tr><td class="line-number" value="93"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="94"></td><td class="line-content"><span class="html-tag">&lt;body <span class="html-attribute-name">style</span>="<span class="html-attribute-value">background-color: white</span>" <span class="html-attribute-name">class</span>="<span class="html-attribute-value">hyphenate</span>"&gt;</span></td></tr><tr><td class="line-number" value="95"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="96"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value"><br></span></span></td></tr><tr><td class="line-number" value="97"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  position: relative;</span></span></td></tr><tr><td class="line-number" value="98"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  height: 390px;</span></span></td></tr><tr><td class="line-number" value="99"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  background-image: url( 'figs/DL-libraries.png' );</span></span></td></tr><tr><td class="line-number" value="100"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  background-repeat: no-repeat;</span></span></td></tr><tr><td class="line-number" value="101"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  background-position: center top;</span></span></td></tr><tr><td class="line-number" value="102"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value"></span>"&gt;</span></td></tr><tr><td class="line-number" value="103"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="104"></td><td class="line-content"><span class="html-comment">&lt;!-- NC State logo, upper-left corner --&gt;</span></td></tr><tr><td class="line-number" value="105"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="106"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value"><br></span></span></td></tr><tr><td class="line-number" value="107"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    position: absolute;</span></span></td></tr><tr><td class="line-number" value="108"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    align: right;</span></span></td></tr><tr><td class="line-number" value="109"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    top: 10px;</span></span></td></tr><tr><td class="line-number" value="110"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    left: 10px;</span></span></td></tr><tr><td class="line-number" value="111"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  </span>"&gt;</span></td></tr><tr><td class="line-number" value="112"></td><td class="line-content">    <span class="html-tag">&lt;a <span class="html-attribute-name">target</span>="<span class="html-attribute-value">_blank</span>" <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.ncsu.edu/" rel="noreferrer noopener">https://www.ncsu.edu</a>"&gt;</span></td></tr><tr><td class="line-number" value="113"></td><td class="line-content">      <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/nc-state-logo-blue.png" rel="noreferrer noopener">figs/nc-state-logo-blue.png</a>"</span></td></tr><tr><td class="line-number" value="114"></td><td class="line-content">       <span class="html-attribute-name">style</span>="<span class="html-attribute-value"><br></span></td></tr><tr><td class="line-number" value="115"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">         border-style: none;</span></span></td></tr><tr><td class="line-number" value="116"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">         -moz-box-shadow: 1px 1px 8px #646464;</span></span></td></tr><tr><td class="line-number" value="117"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">         -webkit-box-shadow: 1px 1px 8px #646464;</span></span></td></tr><tr><td class="line-number" value="118"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">         box-shadow: 1px 1px 8px #646464;</span></span></td></tr><tr><td class="line-number" value="119"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">       </span>" <span class="html-attribute-name">alt</span>="<span class="html-attribute-value">nc-state logo</span>"&gt;</span></td></tr><tr><td class="line-number" value="120"></td><td class="line-content">    <span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="121"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="122"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="123"></td><td class="line-content"><span class="html-comment">&lt;!-- DNNVA text, lower-left corner --&gt;</span></td></tr><tr><td class="line-number" value="124"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="125"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value"><br></span></span></td></tr><tr><td class="line-number" value="126"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    position: absolute;</span></span></td></tr><tr><td class="line-number" value="127"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    bottom: 10px;</span></span></td></tr><tr><td class="line-number" value="128"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    left: 10px;</span></span></td></tr><tr><td class="line-number" value="129"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    font-family: 'Trebuchet MS', Helvetica, sans-serif;</span></span></td></tr><tr><td class="line-number" value="130"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    font-size: 16pt;</span></span></td></tr><tr><td class="line-number" value="131"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    font-weight: normal;</span></span></td></tr><tr><td class="line-number" value="132"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">    color: #646464;</span></span></td></tr><tr><td class="line-number" value="133"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  </span>"&gt;</span></td></tr><tr><td class="line-number" value="134"></td><td class="line-content">    <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">line-height: 90%;</span>"&gt;</span>Deep Learning<span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="135"></td><td class="line-content">    <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">font-size: 10pt;</span>"&gt;</span></td></tr><tr><td class="line-number" value="136"></td><td class="line-content">      <span class="html-tag">&lt;a <span class="html-attribute-name">target</span>="<span class="html-attribute-value">_blank</span>" <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="http://www.csc.ncsu.edu/faculty/healey" rel="noreferrer noopener">http://www.csc.ncsu.edu/faculty/healey</a>"&gt;</span></td></tr><tr><td class="line-number" value="137"></td><td class="line-content">      <span class="html-tag">&lt;i&gt;</span>Christopher G. Healey<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="138"></td><td class="line-content">      <span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="139"></td><td class="line-content">    <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="140"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="141"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="142"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="143"></td><td class="line-content"><span class="html-comment">&lt;!-- Spacer after image of one "line" --&gt;</span></td></tr><tr><td class="line-number" value="144"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="145"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">height: 1em;</span>"&gt;</span><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="146"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="147"></td><td class="line-content"><span class="html-comment">&lt;!-- Navigation toolbar --&gt;</span></td></tr><tr><td class="line-number" value="148"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="149"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">id</span>="<span class="html-attribute-value">navWrap</span>"&gt;</span></td></tr><tr><td class="line-number" value="150"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">id</span>="<span class="html-attribute-value">nav</span>"&gt;</span></td></tr><tr><td class="line-number" value="151"></td><td class="line-content">    <span class="html-tag">&lt;ul <span class="html-attribute-name">id</span>="<span class="html-attribute-value">nav-list</span>"&gt;</span></td></tr><tr><td class="line-number" value="152"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-intro</span>"&gt;</span>Introduction</td></tr><tr><td class="line-number" value="153"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-setup</span>"&gt;</span>Setup</td></tr><tr><td class="line-number" value="154"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-dnn</span>"&gt;</span>DNNs</td></tr><tr><td class="line-number" value="155"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-backprop</span>"&gt;</span>Backpropegation</td></tr><tr><td class="line-number" value="156"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-pytorch</span>"&gt;</span>PyTorch</td></tr><tr><td class="line-number" value="157"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-wheat</span>"&gt;</span>Wheat Ex</td></tr><tr><td class="line-number" value="158"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-num</span>"&gt;</span>Images Ex</td></tr><tr><td class="line-number" value="159"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-fcn-img</span>"&gt;</span>CIFAR FCN Ex</td></tr><tr><td class="line-number" value="160"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-cnn</span>"&gt;</span>CIFAR CNN Ex</td></tr><tr><td class="line-number" value="161"></td><td class="line-content">      <span class="html-tag">&lt;li <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dl-rnn</span>"&gt;</span>RNNs/LSTMs</td></tr><tr><td class="line-number" value="162"></td><td class="line-content">    <span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="163"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="164"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="165"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">id</span>="<span class="html-attribute-value">nav-footer</span>"&gt;</span></td></tr><tr><td class="line-number" value="166"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="167"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="168"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="169"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="170"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">intro</span>"&gt;</span>Introduction<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="171"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="172"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="173"></td><td class="line-content">This module will introduce you to deep learning and deep neural</td></tr><tr><td class="line-number" value="174"></td><td class="line-content">networks using <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://pytorch.org/" rel="noreferrer noopener">https://pytorch.org</a>"&gt;</span>PyTorch<span class="html-tag">&lt;/a&gt;</span>, a</td></tr><tr><td class="line-number" value="175"></td><td class="line-content">Python-based open source deep learning package created by Facebook.</td></tr><tr><td class="line-number" value="176"></td><td class="line-content">We will use PyTorch to introduce both fully connected deep neural</td></tr><tr><td class="line-number" value="177"></td><td class="line-content">networks (FCNs) and convolutional deep neural networks (CNNs) for</td></tr><tr><td class="line-number" value="178"></td><td class="line-content">image classification tasks.</td></tr><tr><td class="line-number" value="179"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="180"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="181"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">setup</span>"&gt;</span>Setup<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="182"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="183"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="184"></td><td class="line-content">Before you can use PyTorch, you must download and install its</td></tr><tr><td class="line-number" value="185"></td><td class="line-content">relevant modules in Anaconda. To do this, open an Anaconda command</td></tr><tr><td class="line-number" value="186"></td><td class="line-content">prompt and issue the command</td></tr><tr><td class="line-number" value="187"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="188"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="189"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="190"></td><td class="line-content">conda install pytorch torchvision -c pytorch<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="191"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="192"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="193"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="194"></td><td class="line-content">You can review the</td></tr><tr><td class="line-number" value="195"></td><td class="line-content"> <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://pytorch.org/get-started/locally/#anaconda-1" rel="noreferrer noopener">https://pytorch.org/get-started/locally/#anaconda-1</a>"&gt;</span>instructions</td></tr><tr><td class="line-number" value="196"></td><td class="line-content">on the PyTorch web site<span class="html-tag">&lt;/a&gt;</span> for more information, as well as commands</td></tr><tr><td class="line-number" value="197"></td><td class="line-content">you can issue to verify the install has been performed correctly.</td></tr><tr><td class="line-number" value="198"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="199"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="200"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">dnn</span>"&gt;</span>Deep Neural Networks<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="201"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="202"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="203"></td><td class="line-content">Deep neural networks (DNNs) are a relatively new area of focused</td></tr><tr><td class="line-number" value="204"></td><td class="line-content">research, based off the foundations of artificial neural networks</td></tr><tr><td class="line-number" value="205"></td><td class="line-content">(ANNs). ANNs and DNNs fall within the broad area of supervised machine</td></tr><tr><td class="line-number" value="206"></td><td class="line-content">learning algorithms. The original idea for a <span class="html-tag">&lt;i&gt;</span>neural network<span class="html-tag">&lt;/i&gt;</span> was</td></tr><tr><td class="line-number" value="207"></td><td class="line-content"><span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf" rel="noreferrer noopener">https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf</a>"&gt;</span>proposed</td></tr><tr><td class="line-number" value="208"></td><td class="line-content">by McCulloch and Pitts in 1943<span class="html-tag">&lt;/a&gt;</span>, based off a biological estimate of</td></tr><tr><td class="line-number" value="209"></td><td class="line-content">how neurons in the brain were hypothesized to function. The problem</td></tr><tr><td class="line-number" value="210"></td><td class="line-content">with the McCulloch-Pitts model was its inability to easily learn. To</td></tr><tr><td class="line-number" value="211"></td><td class="line-content">address</td></tr><tr><td class="line-number" value="212"></td><td class="line-content">this, <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf" rel="noreferrer noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf</a>"&gt;</span>Rosenblatt</td></tr><tr><td class="line-number" value="213"></td><td class="line-content">proposed the Perceptron<span class="html-tag">&lt;/a&gt;</span>, adding weights that allowed the ANN to</td></tr><tr><td class="line-number" value="214"></td><td class="line-content">&amp;quot;learn&amp;quot; by increasing and decreasing weights based on how</td></tr><tr><td class="line-number" value="215"></td><td class="line-content">well the current network categorized relative to a known, correct</td></tr><tr><td class="line-number" value="216"></td><td class="line-content">label (<span class="html-tag">&lt;i&gt;</span>i.e.<span class="html-tag">&lt;/i&gt;</span>, supervised learning).</td></tr><tr><td class="line-number" value="217"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="218"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="219"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="220"></td><td class="line-content">In 1959, Widrow and Hoff developed MADALINE at Stanford University to</td></tr><tr><td class="line-number" value="221"></td><td class="line-content">remove echoes on phone lines. Interestingly, MADALINE is still in</td></tr><tr><td class="line-number" value="222"></td><td class="line-content">use. Careful analysis of MADALINE showed that it found a set of</td></tr><tr><td class="line-number" value="223"></td><td class="line-content">weights for a number of inputs, which is analogous to linear</td></tr><tr><td class="line-number" value="224"></td><td class="line-content">regression. In other words, ANNs at this point could not solve more</td></tr><tr><td class="line-number" value="225"></td><td class="line-content">complex, non-linear</td></tr><tr><td class="line-number" value="226"></td><td class="line-content">problems. <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://mitpress.mit.edu/books/perceptrons" rel="noreferrer noopener">https://mitpress.mit.edu/books/perceptrons</a>"&gt;</span>Marvin</td></tr><tr><td class="line-number" value="227"></td><td class="line-content">Minsky and Seymour Papert at MIT<span class="html-tag">&lt;/a&gt;</span> proved this theoretically,</td></tr><tr><td class="line-number" value="228"></td><td class="line-content">effectively silencing research on neural networks for many years.</td></tr><tr><td class="line-number" value="229"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="230"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="231"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="232"></td><td class="line-content">Moving into the 1980s, multilayer neural networks with <span class="html-tag">&lt;i&gt;</span>hidden</td></tr><tr><td class="line-number" value="233"></td><td class="line-content">layers<span class="html-tag">&lt;/i&gt;</span> were developed. This was critical, since it provides one of</td></tr><tr><td class="line-number" value="234"></td><td class="line-content">the key advantages of a DNN: the ability to automatically create</td></tr><tr><td class="line-number" value="235"></td><td class="line-content">relevant features from the initial inputs. This is also where the</td></tr><tr><td class="line-number" value="236"></td><td class="line-content">terminology &amp;quot;deep&amp;quot; comes from. The intuition is that each</td></tr><tr><td class="line-number" value="237"></td><td class="line-content">hidden layer in a DNN uses results from the previous layer, allowing</td></tr><tr><td class="line-number" value="238"></td><td class="line-content">it to start with highly detailed features, then use those to proceed</td></tr><tr><td class="line-number" value="239"></td><td class="line-content">to identify more abstract elements. This led to another problem,</td></tr><tr><td class="line-number" value="240"></td><td class="line-content">however. Although it was understood how to train single-layer ANNs, it</td></tr><tr><td class="line-number" value="241"></td><td class="line-content">was not know how to adjust weights, biases, and activations on a</td></tr><tr><td class="line-number" value="242"></td><td class="line-content">multi-layer DNN.</td></tr><tr><td class="line-number" value="243"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="244"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="245"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="246"></td><td class="line-content">To address this, the idea of backpropagation, which distributes error</td></tr><tr><td class="line-number" value="247"></td><td class="line-content">throughout the network,</td></tr><tr><td class="line-number" value="248"></td><td class="line-content">was <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf" rel="noreferrer noopener">https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf</a>"&gt;</span>proposed</td></tr><tr><td class="line-number" value="249"></td><td class="line-content">by Rumelhart, Hinton, and Williams<span class="html-tag">&lt;/a&gt;</span> in 1986. In simple terms, we</td></tr><tr><td class="line-number" value="250"></td><td class="line-content">are using calculus to assign some of the blame for error in the output</td></tr><tr><td class="line-number" value="251"></td><td class="line-content">layer to each neuron in the previous hidden layer, further propagating</td></tr><tr><td class="line-number" value="252"></td><td class="line-content">error at that hidden layer to its parent and so on. Initially,</td></tr><tr><td class="line-number" value="253"></td><td class="line-content">stochastic gradient descent was used to find an optimal set of weights</td></tr><tr><td class="line-number" value="254"></td><td class="line-content">to minimize error, although other approaches have now been</td></tr><tr><td class="line-number" value="255"></td><td class="line-content">proposed. Backpropagation is performed at the possible cost of</td></tr><tr><td class="line-number" value="256"></td><td class="line-content">significantly increased training times. Because of this, we now use</td></tr><tr><td class="line-number" value="257"></td><td class="line-content">clusters of graphics processing units (GPUs) which support 1000s of</td></tr><tr><td class="line-number" value="258"></td><td class="line-content">parallel operations to train a DNN.</td></tr><tr><td class="line-number" value="259"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="260"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="261"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>DNNs have been used to solve previously intractable problems in a</td></tr><tr><td class="line-number" value="262"></td><td class="line-content">wide range of areas, including:<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="263"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="264"></td><td class="line-content"><span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="265"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>image recognition,<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="266"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>speech recognition,<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="267"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>game playing,<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="268"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>control systems,<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="269"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>driver-less cars, and<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="270"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>unmanned aerial systems<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="271"></td><td class="line-content"><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="272"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="273"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>The basic structure of a DNN is made up of: an input layer; two or</td></tr><tr><td class="line-number" value="274"></td><td class="line-content">more hidden layers of neurons of some size; an output layer; edges</td></tr><tr><td class="line-number" value="275"></td><td class="line-content">connecting neurons between adjacent layers; activation values and</td></tr><tr><td class="line-number" value="276"></td><td class="line-content">biases at each neuron; weights on each edge.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="277"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="278"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="279"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/FCN.png" rel="noreferrer noopener">figs/FCN.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="280"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="281"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="282"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="283"></td><td class="line-content">This example shows a FCN with two hidden layers. More complicated DNNs</td></tr><tr><td class="line-number" value="284"></td><td class="line-content">like convolutional neural networks precede the FCN with a set of</td></tr><tr><td class="line-number" value="285"></td><td class="line-content">convolution operations to extract <span class="html-tag">&lt;i&gt;</span>features<span class="html-tag">&lt;/i&gt;</span> which are selected,</td></tr><tr><td class="line-number" value="286"></td><td class="line-content">filtered, and then used as an input layer into a final FCN for</td></tr><tr><td class="line-number" value="287"></td><td class="line-content">classification. We will begin our exploration of DNNs with a</td></tr><tr><td class="line-number" value="288"></td><td class="line-content">PyTorch example that classifies images of handwritten numbers from 0</td></tr><tr><td class="line-number" value="289"></td><td class="line-content">to 9 using a simple FCN.</td></tr><tr><td class="line-number" value="290"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="291"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="292"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="293"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Neurons<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="294"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="295"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Neural networks, including deep neural networks, are made up of</td></tr><tr><td class="line-number" value="296"></td><td class="line-content">  specialized perceptrons called neurons. A perceptron is an element</td></tr><tr><td class="line-number" value="297"></td><td class="line-content">  that receives binary input(s) \(\{ x_1, \ldots, x_n \}\) and generates</td></tr><tr><td class="line-number" value="298"></td><td class="line-content">  a binary output.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="299"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="300"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="301"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/perceptron.png" rel="noreferrer noopener">figs/perceptron.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="302"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="303"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="304"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Weights \(\{ w_1, \ldots, w_n \}\) are normally applied to the</td></tr><tr><td class="line-number" value="305"></td><td class="line-content">  inputs to define their <span class="html-tag">&lt;i&gt;</span>importance<span class="html-tag">&lt;/i&gt;</span>. If the sum of the inputs</td></tr><tr><td class="line-number" value="306"></td><td class="line-content">  multiplied by their weights exceeds a threshold or <span class="html-tag">&lt;i&gt;</span>bias<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="307"></td><td class="line-content">  \(b_i\), the perceptron fires 1, otherwise it fires 0.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="308"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="309"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="310"></td><td class="line-content">\text{output} = \begin{cases}</td></tr><tr><td class="line-number" value="311"></td><td class="line-content">0 \; \text{if} \; \sum_i{w_i\,x_i} \leq b_i \\</td></tr><tr><td class="line-number" value="312"></td><td class="line-content">1 \; \text{if} \; \sum_i{w_i\,x_i} &gt; b_i</td></tr><tr><td class="line-number" value="313"></td><td class="line-content">\end{cases}</td></tr><tr><td class="line-number" value="314"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="315"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="316"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>We will normally simplify this notation by moving the bias inside</td></tr><tr><td class="line-number" value="317"></td><td class="line-content">  the equation and using a dot product for the sum.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="318"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="319"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="320"></td><td class="line-content">\text{output} = \begin{cases}</td></tr><tr><td class="line-number" value="321"></td><td class="line-content">0 \; \text{if} \; w \cdot x - b \leq 0 \\</td></tr><tr><td class="line-number" value="322"></td><td class="line-content">1 \; \text{otherwise}</td></tr><tr><td class="line-number" value="323"></td><td class="line-content">\end{cases}</td></tr><tr><td class="line-number" value="324"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="325"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="326"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Bias \(b_i\) is a measure of how easy or difficult it is to make a</td></tr><tr><td class="line-number" value="327"></td><td class="line-content">  perceptron fire. The larger \(b_i\), the more input</td></tr><tr><td class="line-number" value="328"></td><td class="line-content">  or <span class="html-tag">&lt;i&gt;</span>activation<span class="html-tag">&lt;/i&gt;</span> perceptron <span class="html-tag">&lt;i&gt;</span>i<span class="html-tag">&lt;/i&gt;</span> needs to fire.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="329"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="330"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Although useful, perceptrons are inconvenient since both their</td></tr><tr><td class="line-number" value="331"></td><td class="line-content">  inputs and outputs are binary. To address this, we normally convert</td></tr><tr><td class="line-number" value="332"></td><td class="line-content">  perceptrons to neurons. Neurons accept continuous inputs and</td></tr><tr><td class="line-number" value="333"></td><td class="line-content">  generate continuous outputs. To do this, neurons apply a function</td></tr><tr><td class="line-number" value="334"></td><td class="line-content">  like sigmoid to produce smooth output over a fixed range. For</td></tr><tr><td class="line-number" value="335"></td><td class="line-content">  example, "sigmoid" neurons accept continuous input and apply</td></tr><tr><td class="line-number" value="336"></td><td class="line-content">  \(\sigma\) to the weighted sum of activation plus bias to produce</td></tr><tr><td class="line-number" value="337"></td><td class="line-content">  continuous output on the range \(0, \ldots, 1\).<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="338"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="339"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="340"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="341"></td><td class="line-content">z &amp; = w \cdot x + b \\</td></tr><tr><td class="line-number" value="342"></td><td class="line-content">\text{output} &amp; = \sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 +</td></tr><tr><td class="line-number" value="343"></td><td class="line-content">\text{exp}(-(w \cdot x + b))}</td></tr><tr><td class="line-number" value="344"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="345"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="346"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="347"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Intuitively, if \(z=w \cdot x + b\) is large, \(e^{-z} \approx 0\)</td></tr><tr><td class="line-number" value="348"></td><td class="line-content">  so \(\sigma(z) \approx 1\) and if \(z\) is small \(\sigma(z)</td></tr><tr><td class="line-number" value="349"></td><td class="line-content">  \approx 0\), just like a perceptron. The difference is that</td></tr><tr><td class="line-number" value="350"></td><td class="line-content">  \(\sigma\) is smooth over the range \(0, \ldots, 1\) so a small</td></tr><tr><td class="line-number" value="351"></td><td class="line-content">  change in weights or biases \(\Delta w_i\) or \(\Delta b_i\) will</td></tr><tr><td class="line-number" value="352"></td><td class="line-content">  produce a small change in output, versus a perceptron's potential to</td></tr><tr><td class="line-number" value="353"></td><td class="line-content">  flip its output from 0 to 1 or vice versa.</td></tr><tr><td class="line-number" value="354"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="355"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="356"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="357"></td><td class="line-content">  \Delta \text{output} \approx \sum_i\frac{ \partial \,</td></tr><tr><td class="line-number" value="358"></td><td class="line-content">  \text{output}}{\partial w_i} \Delta w_i + \frac{\partial \,</td></tr><tr><td class="line-number" value="359"></td><td class="line-content">  \text{output}}{\partial b_i} \Delta b_i</td></tr><tr><td class="line-number" value="360"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="361"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="362"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>That is, \(\Delta\)output is linear in \(\Delta w\) and \(\Delta</td></tr><tr><td class="line-number" value="363"></td><td class="line-content">  b\).</td></tr><tr><td class="line-number" value="364"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="365"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="366"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="367"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Handwritten Number Recognition<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="368"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="369"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="370"></td><td class="line-content">PyTorch's dataset repository includes</td></tr><tr><td class="line-number" value="371"></td><td class="line-content">the <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="http://yann.lecun.com/exdb/mnist/" rel="noreferrer noopener">http://yann.lecun.com/exdb/mnist/</a>"&gt;</span>MNIST database of</td></tr><tr><td class="line-number" value="372"></td><td class="line-content">handwritten images<span class="html-tag">&lt;/a&gt;</span>, which includes 60,000 training examples and</td></tr><tr><td class="line-number" value="373"></td><td class="line-content">10,000 test examples. It is a very common dataset to use to test</td></tr><tr><td class="line-number" value="374"></td><td class="line-content">learning or pattern recognition algorithms on real-world data,</td></tr><tr><td class="line-number" value="375"></td><td class="line-content">without the need to hand-label a large training dataset.</td></tr><tr><td class="line-number" value="376"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="377"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="378"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="379"></td><td class="line-content">Normally, images are processed with CNNs. However, we will use a</td></tr><tr><td class="line-number" value="380"></td><td class="line-content">simple FCN to recognize the handwritten images. This is done by</td></tr><tr><td class="line-number" value="381"></td><td class="line-content">converting each handwritten digit image into a set of pixel values,</td></tr><tr><td class="line-number" value="382"></td><td class="line-content">then converting that into a one-dimensional vector. This 1D vector</td></tr><tr><td class="line-number" value="383"></td><td class="line-content">acts as input to one or more hidden layers, filters like ReLU</td></tr><tr><td class="line-number" value="384"></td><td class="line-content">(rectified linear unit), and finally to an output layer with ten</td></tr><tr><td class="line-number" value="385"></td><td class="line-content">possible classifications representing the ten digits 0&amp;ndash;9. Here</td></tr><tr><td class="line-number" value="386"></td><td class="line-content">are two simple examples of the digits in the MNIST dataset.</td></tr><tr><td class="line-number" value="387"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="388"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="389"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="390"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/MNIST.png" rel="noreferrer noopener">figs/MNIST.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="391"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="392"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="393"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="394"></td><td class="line-content">  In our example neural network to "learn" MNIST images we have a $28</td></tr><tr><td class="line-number" value="395"></td><td class="line-content">  \times 28 = 784$-element input vector of greyscale intensities, an</td></tr><tr><td class="line-number" value="396"></td><td class="line-content">  $n=15$ neuron hidden layer, and 10 output values representing a</td></tr><tr><td class="line-number" value="397"></td><td class="line-content">  decision on which digit the input image represents. To train and</td></tr><tr><td class="line-number" value="398"></td><td class="line-content">  validate the DNN model, MNIST provides 60,000 labeled training</td></tr><tr><td class="line-number" value="399"></td><td class="line-content">  images and 10,000 labeled test images, denoted as $x$ and $y$,</td></tr><tr><td class="line-number" value="400"></td><td class="line-content">  respectively. Any individual $x_i$ is a 784-length vector and the</td></tr><tr><td class="line-number" value="401"></td><td class="line-content">  corresponding $y_i$ is a 10-length output vector with all 0s except</td></tr><tr><td class="line-number" value="402"></td><td class="line-content">  for the target digit position, which is 1.</td></tr><tr><td class="line-number" value="403"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="404"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="405"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="406"></td><td class="line-content">  Our goal is to train a neural network so output approximates</td></tr><tr><td class="line-number" value="407"></td><td class="line-content">  $y_i = y(x_i) \, \forall x_i$ in the training set. To do this, we define</td></tr><tr><td class="line-number" value="408"></td><td class="line-content">  a cost function to measure accuracy or error for any given</td></tr><tr><td class="line-number" value="409"></td><td class="line-content">  prediction.</td></tr><tr><td class="line-number" value="410"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="411"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="412"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="413"></td><td class="line-content">C(w,b) = \frac{1}{2n} \sum_x || y(x) - a ||^{2}</td></tr><tr><td class="line-number" value="414"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="415"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="416"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="417"></td><td class="line-content">  where $w$, $b$, $y$, and $a$ are weights, biases, known output</td></tr><tr><td class="line-number" value="418"></td><td class="line-content">  vectors (labels), and predicted output vectors (activation) for all</td></tr><tr><td class="line-number" value="419"></td><td class="line-content">  training input samples $x$. You should recognize this as the simple</td></tr><tr><td class="line-number" value="420"></td><td class="line-content">  quadratic function mean squared error (MSE), where $C(w,b)</td></tr><tr><td class="line-number" value="421"></td><td class="line-content">  \rightarrow 0$ as we improve our ability to predict correct output.</td></tr><tr><td class="line-number" value="422"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="423"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="424"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="425"></td><td class="line-content">  Obviously, when we start we expect $C(w,b)$ to be large. To reduce</td></tr><tr><td class="line-number" value="426"></td><td class="line-content">  it, we <span class="html-tag">&lt;i&gt;</span>optimize<span class="html-tag">&lt;/i&gt;</span> $w$ and $b$ throughout the network using</td></tr><tr><td class="line-number" value="427"></td><td class="line-content">  gradient descent. Ignoring for now the specifics of our neural net,</td></tr><tr><td class="line-number" value="428"></td><td class="line-content">  how can we minimize $C(v)$ where $v$ are tunable parameters? For</td></tr><tr><td class="line-number" value="429"></td><td class="line-content">  simplicity, assume $v=(v_1,v_2)$ is a two-dimensional parameter</td></tr><tr><td class="line-number" value="430"></td><td class="line-content">  vector. Plotting $C(v)$ produces a valley-like image. </td></tr><tr><td class="line-number" value="431"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="432"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="433"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="434"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/2d-fn-viz.png" rel="noreferrer noopener">figs/2d-fn-viz.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="435"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="436"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="437"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="438"></td><td class="line-content">  From any point on the valley, we want to move in a direction with</td></tr><tr><td class="line-number" value="439"></td><td class="line-content">  the steepest slope towards the valley bottom (minimum $C$</td></tr><tr><td class="line-number" value="440"></td><td class="line-content">  value). What happens when we move a small amount $\Delta v_1$ in the</td></tr><tr><td class="line-number" value="441"></td><td class="line-content">  $v_1$ direction? Or a small amount $\Delta v_2$ in the $v_2$</td></tr><tr><td class="line-number" value="442"></td><td class="line-content">  direction?</td></tr><tr><td class="line-number" value="443"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="444"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="445"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="446"></td><td class="line-content">  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +</td></tr><tr><td class="line-number" value="447"></td><td class="line-content">  \frac{\partial C}{\partial v_2} \Delta v_2</td></tr><tr><td class="line-number" value="448"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="449"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="450"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="451"></td><td class="line-content">  We want $C$ to be negative. Defining $\Delta v = (\Delta v_1, \Delta</td></tr><tr><td class="line-number" value="452"></td><td class="line-content">  v_2)^{T}$ and gradient of descent $\nabla C =\,$<span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$(</td></tr><tr><td class="line-number" value="453"></td><td class="line-content">  \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}</td></tr><tr><td class="line-number" value="454"></td><td class="line-content">  )$<span class="html-tag">&lt;/span&gt;</span>, we have</td></tr><tr><td class="line-number" value="455"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="456"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="457"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="458"></td><td class="line-content">\Delta C = \nabla C \cdot \Delta v</td></tr><tr><td class="line-number" value="459"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="460"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="461"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="462"></td><td class="line-content">  Now, if we pick $\epsilon$ s.t.</td></tr><tr><td class="line-number" value="463"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="464"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="465"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="466"></td><td class="line-content">\Delta v = -\epsilon \nabla C</td></tr><tr><td class="line-number" value="467"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="468"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="469"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="470"></td><td class="line-content">  where $\epsilon$ is the <span class="html-tag">&lt;i&gt;</span>learning rate<span class="html-tag">&lt;/i&gt;</span> and therefore $\Delta</td></tr><tr><td class="line-number" value="471"></td><td class="line-content">  v$ is a small movement in the direction of steepest gradient, then</td></tr><tr><td class="line-number" value="472"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="473"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="474"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="475"></td><td class="line-content">\Delta C = \nabla C \cdot -\epsilon \nabla C = -\epsilon || \nabla C ||^{2}</td></tr><tr><td class="line-number" value="476"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="477"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="478"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="479"></td><td class="line-content">  Since $||\nabla C\,||^{2}$ is always positive, $\Delta C$ is always</td></tr><tr><td class="line-number" value="480"></td><td class="line-content">  negative. Setting $v \rightarrow v^{\prime} = v - \epsilon \nabla C$</td></tr><tr><td class="line-number" value="481"></td><td class="line-content">  over and over, we will converge on a minimum $C$, as long as</td></tr><tr><td class="line-number" value="482"></td><td class="line-content">  $\epsilon$ is not too big, causing us to "jump" back and forth over</td></tr><tr><td class="line-number" value="483"></td><td class="line-content">  the minimum, or too small, causing convergence to be too</td></tr><tr><td class="line-number" value="484"></td><td class="line-content">  expensive. Moreover, if we expand $v$ into a vector of $m &gt; 2$</td></tr><tr><td class="line-number" value="485"></td><td class="line-content">  variables $v=(v_1, \ldots, v_m)$, this approach continuous to work.</td></tr><tr><td class="line-number" value="486"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="487"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="488"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="489"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="490"></td><td class="line-content">\Delta C &amp; = \nabla C \cdot \Delta v \\</td></tr><tr><td class="line-number" value="491"></td><td class="line-content">\nabla C &amp; = (\frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m}) \\</td></tr><tr><td class="line-number" value="492"></td><td class="line-content">\Delta v &amp; = -\epsilon \nabla C \\</td></tr><tr><td class="line-number" value="493"></td><td class="line-content">v \rightarrow v^{\prime} &amp; = v - \epsilon \nabla C \\</td></tr><tr><td class="line-number" value="494"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="495"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="496"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="497"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="498"></td><td class="line-content">  How do we extend this general gradient descent approach to our specific</td></tr><tr><td class="line-number" value="499"></td><td class="line-content">  goal of optimizing $C$ in a neural network. We simply rewrite the above</td></tr><tr><td class="line-number" value="500"></td><td class="line-content">  equations in terms of $w$ and $b$.</td></tr><tr><td class="line-number" value="501"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="502"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="503"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="504"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="505"></td><td class="line-content">w_i \rightarrow w_i^{\prime} &amp; = w_i - \epsilon \frac{\partial C}{\partial w_i} \\</td></tr><tr><td class="line-number" value="506"></td><td class="line-content">b_i \rightarrow b_i^{\prime} &amp; = b_i - \epsilon \frac{\partial C}{\partial b_i} \\</td></tr><tr><td class="line-number" value="507"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="508"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="509"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="510"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="511"></td><td class="line-content">  We can now walk backwards through the layers in the neural network,</td></tr><tr><td class="line-number" value="512"></td><td class="line-content">  adjusting (backpropegating) weights and biases for each neuron. To</td></tr><tr><td class="line-number" value="513"></td><td class="line-content">  do this, recall $C = \frac{1}{n} \sum_x C_x$ and $C_x</td></tr><tr><td class="line-number" value="514"></td><td class="line-content">  =\,$<span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{||y_i(x_i) -</td></tr><tr><td class="line-number" value="515"></td><td class="line-content">  a_i||^{2}}{2}$<span class="html-tag">&lt;/span&gt;</span>. To compute $\nabla C$ we compute $\Delta C_x</td></tr><tr><td class="line-number" value="516"></td><td class="line-content">  \; \forall x$ and average the result.</td></tr><tr><td class="line-number" value="517"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="518"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="519"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="520"></td><td class="line-content">\nabla C = \frac{1}{n} \sum_x \nabla C_x</td></tr><tr><td class="line-number" value="521"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="522"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="523"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="524"></td><td class="line-content">  If the number of neurons $n$ is very large, we may want to sample</td></tr><tr><td class="line-number" value="525"></td><td class="line-content">  $\nabla C_x$. This is called stochastic gradient descent, where we</td></tr><tr><td class="line-number" value="526"></td><td class="line-content">  randomly choose $m \ll n$ training inputs to form a mini-batch</td></tr><tr><td class="line-number" value="527"></td><td class="line-content">  $(x_1, \ldots, x_m)$, assuming</td></tr><tr><td class="line-number" value="528"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="529"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="530"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="531"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="532"></td><td class="line-content">\sum_{i=1}^{m} \frac{\nabla C_{x_i}}{m} &amp; \approx \sum_x \frac{\nabla C_x}{n} = \nabla C \\</td></tr><tr><td class="line-number" value="533"></td><td class="line-content">\nabla C &amp; \approx \frac{1}{m} \sum_{i=1}^{m} \nabla C_{x_i} \\</td></tr><tr><td class="line-number" value="534"></td><td class="line-content">w_i \rightarrow w_{i}^{\prime} &amp; = w_i = \frac{\epsilon}{m} \sum_i \frac{\partial C_{x_i}}{\partial w_i} \\</td></tr><tr><td class="line-number" value="535"></td><td class="line-content">b_i \rightarrow b_{i}^{\prime} &amp; = b_i = \frac{\epsilon}{m} \sum_i \frac{\partial C_{x_i}}{\partial b_i} \\</td></tr><tr><td class="line-number" value="536"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="537"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="538"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="539"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="540"></td><td class="line-content">  We then pick another mini-batch from the remaining samples, repeat the</td></tr><tr><td class="line-number" value="541"></td><td class="line-content">  above process and continue until the training set is exhausted. This</td></tr><tr><td class="line-number" value="542"></td><td class="line-content">  is defined as one training <span class="html-tag">&lt;i&gt;</span>epoch<span class="html-tag">&lt;/i&gt;</span>.</td></tr><tr><td class="line-number" value="543"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="544"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="545"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">backprop</span>"&gt;</span>Backpropegation<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="546"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="547"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="548"></td><td class="line-content">  To begin, we review and simplify the feed-forward component of DNN</td></tr><tr><td class="line-number" value="549"></td><td class="line-content">  training to use matrix notation.</td></tr><tr><td class="line-number" value="550"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="551"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="552"></td><td class="line-content"><span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="553"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="554"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$w_{jk}^{\ell}$, weight from neuron $k$ in layer $(\ell-1)$ to</td></tr><tr><td class="line-number" value="555"></td><td class="line-content">    neuron $j$ in layer $\ell$</td></tr><tr><td class="line-number" value="556"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="557"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$b_{j}^{\ell}$, bias at neuron $j$ in layer $\ell$</td></tr><tr><td class="line-number" value="558"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="559"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$a_{j}^{\ell}$, activation at neuron $j$ in layer $\ell$</td></tr><tr><td class="line-number" value="560"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="561"></td><td class="line-content"><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="562"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="563"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="564"></td><td class="line-content">$\therefore$ $a_{j}^{\ell} = \sigma(\sum_k w_{jk}^{\ell}</td></tr><tr><td class="line-number" value="565"></td><td class="line-content">a_{k}^{(\ell-1)} + b_{j}^{\ell})$.</td></tr><tr><td class="line-number" value="566"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="567"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="568"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="569"></td><td class="line-content">  To convert to matrix format, define a weight matrix $w^{\ell}$ for layer</td></tr><tr><td class="line-number" value="570"></td><td class="line-content">  $\ell$ where $w^{\ell}$ are weights connecting <span class="html-tag">&lt;i&gt;</span><span class="html-tag">&lt;b&gt;</span>to<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/i&gt;</span> layer</td></tr><tr><td class="line-number" value="571"></td><td class="line-content">  $\ell$'s neurons.</td></tr><tr><td class="line-number" value="572"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="573"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="574"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="575"></td><td class="line-content">w^{\ell} =</td></tr><tr><td class="line-number" value="576"></td><td class="line-content">\underbrace{</td></tr><tr><td class="line-number" value="577"></td><td class="line-content">\begin{bmatrix}</td></tr><tr><td class="line-number" value="578"></td><td class="line-content">  w^{\ell}[j,k] \\</td></tr><tr><td class="line-number" value="579"></td><td class="line-content">  = w_{jk}^{\ell}</td></tr><tr><td class="line-number" value="580"></td><td class="line-content">\end{bmatrix}</td></tr><tr><td class="line-number" value="581"></td><td class="line-content">}_{k\text{ input weights}}</td></tr><tr><td class="line-number" value="582"></td><td class="line-content">\;\;\Bigg\}\;{\scriptsize j\text{ neurons}}</td></tr><tr><td class="line-number" value="583"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="584"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="585"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="586"></td><td class="line-content">  Similarly for layer $\ell$, $b^{\ell}$ is a bias vector with</td></tr><tr><td class="line-number" value="587"></td><td class="line-content">  $b_{j}^{\ell}$ the bias for neuron $j$ in layer $\ell$. $a^{\ell}$</td></tr><tr><td class="line-number" value="588"></td><td class="line-content">  is an activation vector with $a_{j}^{\ell}$ the activation for</td></tr><tr><td class="line-number" value="589"></td><td class="line-content">  neuron $j$ in layer $\ell$.</td></tr><tr><td class="line-number" value="590"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="591"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="592"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="593"></td><td class="line-content">  Finally, we use vectorization to apply the activation function to</td></tr><tr><td class="line-number" value="594"></td><td class="line-content">  weights times previous layer activations plus biases.</td></tr><tr><td class="line-number" value="595"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="596"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="597"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="598"></td><td class="line-content">\sigma \left(</td></tr><tr><td class="line-number" value="599"></td><td class="line-content">\begin{bmatrix}</td></tr><tr><td class="line-number" value="600"></td><td class="line-content">x_1 \\ \cdots \\ x_m</td></tr><tr><td class="line-number" value="601"></td><td class="line-content">\end{bmatrix}</td></tr><tr><td class="line-number" value="602"></td><td class="line-content">\right)</td></tr><tr><td class="line-number" value="603"></td><td class="line-content">=</td></tr><tr><td class="line-number" value="604"></td><td class="line-content">\begin{bmatrix}</td></tr><tr><td class="line-number" value="605"></td><td class="line-content">\sigma(x_1) \\ \cdots \\ \sigma(x_m)</td></tr><tr><td class="line-number" value="606"></td><td class="line-content">\end{bmatrix}</td></tr><tr><td class="line-number" value="607"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="608"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="609"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="610"></td><td class="line-content">  Combining all of these, for layer $\ell$</td></tr><tr><td class="line-number" value="611"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="612"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="613"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="614"></td><td class="line-content">a^{\ell} = \sigma( w^{\ell} a^{(\ell-1)} + b^{\ell})</td></tr><tr><td class="line-number" value="615"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="616"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="617"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="618"></td><td class="line-content">  The value $z^{\ell} = w^{\ell} a^{(\ell-1)}+b^{\ell}$ is important,</td></tr><tr><td class="line-number" value="619"></td><td class="line-content">  so we explicitly extract it and define it as the <span class="html-tag">&lt;i&gt;</span>weighted</td></tr><tr><td class="line-number" value="620"></td><td class="line-content">  input<span class="html-tag">&lt;/i&gt;</span> to neurons in layer $\ell$. We can now write</td></tr><tr><td class="line-number" value="621"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="622"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="623"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="624"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="625"></td><td class="line-content">a^{\ell} &amp; = \sigma(z^{\ell})\\</td></tr><tr><td class="line-number" value="626"></td><td class="line-content">z_{j}^{\ell} &amp; = \sum_k w_{jk}^{\ell} a^{(\ell-1)} + b_{j}^{\ell}</td></tr><tr><td class="line-number" value="627"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="628"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="629"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="630"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Cost Function Assumptions<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="631"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="632"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="633"></td><td class="line-content">  The goal of backpropegation is to</td></tr><tr><td class="line-number" value="634"></td><td class="line-content">  compute <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial c}{\partial w}$<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="635"></td><td class="line-content">  and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial b}$<span class="html-tag">&lt;/span&gt;</span> for</td></tr><tr><td class="line-number" value="636"></td><td class="line-content">  cost function $C$, w.r.t. weights $w$ and biases $b$ in the</td></tr><tr><td class="line-number" value="637"></td><td class="line-content">  network. To do this, we make two assumptions about $C$. You can</td></tr><tr><td class="line-number" value="638"></td><td class="line-content">  assume our $C$ is MSE, $C = \frac{1}{2n} \sum_x || y(x) =</td></tr><tr><td class="line-number" value="639"></td><td class="line-content">  a^{L}(x)||^2$, where</td></tr><tr><td class="line-number" value="640"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="641"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="642"></td><td class="line-content"><span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="643"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$n$, total number of training examples;</td></tr><tr><td class="line-number" value="644"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$\sum_x$, sum over individual examples $x$;</td></tr><tr><td class="line-number" value="645"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$y=y(x)$, expected output;</td></tr><tr><td class="line-number" value="646"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$L$, number of layers in network, and</td></tr><tr><td class="line-number" value="647"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>$a^{L}=a^{L}(x)$, vector of activation outputs for input $x$.</td></tr><tr><td class="line-number" value="648"></td><td class="line-content"><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="649"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="650"></td><td class="line-content"><span class="html-tag">&lt;ol&gt;</span></td></tr><tr><td class="line-number" value="651"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Cost function $C$ can be written as an average $C=\frac{1}{n}</td></tr><tr><td class="line-number" value="652"></td><td class="line-content">  \sum_x C_{x}$ over individual cost functions $C_x$ for individual</td></tr><tr><td class="line-number" value="653"></td><td class="line-content">  training examples $x$. This is required because backpropegation</td></tr><tr><td class="line-number" value="654"></td><td class="line-content">  computes individual <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C_x}{\partial</td></tr><tr><td class="line-number" value="655"></td><td class="line-content">  w}$<span class="html-tag">&lt;/span&gt;</span> and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C_x}{\partial</td></tr><tr><td class="line-number" value="656"></td><td class="line-content">  b}$<span class="html-tag">&lt;/span&gt;</span>, then recovers <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="657"></td><td class="line-content">  C}{\partial w}$<span class="html-tag">&lt;/span&gt;</span> and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="658"></td><td class="line-content">  C}{\partial b}$<span class="html-tag">&lt;/span&gt;</span> by averaging the individual partial</td></tr><tr><td class="line-number" value="659"></td><td class="line-content">  derivatives.</td></tr><tr><td class="line-number" value="660"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="661"></td><td class="line-content">    <span class="html-tag">&lt;li&gt;</span>Cost function $C$ can be written as a function of outputs from</td></tr><tr><td class="line-number" value="662"></td><td class="line-content">    the neural network.</td></tr><tr><td class="line-number" value="663"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="664"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="665"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Elementwise Product (Hadamard or Schur Product)<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="666"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="667"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>For two vectors $s$ and $t$, the elementwise product $s \odot t$ is</td></tr><tr><td class="line-number" value="668"></td><td class="line-content">  $(s \odot t) = s_{j} t_{j}$, <span class="html-tag">&lt;i&gt;</span>e.g.<span class="html-tag">&lt;/i&gt;</span>,</td></tr><tr><td class="line-number" value="669"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="670"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="671"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="672"></td><td class="line-content">\begin{bmatrix} 1 \\ 2 \end{bmatrix}</td></tr><tr><td class="line-number" value="673"></td><td class="line-content">\odot</td></tr><tr><td class="line-number" value="674"></td><td class="line-content">\begin{bmatrix} 3 \\ 4 \end{bmatrix}</td></tr><tr><td class="line-number" value="675"></td><td class="line-content">=</td></tr><tr><td class="line-number" value="676"></td><td class="line-content">\begin{bmatrix} 1 \cdot 3 \\ 2 \cdot 4 \end{bmatrix}</td></tr><tr><td class="line-number" value="677"></td><td class="line-content">=</td></tr><tr><td class="line-number" value="678"></td><td class="line-content">\begin{bmatrix} 3 \\ 8 \end{bmatrix}</td></tr><tr><td class="line-number" value="679"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="680"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="681"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="682"></td><td class="line-content">  This is know as the <span class="html-tag">&lt;i&gt;</span>Hadamard<span class="html-tag">&lt;/i&gt;</span> or <span class="html-tag">&lt;i&gt;</span>Schur<span class="html-tag">&lt;/i&gt;</span> product.</td></tr><tr><td class="line-number" value="683"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="684"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="685"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Four Fundamental Backpropegation Equations<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="686"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="687"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="688"></td><td class="line-content">  At a basic level, backpropegation explains how changing $w$ and $b$</td></tr><tr><td class="line-number" value="689"></td><td class="line-content">  in a network changes $C$. Ultimately, this requires</td></tr><tr><td class="line-number" value="690"></td><td class="line-content">  computing <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="691"></td><td class="line-content">  w_{jk}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="692"></td><td class="line-content">  C}{\partial b^{\ell}}$<span class="html-tag">&lt;/span&gt;</span>. To do this, we introduce an</td></tr><tr><td class="line-number" value="693"></td><td class="line-content">  intermediate quantity $\delta_{j}^{\ell}$<span class="html-tag">&lt;/span&gt;</span>, the error in</td></tr><tr><td class="line-number" value="694"></td><td class="line-content">  neuron $j$ in layer $\ell$. Backpropegation computes</td></tr><tr><td class="line-number" value="695"></td><td class="line-content">  $\delta_{j}^{\ell}$, then relates it</td></tr><tr><td class="line-number" value="696"></td><td class="line-content">  to <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="697"></td><td class="line-content">  w_{jk}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="698"></td><td class="line-content">  C}{\partial b^{\ell}}$<span class="html-tag">&lt;/span&gt;</span>.</td></tr><tr><td class="line-number" value="699"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="700"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="701"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="702"></td><td class="line-content">  Suppose, at some neuron, we modify $z_{j}^{\ell}$ by adding a small</td></tr><tr><td class="line-number" value="703"></td><td class="line-content">  change $\Delta z_{j}^{\ell}$. Now, the neuron outputs $\sigma(</td></tr><tr><td class="line-number" value="704"></td><td class="line-content">  z_{j}^{\ell} + \Delta z_{j}^{\ell} )$, propegating through follow-on</td></tr><tr><td class="line-number" value="705"></td><td class="line-content">  layers in the network and changing the overall cost $C$ by</td></tr><tr><td class="line-number" value="706"></td><td class="line-content">  <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="707"></td><td class="line-content">  z_{j}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span>$\Delta</td></tr><tr><td class="line-number" value="708"></td><td class="line-content">  z_{j}^{\ell}$. If <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="709"></td><td class="line-content">  z_{j}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> is large, we can improve $C$ by choosing</td></tr><tr><td class="line-number" value="710"></td><td class="line-content">  $\Delta z_{j}^{\ell}$ with a sign opposite</td></tr><tr><td class="line-number" value="711"></td><td class="line-content">  to <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="712"></td><td class="line-content">  z_{j}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span>. If <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="713"></td><td class="line-content">  C}{\partial z_{j}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> is small, though, $\Delta</td></tr><tr><td class="line-number" value="714"></td><td class="line-content">  z_{j}^{\ell}$ will have little impact on $C$. Intuitively,</td></tr><tr><td class="line-number" value="715"></td><td class="line-content">  <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial z_{j}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="716"></td><td class="line-content">  is a measure of the amount of error in a neuron.</td></tr><tr><td class="line-number" value="717"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="718"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="719"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="720"></td><td class="line-content">  Given this, we define error $\delta_{j}^{l}$ of neuron $j$ in layer</td></tr><tr><td class="line-number" value="721"></td><td class="line-content">  $\ell$ as</td></tr><tr><td class="line-number" value="722"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="723"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="724"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="725"></td><td class="line-content">\delta_{j}^{\ell} = \frac{\partial C}{\partial z_{j}^{\ell}}</td></tr><tr><td class="line-number" value="726"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="727"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="728"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="729"></td><td class="line-content">  As before, $\delta^{\ell}$ is a vector of errors for neurons in</td></tr><tr><td class="line-number" value="730"></td><td class="line-content">  layer $\ell$. Backpropegation computes $\delta^{\ell}$ for every</td></tr><tr><td class="line-number" value="731"></td><td class="line-content">  layer, then relates $\delta^{\ell}$ to the quantities of real</td></tr><tr><td class="line-number" value="732"></td><td class="line-content">  interest, <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="733"></td><td class="line-content">  w_{jk}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> and <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial</td></tr><tr><td class="line-number" value="734"></td><td class="line-content">  C}{\partial b^{\ell}}$<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="735"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="736"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="737"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="738"></td><td class="line-content">  <span class="html-tag">&lt;b&gt;</span>Eq 1:<span class="html-tag">&lt;/b&gt;</span> Computing $\delta^{L}$ error in the output layer.</td></tr><tr><td class="line-number" value="739"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="740"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="741"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Components of $\delta^{L}$ are</td></tr><tr><td class="line-number" value="742"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="743"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="744"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="745"></td><td class="line-content">\delta_{j}^{L} = \frac{\partial C}{\partial a_{j}^{L}}</td></tr><tr><td class="line-number" value="746"></td><td class="line-content">\sigma^{\prime}(z_{j}^{L})</td></tr><tr><td class="line-number" value="747"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="748"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="749"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="750"></td><td class="line-content">  <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial a_j^L}$<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="751"></td><td class="line-content">  measures how fast cost is changing as a function of output</td></tr><tr><td class="line-number" value="752"></td><td class="line-content">  activation $j$. $\sigma^{\prime}(z_y^L)$ measures how fast</td></tr><tr><td class="line-number" value="753"></td><td class="line-content">  activation function $\sigma$ is changing at $z_y^{L}$. $z_y^{L}$ is</td></tr><tr><td class="line-number" value="754"></td><td class="line-content">  computed during feed-forward, so $\sigma^{\prime}(z_y^L)$ is easily</td></tr><tr><td class="line-number" value="755"></td><td class="line-content">  obtained. <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="756"></td><td class="line-content">  a_j^L}$<span class="html-tag">&lt;/span&gt;</span> depends on $C$, but for our MSE cost function</td></tr><tr><td class="line-number" value="757"></td><td class="line-content">  <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial a_j^L} =\,$<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="758"></td><td class="line-content">  $(a_j^{L} - y_j)$.</td></tr><tr><td class="line-number" value="759"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="760"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="761"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="762"></td><td class="line-content">  To extend $\delta_j^L$ to matrix-based form</td></tr><tr><td class="line-number" value="763"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="764"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="765"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="766"></td><td class="line-content">\delta^L = \nabla_a C \odot \sigma^{\prime}(z^L)</td></tr><tr><td class="line-number" value="767"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="768"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="769"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="770"></td><td class="line-content">  where $\nabla_a C$ is a vector whose components</td></tr><tr><td class="line-number" value="771"></td><td class="line-content">  are <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="772"></td><td class="line-content">  a_j^L}$<span class="html-tag">&lt;/span&gt;</span>. $\nabla_a C$ expresses the rate of change of $C$</td></tr><tr><td class="line-number" value="773"></td><td class="line-content">  w.r.t. output activations. In our example $\nabla_a C = (a^L - y)$,</td></tr><tr><td class="line-number" value="774"></td><td class="line-content">  so the full matrix form of Eq. 1 is</td></tr><tr><td class="line-number" value="775"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="776"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="777"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="778"></td><td class="line-content">\sigma^L = (a^L - y) \odot \sigma^{\prime}(z^L)</td></tr><tr><td class="line-number" value="779"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="780"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="781"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="782"></td><td class="line-content">  <span class="html-tag">&lt;b&gt;</span>Eq 2:<span class="html-tag">&lt;/b&gt;</span> Computing $\delta^{\ell}$ from $\delta^{\ell+1}$</td></tr><tr><td class="line-number" value="783"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="784"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="785"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="786"></td><td class="line-content">  Given $\delta^{\ell+1}$</td></tr><tr><td class="line-number" value="787"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="788"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="789"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="790"></td><td class="line-content">\delta^{\ell} = ((w^{\ell+1})^{T} \delta^{\ell+1}) \odot</td></tr><tr><td class="line-number" value="791"></td><td class="line-content">\sigma^{\prime}(z^{\ell})</td></tr><tr><td class="line-number" value="792"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="793"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="794"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="795"></td><td class="line-content">  where $(w^{\ell+1})^{T}$ is the transpose of the weight matrix</td></tr><tr><td class="line-number" value="796"></td><td class="line-content">  $w^{\ell+1}$ for layer $\ell+1$.</td></tr><tr><td class="line-number" value="797"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="798"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="799"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="800"></td><td class="line-content">  Suppose we know the error $\delta^{\ell+1}$ at layer $\ell+1$.</td></tr><tr><td class="line-number" value="801"></td><td class="line-content">  Applying $(w^{\ell+1})^{T}$ moves error backwards, giving us some</td></tr><tr><td class="line-number" value="802"></td><td class="line-content">  measure of error in layer $\ell$. Applying the Hadamard product</td></tr><tr><td class="line-number" value="803"></td><td class="line-content">  $\odot \sigma^{\prime}(z^{\ell})$ pushes error backwards through the</td></tr><tr><td class="line-number" value="804"></td><td class="line-content">  activation function in layer $\ell$, giving us backpropegated error</td></tr><tr><td class="line-number" value="805"></td><td class="line-content">  $\delta^{\ell}$ in weighted input to layer $\ell$.</td></tr><tr><td class="line-number" value="806"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="807"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="808"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="809"></td><td class="line-content">  We can use this to compute error $\delta^{\ell}$ for any layer</td></tr><tr><td class="line-number" value="810"></td><td class="line-content">  $\ell$ by starting with $\delta^{L}$ (Eq. 1), using it to calculate</td></tr><tr><td class="line-number" value="811"></td><td class="line-content">  $\delta^{L-1}$, then $\delta^{L-2}$ and so on until $L-i=\ell$.</td></tr><tr><td class="line-number" value="812"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="813"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="814"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="815"></td><td class="line-content">  <span class="html-tag">&lt;b&gt;</span>Eq 3:<span class="html-tag">&lt;/b&gt;</span> Rate of change of $C$ w.r.t. bias</td></tr><tr><td class="line-number" value="816"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="817"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="818"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="819"></td><td class="line-content">  It turns out that <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="820"></td><td class="line-content">  b_j^{\ell}} =\,$<span class="html-tag">&lt;/span&gt;</span> $\delta_j^{\ell}$. That is, error</td></tr><tr><td class="line-number" value="821"></td><td class="line-content">  $\delta_j^{\ell}$ is exactly equal to the rate of change of bias</td></tr><tr><td class="line-number" value="822"></td><td class="line-content">  <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="823"></td><td class="line-content">  b_j^{\ell}}$<span class="html-tag">&lt;/span&gt;</span>. Since we already know how to compute</td></tr><tr><td class="line-number" value="824"></td><td class="line-content">  $\delta_j^{\ell}$, we can rewrite this as</td></tr><tr><td class="line-number" value="825"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="826"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="827"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="828"></td><td class="line-content">\frac{\partial C}{\partial b} = \delta</td></tr><tr><td class="line-number" value="829"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="830"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="831"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="832"></td><td class="line-content">  where $\delta$ is being evaluated at the same neuron as bias $b$. </td></tr><tr><td class="line-number" value="833"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="834"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="835"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="836"></td><td class="line-content">  <span class="html-tag">&lt;b&gt;</span>Eq 4:<span class="html-tag">&lt;/b&gt;</span> Rate of change of $C$ w.r.t. weight</td></tr><tr><td class="line-number" value="837"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="838"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="839"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="840"></td><td class="line-content">  Here</td></tr><tr><td class="line-number" value="841"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="842"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="843"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="844"></td><td class="line-content">\frac{\partial C}{\partial w_{jk}^{\ell}} = a_k^{\ell-1} \delta_j^{\ell}</td></tr><tr><td class="line-number" value="845"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="846"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="847"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="848"></td><td class="line-content">so partial derivative <span class="html-tag">&lt;span <span class="html-attribute-name">class</span>="<span class="html-attribute-value">frac</span>"&gt;</span>$\frac{\partial C}{\partial</td></tr><tr><td class="line-number" value="849"></td><td class="line-content">w_{jk}^{\ell}}$<span class="html-tag">&lt;/span&gt;</span> depends on $\delta^{\ell}$ and $a^{\ell-1}$,</td></tr><tr><td class="line-number" value="850"></td><td class="line-content">which we already know how to compute. Our equation can be rewritten as</td></tr><tr><td class="line-number" value="851"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="852"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="853"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="854"></td><td class="line-content">\frac{\partial C}{\partial w} = a_\text{in} \delta_\text{out}</td></tr><tr><td class="line-number" value="855"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="856"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="857"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="858"></td><td class="line-content">  where $a_\text{in}$ is activation of the neuron's input to its</td></tr><tr><td class="line-number" value="859"></td><td class="line-content">  weight $w$ and $\delta_\text{out}$ is the error of the neuron output</td></tr><tr><td class="line-number" value="860"></td><td class="line-content">  for its weight $w$. Examining just weight $w$ and the two neurons</td></tr><tr><td class="line-number" value="861"></td><td class="line-content">  connected by that weight</td></tr><tr><td class="line-number" value="862"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="863"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="864"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="865"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/neuron_weight.png" rel="noreferrer noopener">figs/neuron_weight.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="866"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="867"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="868"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Backpropegation Algorithm<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="869"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="870"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="871"></td><td class="line-content">  Combining all four equations, we obtain</td></tr><tr><td class="line-number" value="872"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="873"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="874"></td><td class="line-content"><span class="html-tag">&lt;ol&gt;</span></td></tr><tr><td class="line-number" value="875"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="876"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Input $x$, set $a^1$ for input layer.</td></tr><tr><td class="line-number" value="877"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="878"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Feed-forward, for each $\ell = 2, 3, \ldots, L$ compute</td></tr><tr><td class="line-number" value="879"></td><td class="line-content">    $z^{\ell} = w^{\ell} a^{\ell-1} + b^{\ell}$ and $a =</td></tr><tr><td class="line-number" value="880"></td><td class="line-content">    \sigma(z^{\ell})$.</td></tr><tr><td class="line-number" value="881"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="882"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Output error $\delta^L$, then compute vector $\delta^L =</td></tr><tr><td class="line-number" value="883"></td><td class="line-content">    \nabla_a C \odot \sigma^{\prime}(z^L)$.</td></tr><tr><td class="line-number" value="884"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="885"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Backpropegate error, for each $\ell = L-1, L-2, \ldots, 2$</td></tr><tr><td class="line-number" value="886"></td><td class="line-content">    compute $\delta^{\ell} = ((w^{\ell+1})^{T} \delta^{\ell+1}) \odot</td></tr><tr><td class="line-number" value="887"></td><td class="line-content">    \sigma^{\prime}(z^{\ell})$.</td></tr><tr><td class="line-number" value="888"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="889"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Result<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="890"></td><td class="line-content">    \[</td></tr><tr><td class="line-number" value="891"></td><td class="line-content">    \frac{\partial C}{\partial w_{jk}^{\ell}} = a^{\ell-1} \delta_j^{\ell}, \;\;\;</td></tr><tr><td class="line-number" value="892"></td><td class="line-content">    \frac{\partial C}{\partial b_j^{\ell}} = \delta_j^{\ell}</td></tr><tr><td class="line-number" value="893"></td><td class="line-content">    \]</td></tr><tr><td class="line-number" value="894"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="895"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="896"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="897"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Full Training Algorithm<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="898"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="899"></td><td class="line-content"><span class="html-tag">&lt;ol&gt;</span></td></tr><tr><td class="line-number" value="900"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="901"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Input training examples.</td></tr><tr><td class="line-number" value="902"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="903"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>For each training example $x$</td></tr><tr><td class="line-number" value="904"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="905"></td><td class="line-content">  <span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="906"></td><td class="line-content">    <span class="html-tag">&lt;li&gt;</span>set input activation $a^{x,1}$,</td></tr><tr><td class="line-number" value="907"></td><td class="line-content">    <span class="html-tag">&lt;li&gt;</span>feed-forward, for each $\ell=2, 3, \ldots, L$ compute</td></tr><tr><td class="line-number" value="908"></td><td class="line-content">      $z^{x,\ell} = w^{\ell}a^{x,\ell-1} + b^{\ell}$ and</td></tr><tr><td class="line-number" value="909"></td><td class="line-content">      $a^{x,l} = \sigma(z^{x,l})$,</td></tr><tr><td class="line-number" value="910"></td><td class="line-content">    <span class="html-tag">&lt;li&gt;</span>output error, compute $\delta^{x,\ell} = \nabla_a C_x \odot</td></tr><tr><td class="line-number" value="911"></td><td class="line-content">      \sigma^{\prime}(z^{x,\ell})$</td></tr><tr><td class="line-number" value="912"></td><td class="line-content">    <span class="html-tag">&lt;li&gt;</span>backpropegate error for each $\ell = L-1, L-2, \ldots, 2$</td></tr><tr><td class="line-number" value="913"></td><td class="line-content">      by computing $\delta^{x,\ell} = ((w^{\ell+1})^{T} \delta^{x,\ell+1})</td></tr><tr><td class="line-number" value="914"></td><td class="line-content">      \odot \sigma^{\prime}(z^{x,\ell})$</td></tr><tr><td class="line-number" value="915"></td><td class="line-content">  <span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="916"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="917"></td><td class="line-content">  <span class="html-tag">&lt;li&gt;</span>Gradient descent, for each $\ell = L, L-1, \ldots, 2$ update</td></tr><tr><td class="line-number" value="918"></td><td class="line-content">    weights and biases according to rules<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="919"></td><td class="line-content">    \[</td></tr><tr><td class="line-number" value="920"></td><td class="line-content">    \begin{align}</td></tr><tr><td class="line-number" value="921"></td><td class="line-content">    w^{\ell} &amp; \rightarrow w^{\ell} - \frac{\eta}{m} \sum_x \delta^{x,\ell} (a^{x,\ell-1})^{T}\\</td></tr><tr><td class="line-number" value="922"></td><td class="line-content">    b^{\ell} &amp; \rightarrow b^{\ell} - \frac{\eta}{m} \sum_x \delta^{x,\ell}</td></tr><tr><td class="line-number" value="923"></td><td class="line-content">    \end{align}</td></tr><tr><td class="line-number" value="924"></td><td class="line-content">    \]</td></tr><tr><td class="line-number" value="925"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="926"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="927"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">pytorch</span>"&gt;</span>PyTorch<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="928"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="929"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="930"></td><td class="line-content">PyTorch is, at its core, a tensor computing language with GPU acceleration</td></tr><tr><td class="line-number" value="931"></td><td class="line-content">support and a deep neural network library. PyTorch began as an internship</td></tr><tr><td class="line-number" value="932"></td><td class="line-content">program by Adam Paszke (now a Senior Research Scientist at Google) in October</td></tr><tr><td class="line-number" value="933"></td><td class="line-content">2016. It was based on Torch, an open-source machine learning library</td></tr><tr><td class="line-number" value="934"></td><td class="line-content">and scientific computing framework written in the Lua programming language.</td></tr><tr><td class="line-number" value="935"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="936"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="937"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="938"></td><td class="line-content">Three additional authors (Sam Gross, Soumith Chintala, and Gregory</td></tr><tr><td class="line-number" value="939"></td><td class="line-content">Chanan) formed the original author list. PyTorch was released as an</td></tr><tr><td class="line-number" value="940"></td><td class="line-content">open source machine learning language using the BSD open source</td></tr><tr><td class="line-number" value="941"></td><td class="line-content">license. Facebook currently operates</td></tr><tr><td class="line-number" value="942"></td><td class="line-content">both <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://ai.facebook.com/tools/pytorch/" rel="noreferrer noopener">https://ai.facebook.com/tools/pytorch/</a>"</span></td></tr><tr><td class="line-number" value="943"></td><td class="line-content"><span class="html-attribute-name">target</span>="<span class="html-attribute-value">_blank</span>"&gt;PyTorch<span class="html-tag">&lt;/a&gt;</span> and</td></tr><tr><td class="line-number" value="944"></td><td class="line-content">the <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://research.fb.com/downloads/caffe2/" rel="noreferrer noopener">https://research.fb.com/downloads/caffe2/</a>"</span></td></tr><tr><td class="line-number" value="945"></td><td class="line-content"><span class="html-attribute-name">target</span>="<span class="html-attribute-value">_blank</span>"&gt;Convolutional Architecture for Fast Feature</td></tr><tr><td class="line-number" value="946"></td><td class="line-content">Embedding<span class="html-tag">&lt;/a&gt;</span> (Caffe2). It is one of the standard libraries for deep</td></tr><tr><td class="line-number" value="947"></td><td class="line-content">neural network research and implementation.</td></tr><tr><td class="line-number" value="948"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="949"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Tensors<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="950"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="951"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>A basic data structure for holding data in PyTorch is</td></tr><tr><td class="line-number" value="952"></td><td class="line-content">a <span class="html-tag">&lt;i&gt;</span>tensor<span class="html-tag">&lt;/i&gt;</span>.  An <span class="html-tag">&lt;i&gt;</span>m&amp;nbsp;&amp;times;&amp;nbsp;n<span class="html-tag">&lt;/i&gt;</span> tensor is a</td></tr><tr><td class="line-number" value="953"></td><td class="line-content">multidimensional data structure with <span class="html-tag">&lt;i&gt;</span>n<span class="html-tag">&lt;/i&gt;</span> rows and <span class="html-tag">&lt;i&gt;</span>m<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="954"></td><td class="line-content">columns, very similar to a Numpy <span class="html-tag">&lt;tt&gt;</span>ndarray<span class="html-tag">&lt;/tt&gt;</span>. One critical</td></tr><tr><td class="line-number" value="955"></td><td class="line-content">difference between tensors and Numpy arrays is that tensors can be</td></tr><tr><td class="line-number" value="956"></td><td class="line-content">moved to the GPU for rapid processing. Below is a very simple example</td></tr><tr><td class="line-number" value="957"></td><td class="line-content">of creating a 2&amp;nbsp;&amp;times;&amp;nbsp;2 PyTorch tensor.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="958"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="959"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="960"></td><td class="line-content">&gt; import torch</td></tr><tr><td class="line-number" value="961"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="962"></td><td class="line-content">&gt; data = [ [1,2], [3,4] ]</td></tr><tr><td class="line-number" value="963"></td><td class="line-content">&gt; t = torch.tensor( data )</td></tr><tr><td class="line-number" value="964"></td><td class="line-content">&gt; print( t )</td></tr><tr><td class="line-number" value="965"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="966"></td><td class="line-content">tensor([[1, 2],</td></tr><tr><td class="line-number" value="967"></td><td class="line-content">       [3, 4]])</td></tr><tr><td class="line-number" value="968"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="969"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="970"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>If we wanted to move the tensor to the GPU, we would first check to</td></tr><tr><td class="line-number" value="971"></td><td class="line-content">ensure GPU processing is available, then use the <span class="html-tag">&lt;tt&gt;</span>to()<span class="html-tag">&lt;/tt&gt;</span> method</td></tr><tr><td class="line-number" value="972"></td><td class="line-content">to transfer the tensor from CPU memory to GPU memory.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="973"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="974"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="975"></td><td class="line-content">&gt; if torch.cuda.is_available():</td></tr><tr><td class="line-number" value="976"></td><td class="line-content">&gt;     device = torch.device( 'cuda' )</td></tr><tr><td class="line-number" value="977"></td><td class="line-content">&gt; else:</td></tr><tr><td class="line-number" value="978"></td><td class="line-content">&gt;     device = torch.device( 'cpu' )</td></tr><tr><td class="line-number" value="979"></td><td class="line-content">&gt; t.to( device )</td></tr><tr><td class="line-number" value="980"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="981"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="982"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>One important caveat is that you must do <span class="html-tag">&lt;b&gt;</span>all<span class="html-tag">&lt;/b&gt;</span> processing on</td></tr><tr><td class="line-number" value="983"></td><td class="line-content">either the CPU or the GPU. You cannot split data structures and</td></tr><tr><td class="line-number" value="984"></td><td class="line-content">processing between the two processors. So, if you move your tensors to</td></tr><tr><td class="line-number" value="985"></td><td class="line-content">the GPU, you must also move your DNN models to the GPU and ensure all</td></tr><tr><td class="line-number" value="986"></td><td class="line-content">operations are performed on the GPU.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="987"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="988"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>This is a <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>='<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/iris-dnn.ipynb" rel="noreferrer noopener">./code/iris-dnn.ipynb</a>'&gt;</span>Jupyter notebook that</td></tr><tr><td class="line-number" value="989"></td><td class="line-content">implements a DNN<span class="html-tag">&lt;/a&gt;</span> in the simplest possible manner. It is a good</td></tr><tr><td class="line-number" value="990"></td><td class="line-content">starting point for understanding what tensors are and how they can be</td></tr><tr><td class="line-number" value="991"></td><td class="line-content">created and manipulated. The DNN itself trains on six examples of</td></tr><tr><td class="line-number" value="992"></td><td class="line-content">three iris types (setosa, versicolor, virginica) using sepal and petal</td></tr><tr><td class="line-number" value="993"></td><td class="line-content">length and width.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="994"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="995"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="996"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">wheat</span>"&gt;</span>FCN Exercise 1: Wheat Seed Classification<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="997"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="998"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="999"></td><td class="line-content">  As an initial example, we will run a simple wheat seed classifier.</td></tr><tr><td class="line-number" value="1000"></td><td class="line-content">  The <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/wheat.csv" rel="noreferrer noopener">./code/wheat.csv</a>"&gt;</span>dataset<span class="html-tag">&lt;/a&gt;</span> contains properties</td></tr><tr><td class="line-number" value="1001"></td><td class="line-content">  of three types of wheat seeds. The goal is to use these properties</td></tr><tr><td class="line-number" value="1002"></td><td class="line-content">  to predict the type of wheat the seed represents. we will first</td></tr><tr><td class="line-number" value="1003"></td><td class="line-content">  demonstrate a "from scratch" DNN that uses a simple single-layer</td></tr><tr><td class="line-number" value="1004"></td><td class="line-content">  neural network to train, then predict wheat seeds.</td></tr><tr><td class="line-number" value="1005"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1006"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1007"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="1008"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin: 0 auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1009"></td><td class="line-content">    <span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1010"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Province<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1011"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Area<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1012"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Perimeter<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1013"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Compactness<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1014"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Kernel Length<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1015"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Kernel Width<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1016"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Assymetry<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1017"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Groove Length<span class="html-tag">&lt;/th&gt;</span></td></tr><tr><td class="line-number" value="1018"></td><td class="line-content">      <span class="html-tag">&lt;th&gt;</span>Type<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1019"></td><td class="line-content">    <span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1020"></td><td class="line-content">    <span class="html-tag">&lt;tr <span class="html-attribute-name">class</span>="<span class="html-attribute-value">even</span>"&gt;</span></td></tr><tr><td class="line-number" value="1021"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>Ontario<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1022"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>15.26<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1023"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>14.84<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1024"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>0.871<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1025"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>5.763<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1026"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>3.312<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1027"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>2.221<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1028"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>5.22<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1029"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>1<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1030"></td><td class="line-content">    <span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1031"></td><td class="line-content">    <span class="html-tag">&lt;tr <span class="html-attribute-name">class</span>="<span class="html-attribute-value">odd</span>"&gt;</span></td></tr><tr><td class="line-number" value="1032"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>Manitoba<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1033"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>14.88<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1034"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>14.57<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1035"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>0.8811<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1036"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>5.554<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1037"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>3.333<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1038"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>21.018/td&gt;</td></tr><tr><td class="line-number" value="1039"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>4.956<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1040"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>1<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1041"></td><td class="line-content">    <span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1042"></td><td class="line-content">    <span class="html-tag">&lt;tr <span class="html-attribute-name">class</span>="<span class="html-attribute-value">even</span>"&gt;</span></td></tr><tr><td class="line-number" value="1043"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>Nova Scotia<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1044"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>19.13<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1045"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>16.31<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1046"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>0.9035<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1047"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>6.183<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1048"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>3.902<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1049"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>2.109<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1050"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>5.924<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1051"></td><td class="line-content">      <span class="html-tag">&lt;td&gt;</span>2<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1052"></td><td class="line-content">    <span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1053"></td><td class="line-content">    <span class="html-tag">&lt;tr <span class="html-attribute-name">class</span>="<span class="html-attribute-value">odd</span>"&gt;</span></td></tr><tr><td class="line-number" value="1054"></td><td class="line-content">      <span class="html-tag">&lt;td <span class="html-attribute-name">colspan</span>="<span class="html-attribute-value">9</span>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1055"></td><td class="line-content">	$\cdots$</td></tr><tr><td class="line-number" value="1056"></td><td class="line-content">      <span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1057"></td><td class="line-content">    <span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1058"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1059"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1060"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1061"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1062"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1063"></td><td class="line-content">  The following Python-only DNN uses a simple single-layer 10-neuron</td></tr><tr><td class="line-number" value="1064"></td><td class="line-content">  DNN (ANN, actually) with a learning rate $\epsilon=0.1$, a learning</td></tr><tr><td class="line-number" value="1065"></td><td class="line-content">  rate decay of 0.01 per epoch, 1000 training iterations per epoch,</td></tr><tr><td class="line-number" value="1066"></td><td class="line-content">  a single epoch, and five-fold cross validation during testing.</td></tr><tr><td class="line-number" value="1067"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1068"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1069"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1070"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1071"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/simple-nn.ipynb" rel="noreferrer noopener">./notebook/simple-nn.ipynb</a>"&gt;</span>Python Wheat DNN, Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1072"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1073"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1074"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1075"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1076"></td><td class="line-content">  Next, we'll show the processing the same dataset, but instead of</td></tr><tr><td class="line-number" value="1077"></td><td class="line-content">  doing it in raw Python, we'll use PyTorch, Facebook's Python-based</td></tr><tr><td class="line-number" value="1078"></td><td class="line-content">  DNN library. This will demonstrate how much easier it is to use</td></tr><tr><td class="line-number" value="1079"></td><td class="line-content">  PyTorch to build a significantly more complicated DNN to train,</td></tr><tr><td class="line-number" value="1080"></td><td class="line-content">  then predict wheat types based on wheat seed properties.</td></tr><tr><td class="line-number" value="1081"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1082"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1083"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1084"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/pytorch-nn.py" rel="noreferrer noopener">./code/pytorch-nn.py</a>"&gt;</span>Pytorch Wheat DNN<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1085"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1086"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1087"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1088"></td><td class="line-content">  The purpose of these examples is, first, to show you how to code</td></tr><tr><td class="line-number" value="1089"></td><td class="line-content">  your own DNN using basic Python, then how to use one of the most</td></tr><tr><td class="line-number" value="1090"></td><td class="line-content">  popular Python libraries (TensorFlow, programmed in C, is the</td></tr><tr><td class="line-number" value="1091"></td><td class="line-content">  other candidate) to perform the same computation in a simpler to</td></tr><tr><td class="line-number" value="1092"></td><td class="line-content">  program and more sophisticated, manner.</td></tr><tr><td class="line-number" value="1093"></td><td class="line-content">  <span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1094"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1095"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">fcn-num</span>"&gt;</span>FCN Exercise 2: Handwritten Number Recognition<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1096"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1097"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>PyTorch's dataset repository includes</td></tr><tr><td class="line-number" value="1098"></td><td class="line-content">the <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noreferrer noopener">https://en.wikipedia.org/wiki/MNIST_database</a>"&gt;</span>MNIST</td></tr><tr><td class="line-number" value="1099"></td><td class="line-content">database of handwritten images<span class="html-tag">&lt;/a&gt;</span>, which</td></tr><tr><td class="line-number" value="1100"></td><td class="line-content">includes <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.python-course.eu/data/mnist/mnist_train.csv" rel="noreferrer noopener">https://www.python-course.eu/data/mnist/mnist_train.csv</a>"&gt;</span>60,000</td></tr><tr><td class="line-number" value="1101"></td><td class="line-content">training examples<span class="html-tag">&lt;/a&gt;</span> and</td></tr><tr><td class="line-number" value="1102"></td><td class="line-content"><span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.python-course.eu/data/mnist/mnist_test.csv" rel="noreferrer noopener">https://www.python-course.eu/data/mnist/mnist_test.csv</a>"&gt;</span>10,000</td></tr><tr><td class="line-number" value="1103"></td><td class="line-content">test examples<span class="html-tag">&lt;/a&gt;</span>. It is a very common dataset to use to test learning</td></tr><tr><td class="line-number" value="1104"></td><td class="line-content">or pattern recognition algorithms on real-world data, without the need</td></tr><tr><td class="line-number" value="1105"></td><td class="line-content">to hand-label a large training dataset.</td></tr><tr><td class="line-number" value="1106"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1107"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1108"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Normally, images are processed with CNNs. However, we will use a</td></tr><tr><td class="line-number" value="1109"></td><td class="line-content">simple FCN to recognize the handwritten images. This is done by</td></tr><tr><td class="line-number" value="1110"></td><td class="line-content">converting each handwritten digit image into a set of pixel values,</td></tr><tr><td class="line-number" value="1111"></td><td class="line-content">then converting that into a one-dimensional vector. This 1D vector</td></tr><tr><td class="line-number" value="1112"></td><td class="line-content">acts as input to a single hidden layer, an ReLU (rectified linear</td></tr><tr><td class="line-number" value="1113"></td><td class="line-content">unit) filter, and an output layer with ten possible classifications</td></tr><tr><td class="line-number" value="1114"></td><td class="line-content">representing the ten digits 0&amp;ndash;9. Here are two simple examples of</td></tr><tr><td class="line-number" value="1115"></td><td class="line-content">the digits in the MNIST dataset.</td></tr><tr><td class="line-number" value="1116"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1117"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1118"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="1119"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/MNIST.png" rel="noreferrer noopener">figs/MNIST.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1120"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1121"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1122"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Here is a Jupyter Notebook we will use to load the MNIST dataset,</td></tr><tr><td class="line-number" value="1123"></td><td class="line-content">construct a simple FCN, then train and test it on the MNIST data.</td></tr><tr><td class="line-number" value="1124"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1125"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1126"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1127"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/digit_ex.ipynb" rel="noreferrer noopener">./notebook/digit_ex.ipynb</a>"&gt;</span>Digits FCN Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1128"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1129"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1130"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>Even with a simple FCN with a single hidden layer and an ReLU</td></tr><tr><td class="line-number" value="1131"></td><td class="line-content">filter, five training epochs (evaluating the training dataset five</td></tr><tr><td class="line-number" value="1132"></td><td class="line-content">times) produces results of 97% or better on the test dataset.</td></tr><tr><td class="line-number" value="1133"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1134"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1135"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">fcn-img</span>"&gt;</span>FCN Exercise 3: Image Processing<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1136"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1137"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1138"></td><td class="line-content">To see how well an FCN works on real two-dimensional images, we will</td></tr><tr><td class="line-number" value="1139"></td><td class="line-content">work with the CIFAR10 dataset, also a part of PyTorch's dataset</td></tr><tr><td class="line-number" value="1140"></td><td class="line-content">repository. CIFAR10 contains 50,000 training images and 10,000 test</td></tr><tr><td class="line-number" value="1141"></td><td class="line-content">images of size $32 \times 32 \times 3$: 32 pixels wide by 32 pixels</td></tr><tr><td class="line-number" value="1142"></td><td class="line-content">high by 3 pixel components R (red), G (green), and B (blue).</td></tr><tr><td class="line-number" value="1143"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1144"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1145"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1146"></td><td class="line-content">train_dataset = dsets.CIFAR10( root='./data', train=True, transform=xforms.ToTensor(), download=True )</td></tr><tr><td class="line-number" value="1147"></td><td class="line-content">test_dataset = dsets.CIFAR10( root='./data', train=False, transform=xforms.ToTensor() )</td></tr><tr><td class="line-number" value="1148"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1149"></td><td class="line-content">classes = ( 'plane','car','bird','cat','deer','dog','frog','horse','ship','truck' )<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="1150"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1151"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1152"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1153"></td><td class="line-content">Note the <span class="html-tag">&lt;code&gt;</span>classes<span class="html-tag">&lt;/code&gt;</span> variable. This is used to convert the</td></tr><tr><td class="line-number" value="1154"></td><td class="line-content">label value for an image into a semantic text description of the class</td></tr><tr><td class="line-number" value="1155"></td><td class="line-content">it belongs to. You will probably want to define this and index into it</td></tr><tr><td class="line-number" value="1156"></td><td class="line-content">to better understand which classes the images belong to.</td></tr><tr><td class="line-number" value="1157"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1158"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1159"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1160"></td><td class="line-content">If you want to examine some of the images in the CIFAR10 dataset, you</td></tr><tr><td class="line-number" value="1161"></td><td class="line-content">can modify the code in our original FCN example. However, a simpler</td></tr><tr><td class="line-number" value="1162"></td><td class="line-content">way to do this and show more images would be as follows.</td></tr><tr><td class="line-number" value="1163"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1164"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1165"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1166"></td><td class="line-content"># Let's look at four of the images in the training dataset, and the</td></tr><tr><td class="line-number" value="1167"></td><td class="line-content"># corresponding label (which defines the object the image represents)</td></tr><tr><td class="line-number" value="1168"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1169"></td><td class="line-content"># Grab the first four images and corresponding objects they represent</td></tr><tr><td class="line-number" value="1170"></td><td class="line-content"># from the training set</td></tr><tr><td class="line-number" value="1171"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1172"></td><td class="line-content">img = []</td></tr><tr><td class="line-number" value="1173"></td><td class="line-content">val = []</td></tr><tr><td class="line-number" value="1174"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1175"></td><td class="line-content">for i in range( 4 ):</td></tr><tr><td class="line-number" value="1176"></td><td class="line-content">    im, v = train_dataset[ 20 * i ]</td></tr><tr><td class="line-number" value="1177"></td><td class="line-content">    img.append( im )</td></tr><tr><td class="line-number" value="1178"></td><td class="line-content">    val.append( v )</td></tr><tr><td class="line-number" value="1179"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1180"></td><td class="line-content">    img[ i ] = img[ i ] / 2 + 0.5</td></tr><tr><td class="line-number" value="1181"></td><td class="line-content">    img[ i ] = img[ i ].numpy()</td></tr><tr><td class="line-number" value="1182"></td><td class="line-content">    img[ i ] = img[ i ].transpose( 1, 2, 0 )</td></tr><tr><td class="line-number" value="1183"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1184"></td><td class="line-content"># Print out what objects the four images are meant to represent</td></tr><tr><td class="line-number" value="1185"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1186"></td><td class="line-content">print( ' '.join( classes[ val[ i ] ] for i in range( 4 ) ) )</td></tr><tr><td class="line-number" value="1187"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1188"></td><td class="line-content"># Create a single row of four images</td></tr><tr><td class="line-number" value="1189"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1190"></td><td class="line-content">fig, axes = plt.subplots( 1, 4, figsize=( 12, 2.5 ) )</td></tr><tr><td class="line-number" value="1191"></td><td class="line-content">for i in range( 4 ):</td></tr><tr><td class="line-number" value="1192"></td><td class="line-content">    axes[ i ].imshow( img[ i ] )<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="1193"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1194"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1195"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1196"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="1197"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/CIFAR_ex.png" rel="noreferrer noopener">figs/CIFAR_ex.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1198"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1199"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1200"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1201"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1202"></td><td class="line-content">Apart from changing any occurrences <span class="html-tag">&lt;code&gt;</span>28 * 28<span class="html-tag">&lt;/code&gt;</span> to <span class="html-tag">&lt;code&gt;</span>32</td></tr><tr><td class="line-number" value="1203"></td><td class="line-content">* 32 * 3<span class="html-tag">&lt;/code&gt;</span> (to account for the different input size), the</td></tr><tr><td class="line-number" value="1204"></td><td class="line-content">remainder of the code can be identical to the MNIST FCN. You're</td></tr><tr><td class="line-number" value="1205"></td><td class="line-content">certainly encouraged to also vary things like</td></tr><tr><td class="line-number" value="1206"></td><td class="line-content"><span class="html-tag">&lt;code&gt;</span>criteria<span class="html-tag">&lt;/code&gt;</span>, <span class="html-tag">&lt;code&gt;</span>optimizer<span class="html-tag">&lt;/code&gt;</span>, <span class="html-tag">&lt;code&gt;</span>hidden_size<span class="html-tag">&lt;/code&gt;</span>,</td></tr><tr><td class="line-number" value="1207"></td><td class="line-content">the number of hidden layers, and other properties of the FCN to try to</td></tr><tr><td class="line-number" value="1208"></td><td class="line-content">improve performance. Remember that, for ten classes, just like the</td></tr><tr><td class="line-number" value="1209"></td><td class="line-content">MNIST dataset, chance is 10%. You're unlikely to obtain accuracies</td></tr><tr><td class="line-number" value="1210"></td><td class="line-content">anywhere near the 97% we produce for the MNIST images, but you should</td></tr><tr><td class="line-number" value="1211"></td><td class="line-content">be able to do much better than 10%, even for this most simple FCN.</td></tr><tr><td class="line-number" value="1212"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1213"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1214"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1215"></td><td class="line-content">Here is a simple Jupyter Notebook that trains on the CIFAR10</td></tr><tr><td class="line-number" value="1216"></td><td class="line-content">dataset using our simple FCN.</td></tr><tr><td class="line-number" value="1217"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1218"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1219"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1220"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/CIFAR_FCN_ex.ipynb" rel="noreferrer noopener">./notebook/CIFAR_FCN_ex.ipynb</a>"&gt;</span>CIFAR FCN Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1221"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1222"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1223"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1224"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">cnn-img</span>"&gt;</span>CNN Exercise 4: Image Processing<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1225"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1226"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1227"></td><td class="line-content">  The wheat dataset is fairly easy to process, but it is probably more</td></tr><tr><td class="line-number" value="1228"></td><td class="line-content">  representative of the type of task you need to solve: classification</td></tr><tr><td class="line-number" value="1229"></td><td class="line-content">  from one or more input properties. DNNs themselves have been applied</td></tr><tr><td class="line-number" value="1230"></td><td class="line-content">  most successfully to image data using convolutional neural networks</td></tr><tr><td class="line-number" value="1231"></td><td class="line-content">  (CNNs). A CNN converts an $n \times m$ image to an $nm$ vector, then</td></tr><tr><td class="line-number" value="1232"></td><td class="line-content">  uses that vector as input into a convolution stage. Here, a</td></tr><tr><td class="line-number" value="1233"></td><td class="line-content">  collection of $k$ <span class="html-tag">&lt;i&gt;</span>kernels<span class="html-tag">&lt;/i&gt;</span> are convolved against the pixels</td></tr><tr><td class="line-number" value="1234"></td><td class="line-content">  and their immediate neighbours to produce scalar values. For each</td></tr><tr><td class="line-number" value="1235"></td><td class="line-content">  kernel, a column of $nm$ convolved values are created, one for each</td></tr><tr><td class="line-number" value="1236"></td><td class="line-content">  pixel in the image. In the simplest CNN, the column values are</td></tr><tr><td class="line-number" value="1237"></td><td class="line-content">  evaluated to produce a single, representative value. For</td></tr><tr><td class="line-number" value="1238"></td><td class="line-content">  example, <span class="html-tag">&lt;i&gt;</span>max<span class="html-tag">&lt;/i&gt;</span> scans a column and extracts the largest</td></tr><tr><td class="line-number" value="1239"></td><td class="line-content">  value. These values form a $k$-length <span class="html-tag">&lt;i&gt;</span>input vector<span class="html-tag">&lt;/i&gt;</span> to a</td></tr><tr><td class="line-number" value="1240"></td><td class="line-content">  follow-on fully connected network.  This network processes output</td></tr><tr><td class="line-number" value="1241"></td><td class="line-content">  from the CNN to generate a final classification prediction.</td></tr><tr><td class="line-number" value="1242"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1243"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1244"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1245"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">cnn</span>"&gt;</span>Convolutional Neural Networks<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1246"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1247"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1248"></td><td class="line-content">Convolutional neural networks (CNNs) extend FCNs by preceding the</td></tr><tr><td class="line-number" value="1249"></td><td class="line-content">fully connected layers with a set of convolutional layers.  CNNs are</td></tr><tr><td class="line-number" value="1250"></td><td class="line-content">commonly used to analyze images,</td></tr><tr><td class="line-number" value="1251"></td><td class="line-content">although <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://arxiv.org/pdf/1803.01271v2.pdf" rel="noreferrer noopener">https://arxiv.org/pdf/1803.01271v2.pdf</a>"&gt;</span>recent</td></tr><tr><td class="line-number" value="1252"></td><td class="line-content">research<span class="html-tag">&lt;/a&gt;</span> has shown that they can handle other data modalities like</td></tr><tr><td class="line-number" value="1253"></td><td class="line-content">text with excellent performance.</td></tr><tr><td class="line-number" value="1254"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1255"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1256"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1257"></td><td class="line-content">CNNs are made up of a number of standard operations to produce new,</td></tr><tr><td class="line-number" value="1258"></td><td class="line-content">hidden layers. These include</td></tr><tr><td class="line-number" value="1259"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1260"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1261"></td><td class="line-content"><span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="1262"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1263"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;b&gt;</span>Convolution (CONV):<span class="html-tag">&lt;/b&gt;</span> Application of a <span class="html-tag">&lt;i&gt;</span>convolve<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="1264"></td><td class="line-content">operation across a layer, producing a new layer with the convolved</td></tr><tr><td class="line-number" value="1265"></td><td class="line-content">results</td></tr><tr><td class="line-number" value="1266"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1267"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;b&gt;</span>Filter (RELU):<span class="html-tag">&lt;/b&gt;</span> Application of a filter like ReLU to values</td></tr><tr><td class="line-number" value="1268"></td><td class="line-content">in a layer, produce a new layer with filtered results</td></tr><tr><td class="line-number" value="1269"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1270"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;b&gt;</span>Pooling (POOL):<span class="html-tag">&lt;/b&gt;</span> Application of a pooling (aggregation)</td></tr><tr><td class="line-number" value="1271"></td><td class="line-content">operator to values in a layer, produce a new, smaller layer with</td></tr><tr><td class="line-number" value="1272"></td><td class="line-content">pooled results</td></tr><tr><td class="line-number" value="1273"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1274"></td><td class="line-content"><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="1275"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1276"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1277"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="1278"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/CNN-pipeline.png" rel="noreferrer noopener">figs/CNN-pipeline.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1279"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1280"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1281"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1282"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1283"></td><td class="line-content">In practice, it is common to apply a series of CONV-RELU layers,</td></tr><tr><td class="line-number" value="1284"></td><td class="line-content">follow them with a POOL layer, and repeat this pattern until an image</td></tr><tr><td class="line-number" value="1285"></td><td class="line-content">has been processed to a small size. At this point, results are formed</td></tr><tr><td class="line-number" value="1286"></td><td class="line-content">into a 1D vector which acts as input to an FCN. The FCN produces a set</td></tr><tr><td class="line-number" value="1287"></td><td class="line-content">of probabilities for each possible classification (softmax).</td></tr><tr><td class="line-number" value="1288"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1289"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1290"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Convolution<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="1291"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1292"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1293"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="1294"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/convolve.png" rel="noreferrer noopener">figs/convolve.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1295"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1296"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1297"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1298"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1299"></td><td class="line-content">Convolution combines a pixel in an image and its neighbours by placing</td></tr><tr><td class="line-number" value="1300"></td><td class="line-content">a <span class="html-tag">&lt;i&gt;</span>kernel<span class="html-tag">&lt;/i&gt;</span> of a given size centered over the pixel, then</td></tr><tr><td class="line-number" value="1301"></td><td class="line-content">multiplying the corresponding pixel and kernel values, and summing</td></tr><tr><td class="line-number" value="1302"></td><td class="line-content">them together to produce a final filtered result.</td></tr><tr><td class="line-number" value="1303"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1304"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1305"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1306"></td><td class="line-content">The example above using a 3&amp;times;3 kernel with values -1, 0, and 5 at</td></tr><tr><td class="line-number" value="1307"></td><td class="line-content">various positions within its nine cells. When we center it over the</td></tr><tr><td class="line-number" value="1308"></td><td class="line-content">pixel at the center of the purple box, multiple, and sum, we obtain a</td></tr><tr><td class="line-number" value="1309"></td><td class="line-content">final filtered value of 210 for that position in the image. Kernels</td></tr><tr><td class="line-number" value="1310"></td><td class="line-content">are designed to identify specific properties of an image, producing</td></tr><tr><td class="line-number" value="1311"></td><td class="line-content">large values when those properties are located, and small values when</td></tr><tr><td class="line-number" value="1312"></td><td class="line-content">they are not. For example, the Kirsch filter is designed to identify</td></tr><tr><td class="line-number" value="1313"></td><td class="line-content">edges in an image. Convolution of a simple animation with a Kirsch</td></tr><tr><td class="line-number" value="1314"></td><td class="line-content">filter produces the result shown below.</td></tr><tr><td class="line-number" value="1315"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1316"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1317"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1318"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1319"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/anime.png" rel="noreferrer noopener">figs/anime.png</a>"&gt;</span><span class="html-tag">&lt;/figs&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1320"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>" <span class="html-attribute-name">rowspan</span>="<span class="html-attribute-value">2</span>"&gt;</span></td></tr><tr><td class="line-number" value="1321"></td><td class="line-content">    <span class="html-tag">&lt;table&gt;</span></td></tr><tr><td class="line-number" value="1322"></td><td class="line-content">    <span class="html-tag">&lt;tr&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>5<span class="html-tag">&lt;/td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>5<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;td&gt;</span>5<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1323"></td><td class="line-content">    <span class="html-tag">&lt;tr&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>-3<span class="html-tag">&lt;/td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>0<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;td&gt;</span>-3<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1324"></td><td class="line-content">    <span class="html-tag">&lt;tr&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>-3<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span>-3<span class="html-tag">&lt;/td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center;</span>"&gt;</span><span class="html-tag">&lt;td&gt;</span>-3<span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1325"></td><td class="line-content">    <span class="html-tag">&lt;tr&gt;</span><span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; border-width: 0px;</span>" <span class="html-attribute-name">colspan</span>="<span class="html-attribute-value">3</span>"&gt;</span><span class="html-tag">&lt;span <span class="html-attribute-name">style</span>="<span class="html-attribute-value">white-space: nowrap; font-size: 0.75em;</span>"&gt;</span>Kirsch kernel<span class="html-tag">&lt;/span&gt;</span><span class="html-tag">&lt;/td&gt;</span><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1326"></td><td class="line-content">    <span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1327"></td><td class="line-content">  <span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1328"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1329"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1330"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/anime-edge.png" rel="noreferrer noopener">figs/anime-edge.png</a>"&gt;</span><span class="html-tag">&lt;/figs&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1331"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1332"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1333"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1334"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1335"></td><td class="line-content">Normally, we use numerous kernels to identify different properties</td></tr><tr><td class="line-number" value="1336"></td><td class="line-content">or <span class="html-tag">&lt;i&gt;</span>features<span class="html-tag">&lt;/i&gt;</span> in an image. Each kernel produces a <span class="html-tag">&lt;i&gt;</span>feature map<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="1337"></td><td class="line-content">from the image. These feature maps are normally stacked one on top of</td></tr><tr><td class="line-number" value="1338"></td><td class="line-content">another, producing a result with width and height one less than the</td></tr><tr><td class="line-number" value="1339"></td><td class="line-content">original image size, and depth equal to the number of kernels applied</td></tr><tr><td class="line-number" value="1340"></td><td class="line-content">to the image. How does the CNN decide on the kernel values for each</td></tr><tr><td class="line-number" value="1341"></td><td class="line-content">kernel? This occurs during training. Kernel values are initially random,</td></tr><tr><td class="line-number" value="1342"></td><td class="line-content">and slowly converge along with edge weights during backpropagation to</td></tr><tr><td class="line-number" value="1343"></td><td class="line-content">identify image properties salient to classifying the images.</td></tr><tr><td class="line-number" value="1344"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1345"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1346"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Filtering<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="1347"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1348"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1349"></td><td class="line-content">Filtering adjusts values in a feature map, for example, by normalizing</td></tr><tr><td class="line-number" value="1350"></td><td class="line-content">them, or by removing negative values. The common ReLU filter, for</td></tr><tr><td class="line-number" value="1351"></td><td class="line-content">example, removes negative values and retains positive values.</td></tr><tr><td class="line-number" value="1352"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1353"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1354"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1355"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px; font-size: 0.75em;</span>" <span class="html-attribute-name">rowspan</span>="<span class="html-attribute-value">2</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/filter-img.png" rel="noreferrer noopener">figs/filter-img.png</a>"&gt;</span><span class="html-tag">&lt;/figs&gt;</span><span class="html-tag">&lt;br&gt;</span>Original<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1356"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; border-width: 0px; font-size: 0.75em;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/filter-orig.png" rel="noreferrer noopener">figs/filter-orig.png</a>"&gt;</span><span class="html-tag">&lt;/figs&gt;</span><span class="html-tag">&lt;br&gt;</span>Filtered<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1357"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1358"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1359"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; border-width: 0px; font-size: 0.75em;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/filter-relu.png" rel="noreferrer noopener">figs/filter-relu.png</a>"&gt;</span><span class="html-tag">&lt;/figs&gt;</span><span class="html-tag">&lt;br&gt;</span>Filtered w/ReLU<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1360"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1361"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1362"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1363"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Pooling<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="1364"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1365"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1366"></td><td class="line-content">Pooling takes a feature map, and reduces its size by aggregating block</td></tr><tr><td class="line-number" value="1367"></td><td class="line-content">of values. Aggregation can use any common mathematical operator like</td></tr><tr><td class="line-number" value="1368"></td><td class="line-content">average, median, maximum or minimum. Pooling uses a <span class="html-tag">&lt;i&gt;</span>window</td></tr><tr><td class="line-number" value="1369"></td><td class="line-content">size<span class="html-tag">&lt;/i&gt;</span> which defines its width and height, and a <span class="html-tag">&lt;i&gt;</span>stride<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="1370"></td><td class="line-content">which defines the step size it uses as it slides over the feature</td></tr><tr><td class="line-number" value="1371"></td><td class="line-content">map values it pools.</td></tr><tr><td class="line-number" value="1372"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1373"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1374"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1375"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1376"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1377"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/maxpool-matrix-L.png" rel="noreferrer noopener">figs/maxpool-matrix-L.png</a>"&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1378"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px; font-size: 0.75em;</span>"&gt;</span>max pooling<span class="html-tag">&lt;br&gt;</span>&amp;rarr;&amp;nbsp;&amp;nbsp; 2&amp;times;2 window &amp;nbsp;&amp;nbsp;&amp;rarr;<span class="html-tag">&lt;br&gt;</span>stride 2<span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1379"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/maxpool-matrix-R.png" rel="noreferrer noopener">figs/maxpool-matrix-R.png</a>"&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1380"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1381"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1382"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1383"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1384"></td><td class="line-content"><span class="html-tag">&lt;h2&gt;</span>Images<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1385"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1386"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1387"></td><td class="line-content">Recall in the previous discussion of FCNs, we used the MNIST</td></tr><tr><td class="line-number" value="1388"></td><td class="line-content">handwriting image dataset. We are now switching to the CIFAR10</td></tr><tr><td class="line-number" value="1389"></td><td class="line-content">dataset of photographic images. In your exercise, you were</td></tr><tr><td class="line-number" value="1390"></td><td class="line-content">asked to re-purpose the FCN to handle images from CIFAR10.</td></tr><tr><td class="line-number" value="1391"></td><td class="line-content">Here is a simple Jupyter Notebook that does that.</td></tr><tr><td class="line-number" value="1392"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1393"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1394"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/CIFAR_FCN_ex.ipynb" rel="noreferrer noopener">./notebook/CIFAR_FCN_ex.ipynb</a>"&gt;</span>CIFAR FCN Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1395"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1396"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1397"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1398"></td><td class="line-content">Although this produces results better than chance, we can further</td></tr><tr><td class="line-number" value="1399"></td><td class="line-content">improve performance by using a simple CNN.</td></tr><tr><td class="line-number" value="1400"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1401"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1402"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: 1.25in;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1403"></td><td class="line-content">  <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/CIFAR_CNN_ex.ipynb" rel="noreferrer noopener">./notebook/CIFAR_CNN_ex.ipynb</a>"&gt;</span>CIFAR CNN Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="1404"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1405"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1406"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1407"></td><td class="line-content">Results for the CNN are indeed better than the FCN, but only by a few</td></tr><tr><td class="line-number" value="1408"></td><td class="line-content">percentage points. This suggests the CNN could be improved possibly</td></tr><tr><td class="line-number" value="1409"></td><td class="line-content">significantly, by expanding the CONV/RELU/POOL part of the image to</td></tr><tr><td class="line-number" value="1410"></td><td class="line-content">better capture the features needed to differentiate different image</td></tr><tr><td class="line-number" value="1411"></td><td class="line-content">classes from one another. One clue is in the individual class</td></tr><tr><td class="line-number" value="1412"></td><td class="line-content">accuracies, which show that natural images like cats and deer are</td></tr><tr><td class="line-number" value="1413"></td><td class="line-content">being labeled much less accurately than man-made images like cars</td></tr><tr><td class="line-number" value="1414"></td><td class="line-content">and trucks.</td></tr><tr><td class="line-number" value="1415"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1416"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1417"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1418"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">rnn</span>"&gt;</span>Recurrent Neural Networks<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1419"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1420"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1421"></td><td class="line-content">Recurrent neural networks (RNNs) are normally used to handle</td></tr><tr><td class="line-number" value="1422"></td><td class="line-content">sequence-based data $I= \{ i_1, \ldots, i_n \}$. In simple terms, a</td></tr><tr><td class="line-number" value="1423"></td><td class="line-content">basic DNN is designed. The first sample in the input sequence $i_1$ is</td></tr><tr><td class="line-number" value="1424"></td><td class="line-content">fed into the DNN, producing both an output $o_1$ and a hidden output</td></tr><tr><td class="line-number" value="1425"></td><td class="line-content">$h_1$, $o1 = h1$. The output can be fed into a standard FCN is the</td></tr><tr><td class="line-number" value="1426"></td><td class="line-content">user wants to use it for classification. The hidden output $h_1$ is</td></tr><tr><td class="line-number" value="1427"></td><td class="line-content">combined with the next sample in the input sequence $i_2$, and this</td></tr><tr><td class="line-number" value="1428"></td><td class="line-content">pair $(h_1,i_2)$ is used as input into the DNN. This produces another</td></tr><tr><td class="line-number" value="1429"></td><td class="line-content">output $o_2$ and hidden output $h_2$, $o_2 = h_2$. The process</td></tr><tr><td class="line-number" value="1430"></td><td class="line-content">continues until all samples in the input sequence are processed,</td></tr><tr><td class="line-number" value="1431"></td><td class="line-content">producing a final output $o_n$ and hidden output $h_n$. In other</td></tr><tr><td class="line-number" value="1432"></td><td class="line-content">words, a single DNN processes input samples and hidden output from the</td></tr><tr><td class="line-number" value="1433"></td><td class="line-content">previous step <span class="html-tag">&lt;i&gt;</span>recursively<span class="html-tag">&lt;/i&gt;</span> or <span class="html-tag">&lt;i&gt;</span>recurrently<span class="html-tag">&lt;/i&gt;</span>, generating a</td></tr><tr><td class="line-number" value="1434"></td><td class="line-content">result that represent both the output from the current RNN step and a</td></tr><tr><td class="line-number" value="1435"></td><td class="line-content">hidden output for the next RNN step.</td></tr><tr><td class="line-number" value="1436"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1437"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1438"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1439"></td><td class="line-content">Note that one detail was left unspecified: if the DNN expects a</td></tr><tr><td class="line-number" value="1440"></td><td class="line-content">sample input and previous hidden output pair as input, what is the</td></tr><tr><td class="line-number" value="1441"></td><td class="line-content">hidden output for the first step in the recursion? Normally, a random</td></tr><tr><td class="line-number" value="1442"></td><td class="line-content">hidden output $h_0$ is generated and used for the first processing</td></tr><tr><td class="line-number" value="1443"></td><td class="line-content">step in an RNN.</td></tr><tr><td class="line-number" value="1444"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1445"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1446"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1447"></td><td class="line-content">Visually, these images from Michael Phi's</td></tr><tr><td class="line-number" value="1448"></td><td class="line-content">excellent <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noreferrer noopener">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>"&gt;</span>page</td></tr><tr><td class="line-number" value="1449"></td><td class="line-content">on LSTMs and GRUs<span class="html-tag">&lt;/a&gt;</span> shows clearly how input samples $i_j$ from the</td></tr><tr><td class="line-number" value="1450"></td><td class="line-content">input sequence $I$ are fed into the RNN's DNN structure one-by-one.</td></tr><tr><td class="line-number" value="1451"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1452"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1453"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1454"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">style</span>="<span class="html-attribute-value">max-width: 80%;</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/RNN-01.gif" rel="noreferrer noopener">figs/RNN-01.gif</a>"&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1455"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1456"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1457"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width:</span></span></td></tr><tr><td class="line-number" value="1458"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  0px; font-size: 0.75em;</span>"&gt;</span>Processing input samples one-by-one from</td></tr><tr><td class="line-number" value="1459"></td><td class="line-content">  input sequence $I$</td></tr><tr><td class="line-number" value="1460"></td><td class="line-content">  (attribution: <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noreferrer noopener">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>"&gt;</span>Michael</td></tr><tr><td class="line-number" value="1461"></td><td class="line-content">  Phi)<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1462"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1463"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1464"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1465"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1466"></td><td class="line-content">This close-up image shows how the hidden output $h_{j-1}$ and the</td></tr><tr><td class="line-number" value="1467"></td><td class="line-content">current input $i_j$ are combined and passed through a tanh function to</td></tr><tr><td class="line-number" value="1468"></td><td class="line-content">produce a continuous output $o_j$ and hidden output value $h_j$.</td></tr><tr><td class="line-number" value="1469"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1470"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1471"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1472"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1473"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">style</span>="<span class="html-attribute-value">max-width: 80%;</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/RNN-02.gif" rel="noreferrer noopener">figs/RNN-02.gif</a>"&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1474"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1475"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1476"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width:</span></span></td></tr><tr><td class="line-number" value="1477"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  0px; font-size: 0.75em;</span>"&gt;</span>A recurrent step combines previous hidden</td></tr><tr><td class="line-number" value="1478"></td><td class="line-content">  input $h_{j-1}$ and current input $i_j$, then passes the combination</td></tr><tr><td class="line-number" value="1479"></td><td class="line-content">  through a tanh function to produce a continuous output $o_j$ and</td></tr><tr><td class="line-number" value="1480"></td><td class="line-content">  hidden output $h_j$ from the DNN</td></tr><tr><td class="line-number" value="1481"></td><td class="line-content">  (attribution: <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noreferrer noopener">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>"&gt;</span>Michael</td></tr><tr><td class="line-number" value="1482"></td><td class="line-content">  Phi)<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1483"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1484"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1485"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1486"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1487"></td><td class="line-content">One way of understanding RNNs is that they use the concept</td></tr><tr><td class="line-number" value="1488"></td><td class="line-content">of <span class="html-tag">&lt;i&gt;</span>sequence memory<span class="html-tag">&lt;/i&gt;</span>. Over time, they have the ability to "learn"</td></tr><tr><td class="line-number" value="1489"></td><td class="line-content">sequential patterns. In theory, this is something an FCN or CNN would</td></tr><tr><td class="line-number" value="1490"></td><td class="line-content">have more difficulty to do, since an RNN uses previous information</td></tr><tr><td class="line-number" value="1491"></td><td class="line-content">(hidden output) where FCNs and CNNs do not. Programatically, this can</td></tr><tr><td class="line-number" value="1492"></td><td class="line-content">be expressed in the following way.</td></tr><tr><td class="line-number" value="1493"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1494"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1495"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1496"></td><td class="line-content">rnn = RNN()</td></tr><tr><td class="line-number" value="1497"></td><td class="line-content">fcn = FeedForwardDNN()</td></tr><tr><td class="line-number" value="1498"></td><td class="line-content">hidden = random()</td></tr><tr><td class="line-number" value="1499"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1500"></td><td class="line-content">for i in input:</td></tr><tr><td class="line-number" value="1501"></td><td class="line-content">	output,hidden = rnn( i, hidden )</td></tr><tr><td class="line-number" value="1502"></td><td class="line-content">	prediction = fcn( output )<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="1503"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1504"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1505"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1506"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Vanishing Gradients<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="1507"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1508"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1509"></td><td class="line-content">Although basic RNNs are designed to "remember" previous results, they</td></tr><tr><td class="line-number" value="1510"></td><td class="line-content">do have an important problem. As an RNN processes inputs, it begins to</td></tr><tr><td class="line-number" value="1511"></td><td class="line-content">forget what it has seen in previous steps. Intuitively, you could say</td></tr><tr><td class="line-number" value="1512"></td><td class="line-content">that a basic RNN has a short-term memory, but not a long-term memory.</td></tr><tr><td class="line-number" value="1513"></td><td class="line-content">This happens because of the way backpropegation occurs. To understand</td></tr><tr><td class="line-number" value="1514"></td><td class="line-content">this, think of how any DNN works. There are three major steps: (1)</td></tr><tr><td class="line-number" value="1515"></td><td class="line-content">feedforward; (2) error based on predicted class; (3) backpropegation</td></tr><tr><td class="line-number" value="1516"></td><td class="line-content">of error based on gradient descent to adjust edge weights. At any</td></tr><tr><td class="line-number" value="1517"></td><td class="line-content">given layer in the DNN, its gradient values depend on the successive</td></tr><tr><td class="line-number" value="1518"></td><td class="line-content">layer's gradient values. So, if the following layer has small gradient</td></tr><tr><td class="line-number" value="1519"></td><td class="line-content">values, the current layer's gradients will be even smaller. This is</td></tr><tr><td class="line-number" value="1520"></td><td class="line-content">the vanishing gradient effect. Layers near the beginning of the DNN</td></tr><tr><td class="line-number" value="1521"></td><td class="line-content">are learning little or nothing.</td></tr><tr><td class="line-number" value="1522"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1523"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1524"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1525"></td><td class="line-content">Consider an RNN, where each step in the RNN is analogous to a layer in</td></tr><tr><td class="line-number" value="1526"></td><td class="line-content">a DNN. A method similar to backpropegation (backpropegation over time)</td></tr><tr><td class="line-number" value="1527"></td><td class="line-content">is used to improve the network, so the vanishing gradient effect</td></tr><tr><td class="line-number" value="1528"></td><td class="line-content">occurs over steps in the RNN, as opposed to layers in a DNN.  This</td></tr><tr><td class="line-number" value="1529"></td><td class="line-content">explains intuitively why an RNN can remember recent timesteps, but not</td></tr><tr><td class="line-number" value="1530"></td><td class="line-content">ones further back in time.</td></tr><tr><td class="line-number" value="1531"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1532"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1533"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1534"></td><td class="line-content">What is the practical effect of the lack of long-term memory? Consider</td></tr><tr><td class="line-number" value="1535"></td><td class="line-content">an RNN processing a sentence term by term.</td></tr><tr><td class="line-number" value="1536"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1537"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1538"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1539"></td><td class="line-content">My dog Goro and I went for a walk, but a squirrel ran in front of us and he started chasing it.<span class="html-tag">&lt;br&gt;</span></td></tr><tr><td class="line-number" value="1540"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1541"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1542"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1543"></td><td class="line-content">It's easy for us to recognize that "he started chasing it" refers to</td></tr><tr><td class="line-number" value="1544"></td><td class="line-content">Goro. But an RNN may no longer remember that "he" refers to a dog named</td></tr><tr><td class="line-number" value="1545"></td><td class="line-content">Goro. This is a serious disadvantage for a type of DNN specifically</td></tr><tr><td class="line-number" value="1546"></td><td class="line-content">designed to process sequence data.</td></tr><tr><td class="line-number" value="1547"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1548"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1549"></td><td class="line-content"><span class="html-tag">&lt;h2&gt;</span>Long Short-Term Memory RNNs<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="1550"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1551"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1552"></td><td class="line-content">Long short-term memory (LSTM) DNNs are a type of RNN designed to</td></tr><tr><td class="line-number" value="1553"></td><td class="line-content">address the vanishing gradient problem. Initially introduced by</td></tr><tr><td class="line-number" value="1554"></td><td class="line-content">Hochreiter &amp;amp; Schmidhuber in 1977, LSTMs use a set of three "gates" to</td></tr><tr><td class="line-number" value="1555"></td><td class="line-content">maintain both a long-term and a short-term memory.  Below is an image</td></tr><tr><td class="line-number" value="1556"></td><td class="line-content">of the internals of an LSTM.</td></tr><tr><td class="line-number" value="1557"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1558"></td><td class="line-content"><span class="html-tag">&lt;table <span class="html-attribute-name">style</span>="<span class="html-attribute-value">margin-left: auto; margin-right: auto; text-align: center;</span>"&gt;</span></td></tr><tr><td class="line-number" value="1559"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1560"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width: 0px;</span>"&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">style</span>="<span class="html-attribute-value">max-width: 80%;</span>" <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/lstm-01.png" rel="noreferrer noopener">figs/lstm-01.png</a>"&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1561"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1562"></td><td class="line-content"><span class="html-tag">&lt;tr&gt;</span></td></tr><tr><td class="line-number" value="1563"></td><td class="line-content">  <span class="html-tag">&lt;td <span class="html-attribute-name">style</span>="<span class="html-attribute-value">text-align: center; vertical-align: middle; border-width:</span></span></td></tr><tr><td class="line-number" value="1564"></td><td class="line-content"><span class="html-tag"><span class="html-attribute-value">  0px; font-size: 0.75em;</span>"&gt;</span>The gates, activation functions, and data</td></tr><tr><td class="line-number" value="1565"></td><td class="line-content">  flow through the DNNs used in LSTMs</td></tr><tr><td class="line-number" value="1566"></td><td class="line-content">  (attribution: <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noreferrer noopener">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>"&gt;</span>Michael</td></tr><tr><td class="line-number" value="1567"></td><td class="line-content">  Phi)<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/td&gt;</span></td></tr><tr><td class="line-number" value="1568"></td><td class="line-content"><span class="html-tag">&lt;/tr&gt;</span></td></tr><tr><td class="line-number" value="1569"></td><td class="line-content"><span class="html-tag">&lt;/table&gt;</span></td></tr><tr><td class="line-number" value="1570"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1571"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1572"></td><td class="line-content">To understand what happens internally within the LSTM, we can look at</td></tr><tr><td class="line-number" value="1573"></td><td class="line-content">the various data paths. An LSTM is made up of three pieces of</td></tr><tr><td class="line-number" value="1574"></td><td class="line-content">information: the current input value from the sequence $i_j$, the</td></tr><tr><td class="line-number" value="1575"></td><td class="line-content">previous hidden state $h_{j-1}$, and the previous <span class="html-tag">&lt;i&gt;</span>cell state<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="1576"></td><td class="line-content">$c_{j-1}$. Each iteration of an LSTM RNN produces a new hidden state</td></tr><tr><td class="line-number" value="1577"></td><td class="line-content">$h_j$, and new cell state $c_j$, and an output $o_j$, with $h_j = o_j$</td></tr><tr><td class="line-number" value="1578"></td><td class="line-content">as before. Intuitively, we think of the hidden state as our short-term</td></tr><tr><td class="line-number" value="1579"></td><td class="line-content">memory, and the cell state as our long-term memory.</td></tr><tr><td class="line-number" value="1580"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1581"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1582"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1583"></td><td class="line-content">The path along the top manages the cell state $c_{j-1}$. As noted</td></tr><tr><td class="line-number" value="1584"></td><td class="line-content">above, the cell state is the long-term "memory" of the network over</td></tr><tr><td class="line-number" value="1585"></td><td class="line-content">the sequence processed to date. We must decide what to remove or</td></tr><tr><td class="line-number" value="1586"></td><td class="line-content">"forget" from the cell state. Not surprisingly, this is called</td></tr><tr><td class="line-number" value="1587"></td><td class="line-content">the <span class="html-tag">&lt;i&gt;</span>forget gate layer<span class="html-tag">&lt;/i&gt;</span>. The previous hidden state $h_{j-1}$ and</td></tr><tr><td class="line-number" value="1588"></td><td class="line-content">current input $i_j$ are used to control this decision. Mathematically,</td></tr><tr><td class="line-number" value="1589"></td><td class="line-content">the output combines the hidden and input values, then passes them</td></tr><tr><td class="line-number" value="1590"></td><td class="line-content">through a sigmoid operator.</td></tr><tr><td class="line-number" value="1591"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1592"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1593"></td><td class="line-content">\[ f = \sigma ( W_f \cdot [ h_{j-1},i_j ] + b_f ) \]</td></tr><tr><td class="line-number" value="1594"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1595"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1596"></td><td class="line-content">The previous hidden state and current input value $[ h_{j-1},i_j ]$</td></tr><tr><td class="line-number" value="1597"></td><td class="line-content">are multiplied by a weight in $W_f$ and combined with a bias in $b_f$,</td></tr><tr><td class="line-number" value="1598"></td><td class="line-content">then passed through the $\sigma$ function to transform the result to</td></tr><tr><td class="line-number" value="1599"></td><td class="line-content">the continuous range $[0 \ldots 1]$. A value close to 0 means</td></tr><tr><td class="line-number" value="1600"></td><td class="line-content">completely forget, and a value close to 1 means fully retain. Remember</td></tr><tr><td class="line-number" value="1601"></td><td class="line-content">that we are (normally) working with vectors $h_{j-1}$, $i_j$, $W_f$,</td></tr><tr><td class="line-number" value="1602"></td><td class="line-content">and $b_f$. The resulting vector $f$ is pointwise-multiplied to the</td></tr><tr><td class="line-number" value="1603"></td><td class="line-content">previous cell state $c_{j-1}$ using the elementwise product operator</td></tr><tr><td class="line-number" value="1604"></td><td class="line-content">$\odot$ to alter its values. At this point it should be clear why</td></tr><tr><td class="line-number" value="1605"></td><td class="line-content">values close to 0 or close to 1 remove or retain information in</td></tr><tr><td class="line-number" value="1606"></td><td class="line-content">$c_{j-1}$.</td></tr><tr><td class="line-number" value="1607"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1608"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1609"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1610"></td><td class="line-content">Next comes the <span class="html-tag">&lt;i&gt;</span>input gate<span class="html-tag">&lt;/i&gt;</span>, which decides what new information</td></tr><tr><td class="line-number" value="1611"></td><td class="line-content">will be stored in long-term memory $c_j$. As with the forget gate, it</td></tr><tr><td class="line-number" value="1612"></td><td class="line-content">combines the previous hidden state $h_{j-1}$ and the current input</td></tr><tr><td class="line-number" value="1613"></td><td class="line-content">value $i_j$ to make this decision. This is made up of two parts $i_1$</td></tr><tr><td class="line-number" value="1614"></td><td class="line-content">and $i_2$.</td></tr><tr><td class="line-number" value="1615"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1616"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="1617"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="1618"></td><td class="line-content">i_1 &amp; = \sigma( W_{i_1} \cdot [ h_{j-1}, i_{j} ] + b_{i_1} )\\</td></tr><tr><td class="line-number" value="1619"></td><td class="line-content">i_2 &amp; = \textrm{tanh}( W_{i_2} \cdot [ h_{j-1}, i_{j} ] + b_{i_2} )\\</td></tr><tr><td class="line-number" value="1620"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="1621"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="1622"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1623"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1624"></td><td class="line-content">Intuitively, $i_1$ dictates what information "passes through" to the</td></tr><tr><td class="line-number" value="1625"></td><td class="line-content">cell state, again with 0 indicating no pass-thru, and 1 indicating</td></tr><tr><td class="line-number" value="1626"></td><td class="line-content">complete pass-thru. $i_2$ is more difficult to explain intuitively,</td></tr><tr><td class="line-number" value="1627"></td><td class="line-content">but its purpose is to regulate the network. $i_1$ and $i_2$ are</td></tr><tr><td class="line-number" value="1628"></td><td class="line-content">multiplied, and the result is pointwise-added to the cell state after</td></tr><tr><td class="line-number" value="1629"></td><td class="line-content">the forget gate is applied. Again, at this point it should be clear</td></tr><tr><td class="line-number" value="1630"></td><td class="line-content">how values on the range $[0 \ldots 1]$ are affecting what new</td></tr><tr><td class="line-number" value="1631"></td><td class="line-content">information is being added into long-term memory.</td></tr><tr><td class="line-number" value="1632"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1633"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1634"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1635"></td><td class="line-content">The final stage is the <span class="html-tag">&lt;i&gt;</span>output gate<span class="html-tag">&lt;/i&gt;</span>, which combines all three</td></tr><tr><td class="line-number" value="1636"></td><td class="line-content">pieces of information: the previous hidden state $h_{j-1}$ (short-term</td></tr><tr><td class="line-number" value="1637"></td><td class="line-content">memory), the current input value $i_j$ (new input), and the new cell</td></tr><tr><td class="line-number" value="1638"></td><td class="line-content">state after the forget and input gates are applied, $c_j$ (long-term</td></tr><tr><td class="line-number" value="1639"></td><td class="line-content">memory) using two parts $o_1$ representing a combination of the</td></tr><tr><td class="line-number" value="1640"></td><td class="line-content">previous hidden state and the current input value and $o_2$</td></tr><tr><td class="line-number" value="1641"></td><td class="line-content">representing long-term memory.</td></tr><tr><td class="line-number" value="1642"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1643"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1644"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="1645"></td><td class="line-content">\begin{align}</td></tr><tr><td class="line-number" value="1646"></td><td class="line-content">o_1 &amp;= \sigma( W_{o_1} \cdot [ h_{j-1}, i_{j} ] + b_{o_1} )\\</td></tr><tr><td class="line-number" value="1647"></td><td class="line-content">o_2 &amp;= \textrm{tanh}( W_{o_2} \cdot c_j + b_{o_2} )\\</td></tr><tr><td class="line-number" value="1648"></td><td class="line-content">h_j, o_j &amp;= o_1 \odot o_2\\</td></tr><tr><td class="line-number" value="1649"></td><td class="line-content">\end{align}</td></tr><tr><td class="line-number" value="1650"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="1651"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1652"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1653"></td><td class="line-content">The new short-term memory $o_1$ and long-term memory $o_2$ are</td></tr><tr><td class="line-number" value="1654"></td><td class="line-content">pointwise-multiplied to produce the new hidden state and output $h_j$</td></tr><tr><td class="line-number" value="1655"></td><td class="line-content">and $o_j$ for the current sequence entry.</td></tr><tr><td class="line-number" value="1656"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1657"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1658"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1659"></td><td class="line-content">As with all DNNs, the key on how the network functions lies in its</td></tr><tr><td class="line-number" value="1660"></td><td class="line-content">weights and biases $W_f$, $b_f$, $W_{i_1}$, $b_{i_1}$, $W_{i_2}$,</td></tr><tr><td class="line-number" value="1661"></td><td class="line-content">$b_{i_2}$, $W_{o_1}$, $b_{o_1}$, $W_{o_2}$, and $b_{o_2}$. And as with</td></tr><tr><td class="line-number" value="1662"></td><td class="line-content">all DNNs, these values are updated during each step of DNN training by</td></tr><tr><td class="line-number" value="1663"></td><td class="line-content">using gradient descent and optimization to adjust the weights and</td></tr><tr><td class="line-number" value="1664"></td><td class="line-content">biases in directions that produce results with a lower loss or error.</td></tr><tr><td class="line-number" value="1665"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1666"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1667"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1668"></td><td class="line-content">One final note about LSTMs is that they generally come in two forms:</td></tr><tr><td class="line-number" value="1669"></td><td class="line-content"><span class="html-tag">&lt;i&gt;</span>many-to-many<span class="html-tag">&lt;/i&gt;</span> or <span class="html-tag">&lt;i&gt;</span>many-to-one<span class="html-tag">&lt;/i&gt;</span>. The difference is in</td></tr><tr><td class="line-number" value="1670"></td><td class="line-content">whether we care about intermediate output values, or only the final</td></tr><tr><td class="line-number" value="1671"></td><td class="line-content">output value produced after the last input $i_n$ in the sequence is</td></tr><tr><td class="line-number" value="1672"></td><td class="line-content">processed.  For example, if we are performing text generation, we</td></tr><tr><td class="line-number" value="1673"></td><td class="line-content">would use a many-to-many approach. The output from each step would be</td></tr><tr><td class="line-number" value="1674"></td><td class="line-content">fed through an FCN to produce a generated term, appended to the terms</td></tr><tr><td class="line-number" value="1675"></td><td class="line-content">to date as we build up a sentence related to the input being</td></tr><tr><td class="line-number" value="1676"></td><td class="line-content">processed. If we were performing sentiment analysis, we would most</td></tr><tr><td class="line-number" value="1677"></td><td class="line-content">likely use a many-to-one approach, where we ignored intermediate</td></tr><tr><td class="line-number" value="1678"></td><td class="line-content">outputs and passed only the final output through an FCN to determine a</td></tr><tr><td class="line-number" value="1679"></td><td class="line-content">sentiment for the input sequence as a whole.</td></tr><tr><td class="line-number" value="1680"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1681"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1682"></td><td class="line-content"><span class="html-tag">&lt;h3&gt;</span>Airline Passenger Example<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="1683"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1684"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1685"></td><td class="line-content">As a "simple" example of an LSTM, we'll use a built-in dataset from</td></tr><tr><td class="line-number" value="1686"></td><td class="line-content"><span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://seaborn.pydata.org/" rel="noreferrer noopener">https://seaborn.pydata.org</a>" <span class="html-attribute-name">target</span>="<span class="html-attribute-value">_blank</span>"&gt;</span>seaborn<span class="html-tag">&lt;/a&gt;</span>, a</td></tr><tr><td class="line-number" value="1687"></td><td class="line-content">Python statistical graphics and visualization library. seaborn</td></tr><tr><td class="line-number" value="1688"></td><td class="line-content">includes a airline flight dataset with 144 months of data that</td></tr><tr><td class="line-number" value="1689"></td><td class="line-content">includes, among other things, the number of passengers that flew each</td></tr><tr><td class="line-number" value="1690"></td><td class="line-content">month.  We will use an LSTM RNN to train on twelve months (one year)</td></tr><tr><td class="line-number" value="1691"></td><td class="line-content">of passenger data, then use the resulting model to predict the number</td></tr><tr><td class="line-number" value="1692"></td><td class="line-content">of passengers flying the month immediately following our twelve month</td></tr><tr><td class="line-number" value="1693"></td><td class="line-content">training period.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1694"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1695"></td><td class="line-content"><span class="html-tag">&lt;ul&gt;</span></td></tr><tr><td class="line-number" value="1696"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/flight.ipynb" rel="noreferrer noopener">./notebook/flight.ipynb</a>"&gt;</span>Flight LSTM Jupyter Notebook<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="1697"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/flight.py" rel="noreferrer noopener">./code/flight.py</a>"&gt;</span>PyTorch Flight LSTM<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="1698"></td><td class="line-content"><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="1699"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1700"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1701"></td><td class="line-content">import seaborn as sns</td></tr><tr><td class="line-number" value="1702"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1703"></td><td class="line-content">flight_data = sns.load_dataset( 'flights' )</td></tr><tr><td class="line-number" value="1704"></td><td class="line-content">print( flight_data.head() )</td></tr><tr><td class="line-number" value="1705"></td><td class="line-content">print( len( flight_data ) )</td></tr><tr><td class="line-number" value="1706"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1707"></td><td class="line-content">  year    month  passengers</td></tr><tr><td class="line-number" value="1708"></td><td class="line-content">0 1949  January         112</td></tr><tr><td class="line-number" value="1709"></td><td class="line-content">1 1949 February         118</td></tr><tr><td class="line-number" value="1710"></td><td class="line-content">2 1949    March         132</td></tr><tr><td class="line-number" value="1711"></td><td class="line-content">3 1949    April         129</td></tr><tr><td class="line-number" value="1712"></td><td class="line-content">4 1949      May         121</td></tr><tr><td class="line-number" value="1713"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1714"></td><td class="line-content">144</td></tr><tr><td class="line-number" value="1715"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1716"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1717"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1718"></td><td class="line-content">To start, we load the libraries we will use in our program.</td></tr><tr><td class="line-number" value="1719"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1720"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1721"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1722"></td><td class="line-content">import matplotlib.pyplot as plt</td></tr><tr><td class="line-number" value="1723"></td><td class="line-content">import numpy as np</td></tr><tr><td class="line-number" value="1724"></td><td class="line-content">import pandas as pd</td></tr><tr><td class="line-number" value="1725"></td><td class="line-content">import seaborn as sns</td></tr><tr><td class="line-number" value="1726"></td><td class="line-content">import torch</td></tr><tr><td class="line-number" value="1727"></td><td class="line-content">import torch.nn as nn</td></tr><tr><td class="line-number" value="1728"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1729"></td><td class="line-content">from sklearn.preprocessing import MinMaxScaler</td></tr><tr><td class="line-number" value="1730"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1731"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1732"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1733"></td><td class="line-content">Next, we will create our LSTM neural network. Since this is a simple</td></tr><tr><td class="line-number" value="1734"></td><td class="line-content">example, the LSTM will be made of of a recurrent node as described</td></tr><tr><td class="line-number" value="1735"></td><td class="line-content">above, with output from the node fed into a fully-connected network</td></tr><tr><td class="line-number" value="1736"></td><td class="line-content">with a single hidden layer.</td></tr><tr><td class="line-number" value="1737"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1738"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1739"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1740"></td><td class="line-content">class LSTM( nn.Module ):</td></tr><tr><td class="line-number" value="1741"></td><td class="line-content">    def __init__( self, input_size=1, hidden_n=100, output_size=1 ):</td></tr><tr><td class="line-number" value="1742"></td><td class="line-content">        super( LSTM, self ).__init__()</td></tr><tr><td class="line-number" value="1743"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1744"></td><td class="line-content">        self.hidden_layer_size = hidden_n</td></tr><tr><td class="line-number" value="1745"></td><td class="line-content">        self.lstm = nn.LSTM( input_size, hidden_n )</td></tr><tr><td class="line-number" value="1746"></td><td class="line-content">        self.fcn = nn.Linear( hidden_n, output_size )</td></tr><tr><td class="line-number" value="1747"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1748"></td><td class="line-content">        #  Hidden cell contains previous hidden state, previous cell state,</td></tr><tr><td class="line-number" value="1749"></td><td class="line-content">        #  randomized to start</td></tr><tr><td class="line-number" value="1750"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1751"></td><td class="line-content">        self.hidden_cell =\</td></tr><tr><td class="line-number" value="1752"></td><td class="line-content">          ( torch.zeros( 1, 1, self.hidden_layer_size ),</td></tr><tr><td class="line-number" value="1753"></td><td class="line-content">            torch.zeros( 1, 1, self.hidden_layer_size ) )</td></tr><tr><td class="line-number" value="1754"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1755"></td><td class="line-content">    #  End method __init__</td></tr><tr><td class="line-number" value="1756"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1757"></td><td class="line-content">    def forward( self, input_seq ):</td></tr><tr><td class="line-number" value="1758"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1759"></td><td class="line-content">        #  input_seq is a 12-value tensor, the 12 months of passengers to</td></tr><tr><td class="line-number" value="1760"></td><td class="line-content">        #  train on, normalized on the range -1..1, as a single row</td></tr><tr><td class="line-number" value="1761"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1762"></td><td class="line-content">        #  Input to an LSTM is of shape (seq_len, batch, input_size)</td></tr><tr><td class="line-number" value="1763"></td><td class="line-content">        #  so view below creates a column of 12 "samples", each sample is</td></tr><tr><td class="line-number" value="1764"></td><td class="line-content">        #  a single value, a batch size of 1, and a length of 1 (one 12-value</td></tr><tr><td class="line-number" value="1765"></td><td class="line-content">        #  sample sequence)</td></tr><tr><td class="line-number" value="1766"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1767"></td><td class="line-content">        input_seq = input_seq.view( len( input_seq ), 1, 1 )</td></tr><tr><td class="line-number" value="1768"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1769"></td><td class="line-content">        #  Ask the LSTM for the output and the (hidden,cell) state based</td></tr><tr><td class="line-number" value="1770"></td><td class="line-content">        #  on the 12-value input and the current (hidden,cell) state, this</td></tr><tr><td class="line-number" value="1771"></td><td class="line-content">        #  will recurse the LSTM 12 times</td></tr><tr><td class="line-number" value="1772"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1773"></td><td class="line-content">        lstm_out,self.hidden_cell = self.lstm( input_seq, self.hidden_cell )</td></tr><tr><td class="line-number" value="1774"></td><td class="line-content">        lstm_out = lstm_out.view( len( input_seq ), -1 )</td></tr><tr><td class="line-number" value="1775"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1776"></td><td class="line-content">        #  Run final output through the LSTM's FCN to get class</td></tr><tr><td class="line-number" value="1777"></td><td class="line-content">        #  probabilities</td></tr><tr><td class="line-number" value="1778"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1779"></td><td class="line-content">        predictions = self.fcn( lstm_out )</td></tr><tr><td class="line-number" value="1780"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1781"></td><td class="line-content">        #  Highest probability is the class we estimate</td></tr><tr><td class="line-number" value="1782"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1783"></td><td class="line-content">        return predictions[ -1 ]</td></tr><tr><td class="line-number" value="1784"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1785"></td><td class="line-content">    #  End method forward</td></tr><tr><td class="line-number" value="1786"></td><td class="line-content">#  End class LSTM</td></tr><tr><td class="line-number" value="1787"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1788"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1789"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1790"></td><td class="line-content">We will also create a helper function that takes a full sequence of</td></tr><tr><td class="line-number" value="1791"></td><td class="line-content">monthly passenger data, and breaks it into tuples of training and</td></tr><tr><td class="line-number" value="1792"></td><td class="line-content">label data. The training data is a list of twelve months of passenger</td></tr><tr><td class="line-number" value="1793"></td><td class="line-content">counts $t_i = \{ p_j, \ldots, p_{j+11} \}$. The label is the passenger</td></tr><tr><td class="line-number" value="1794"></td><td class="line-content">count for the month immediately following the twelve-month training</td></tr><tr><td class="line-number" value="1795"></td><td class="line-content">period $l_i = p_{j+12}$. Each of these $(t_i, l_i)$ tuples are stores</td></tr><tr><td class="line-number" value="1796"></td><td class="line-content">in a list of training samples used to train our LSTM $T = \{ t_0, t_1,</td></tr><tr><td class="line-number" value="1797"></td><td class="line-content">\ldots, t_{119} \}$.</td></tr><tr><td class="line-number" value="1798"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1799"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1800"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1801"></td><td class="line-content">def create_IO_seq( input, tw ):</td></tr><tr><td class="line-number" value="1802"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1803"></td><td class="line-content">    #  Create a set of time series to process during training, input is the</td></tr><tr><td class="line-number" value="1804"></td><td class="line-content">    #  entire data stream to divide, tw is time window size in samples</td></tr><tr><td class="line-number" value="1805"></td><td class="line-content">    #</td></tr><tr><td class="line-number" value="1806"></td><td class="line-content">    #  IO_seq is a (train_seq,label) tuple list, train_seq is a 12-month</td></tr><tr><td class="line-number" value="1807"></td><td class="line-content">    #  set of passengers, label is a single passenger count following the</td></tr><tr><td class="line-number" value="1808"></td><td class="line-content">    #  12-month sequence</td></tr><tr><td class="line-number" value="1809"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1810"></td><td class="line-content">    IO_seq = [ ]</td></tr><tr><td class="line-number" value="1811"></td><td class="line-content">    n = len( input )</td></tr><tr><td class="line-number" value="1812"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="1813"></td><td class="line-content">    for i in range( 0, n - tw ):</td></tr><tr><td class="line-number" value="1814"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1815"></td><td class="line-content">        #  Grab tw elements as training sequence, next element that follows</td></tr><tr><td class="line-number" value="1816"></td><td class="line-content">        #  is the label (i.e., the number of passengers following the given</td></tr><tr><td class="line-number" value="1817"></td><td class="line-content">        #  12-month period)</td></tr><tr><td class="line-number" value="1818"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1819"></td><td class="line-content">        train_seq = input[ i: i + tw ]</td></tr><tr><td class="line-number" value="1820"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1821"></td><td class="line-content">        #  To be pedantic, pull single tensor value as float, then make a</td></tr><tr><td class="line-number" value="1822"></td><td class="line-content">        #  single-element float list and convert it back to a tensor; can be</td></tr><tr><td class="line-number" value="1823"></td><td class="line-content">        #  done in a single step as:</td></tr><tr><td class="line-number" value="1824"></td><td class="line-content">        #</td></tr><tr><td class="line-number" value="1825"></td><td class="line-content">        #  train_label = input[ i + tw: i + tw + 1 ]</td></tr><tr><td class="line-number" value="1826"></td><td class="line-content">        #</td></tr><tr><td class="line-number" value="1827"></td><td class="line-content">        #  but I find that harder to understand</td></tr><tr><td class="line-number" value="1828"></td><td class="line-content">        #</td></tr><tr><td class="line-number" value="1829"></td><td class="line-content">        #  val is a single float, the number of passengers following the</td></tr><tr><td class="line-number" value="1830"></td><td class="line-content">        #  12-month training sequence we just extracted</td></tr><tr><td class="line-number" value="1831"></td><td class="line-content">        #</td></tr><tr><td class="line-number" value="1832"></td><td class="line-content">        #  train_label is [ val ] (a single-element float list) as a tensor</td></tr><tr><td class="line-number" value="1833"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1834"></td><td class="line-content">        val =  input[ i + tw ].item()</td></tr><tr><td class="line-number" value="1835"></td><td class="line-content">        train_label = torch.FloatTensor( [ val ] )</td></tr><tr><td class="line-number" value="1836"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1837"></td><td class="line-content">        IO_seq.append(  (train_seq,train_label) )</td></tr><tr><td class="line-number" value="1838"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1839"></td><td class="line-content">    return IO_seq</td></tr><tr><td class="line-number" value="1840"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1841"></td><td class="line-content">#  End function create_IO_seq</td></tr><tr><td class="line-number" value="1842"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1843"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1844"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1845"></td><td class="line-content">Next, we will load our dataset, transform the values from raw</td></tr><tr><td class="line-number" value="1846"></td><td class="line-content">passenger counts into "normalized" values on the range $[0 \ldots 1]$,</td></tr><tr><td class="line-number" value="1847"></td><td class="line-content">then use our helper function <span class="html-tag">&lt;tt&gt;</span>create_IO_seq<span class="html-tag">&lt;/tt&gt;</span> to create our</td></tr><tr><td class="line-number" value="1848"></td><td class="line-content">training sequence.</td></tr><tr><td class="line-number" value="1849"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1850"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1851"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1852"></td><td class="line-content">#  Load data, divide into train and test</td></tr><tr><td class="line-number" value="1853"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1854"></td><td class="line-content">flight_data = sns.load_dataset( 'flights' )</td></tr><tr><td class="line-number" value="1855"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1856"></td><td class="line-content">#  First 132 months for train, last 12 months for test, split into</td></tr><tr><td class="line-number" value="1857"></td><td class="line-content">#  12-month training sequences and 1-value labels:</td></tr><tr><td class="line-number" value="1858"></td><td class="line-content">#</td></tr><tr><td class="line-number" value="1859"></td><td class="line-content">#  1 2 3 4 5 6 7 8 9 10 11 12       13</td></tr><tr><td class="line-number" value="1860"></td><td class="line-content">#  - 12 months of training          - next value is label</td></tr><tr><td class="line-number" value="1861"></td><td class="line-content">#    so train on 12-month sequence..then see how many passengers next month</td></tr><tr><td class="line-number" value="1862"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1863"></td><td class="line-content">all_data = flight_data[ 'passengers' ].values.astype( float )</td></tr><tr><td class="line-number" value="1864"></td><td class="line-content">test_data_size = 12</td></tr><tr><td class="line-number" value="1865"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1866"></td><td class="line-content">train_data = all_data[ :-test_data_size ]</td></tr><tr><td class="line-number" value="1867"></td><td class="line-content">test_data = all_data[ -test_data_size: ]</td></tr><tr><td class="line-number" value="1868"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1869"></td><td class="line-content">#  Transform/normalize data to range -1..1</td></tr><tr><td class="line-number" value="1870"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1871"></td><td class="line-content">scaler = MinMaxScaler( feature_range=(-1,1) )</td></tr><tr><td class="line-number" value="1872"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1873"></td><td class="line-content">#  First reshape data into a single column then transform to range -1..1</td></tr><tr><td class="line-number" value="1874"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1875"></td><td class="line-content">train_data_norm =\</td></tr><tr><td class="line-number" value="1876"></td><td class="line-content">  scaler.fit_transform( train_data.reshape( len( train_data ), 1 ) )</td></tr><tr><td class="line-number" value="1877"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1878"></td><td class="line-content">#  Convert back to PyTorch tensor that's a single row</td></tr><tr><td class="line-number" value="1879"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1880"></td><td class="line-content">train_data_norm =\</td></tr><tr><td class="line-number" value="1881"></td><td class="line-content">  torch.FloatTensor( train_data_norm ).view( len( train_data_norm ) )</td></tr><tr><td class="line-number" value="1882"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1883"></td><td class="line-content">#  Create 12-value training sequences and correspond next value label</td></tr><tr><td class="line-number" value="1884"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1885"></td><td class="line-content">train_window = 12</td></tr><tr><td class="line-number" value="1886"></td><td class="line-content">train_IO_seq = create_IO_seq( train_data_norm, train_window )</td></tr><tr><td class="line-number" value="1887"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1888"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1889"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1890"></td><td class="line-content">Finally, we can create our LSTM RNN, choose our loss and optimizer</td></tr><tr><td class="line-number" value="1891"></td><td class="line-content">functions (mean squared error and Adam, respectively), and use our</td></tr><tr><td class="line-number" value="1892"></td><td class="line-content">training sequence to teach the model how to predict future passenger</td></tr><tr><td class="line-number" value="1893"></td><td class="line-content">counts based on the previous year's monthly passenger count sequences.</td></tr><tr><td class="line-number" value="1894"></td><td class="line-content">We will process the entire training sequence for 150 epochs.</td></tr><tr><td class="line-number" value="1895"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1896"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1897"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1898"></td><td class="line-content">#  Train LSTM</td></tr><tr><td class="line-number" value="1899"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1900"></td><td class="line-content">model.train()</td></tr><tr><td class="line-number" value="1901"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1902"></td><td class="line-content">epochs = 150</td></tr><tr><td class="line-number" value="1903"></td><td class="line-content">for i in range( 0, epochs ):</td></tr><tr><td class="line-number" value="1904"></td><td class="line-content">    for seq,label in train_IO_seq:</td></tr><tr><td class="line-number" value="1905"></td><td class="line-content">        optimizer.zero_grad()</td></tr><tr><td class="line-number" value="1906"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1907"></td><td class="line-content">        #  Re-initialize hidden and cell state to random prior to</td></tr><tr><td class="line-number" value="1908"></td><td class="line-content">        #  walking over the samples in the training sequence</td></tr><tr><td class="line-number" value="1909"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1910"></td><td class="line-content">        model.hidden_cell =\</td></tr><tr><td class="line-number" value="1911"></td><td class="line-content">          ( torch.zeros( 1, 1, model.hidden_layer_size ),</td></tr><tr><td class="line-number" value="1912"></td><td class="line-content">            torch.zeros( 1, 1, model.hidden_layer_size ) )</td></tr><tr><td class="line-number" value="1913"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1914"></td><td class="line-content">        y_pred = model( seq )</td></tr><tr><td class="line-number" value="1915"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1916"></td><td class="line-content">        single_loss = loss_function( y_pred, label )</td></tr><tr><td class="line-number" value="1917"></td><td class="line-content">        single_loss.backward()</td></tr><tr><td class="line-number" value="1918"></td><td class="line-content">        optimizer.step()</td></tr><tr><td class="line-number" value="1919"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1920"></td><td class="line-content">    if i % 25 == 1:</td></tr><tr><td class="line-number" value="1921"></td><td class="line-content">        print( f'epoch {i:3}; loss: {single_loss.item():10.8f}' )</td></tr><tr><td class="line-number" value="1922"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="1923"></td><td class="line-content">print( f'epoch {i:3}; loss: {single_loss.item():10.10f}' )</td></tr><tr><td class="line-number" value="1924"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="1925"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1926"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1927"></td><td class="line-content">Notice an important subtlety here. The order of processing for our</td></tr><tr><td class="line-number" value="1928"></td><td class="line-content">LSTM is:</td></tr><tr><td class="line-number" value="1929"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1930"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1931"></td><td class="line-content"><span class="html-tag">&lt;ol&gt;</span></td></tr><tr><td class="line-number" value="1932"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1933"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Zero any previous gradient information.</td></tr><tr><td class="line-number" value="1934"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1935"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Randomize both our hidden and cell states.</td></tr><tr><td class="line-number" value="1936"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1937"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Run the LSTM over <span class="html-tag">&lt;b&gt;</span>the entire<span class="html-tag">&lt;/b&gt;</span> 12-month sequence, storing the</td></tr><tr><td class="line-number" value="1938"></td><td class="line-content">final output value as <span class="html-tag">&lt;tt&gt;</span>y_pred<span class="html-tag">&lt;/tt&gt;</span>.</td></tr><tr><td class="line-number" value="1939"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1940"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Compute the loss between our prediction <span class="html-tag">&lt;tt&gt;</span>y_pred<span class="html-tag">&lt;/tt&gt;</span> and the</td></tr><tr><td class="line-number" value="1941"></td><td class="line-content">known passenger count for the 13<span class="html-tag">&lt;sup&gt;</span>th<span class="html-tag">&lt;/sup&gt;</span> month.</td></tr><tr><td class="line-number" value="1942"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1943"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span><span class="html-tag">&lt;b&gt;</span>Now<span class="html-tag">&lt;/b&gt;</span> use gradient descent to backpropegate the loss through</td></tr><tr><td class="line-number" value="1944"></td><td class="line-content">the LSTM.</td></tr><tr><td class="line-number" value="1945"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1946"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Use the Adam optimizer to update the LSTM's weights, hopefully to</td></tr><tr><td class="line-number" value="1947"></td><td class="line-content">better predict future passenger counts.</td></tr><tr><td class="line-number" value="1948"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1949"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="1950"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1951"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1952"></td><td class="line-content">Critically, gradient descent, backpropegation, and weight optimization</td></tr><tr><td class="line-number" value="1953"></td><td class="line-content">happen after each 12-month sequence is processed, and <span class="html-tag">&lt;b&gt;</span>not<span class="html-tag">&lt;/b&gt;</span></td></tr><tr><td class="line-number" value="1954"></td><td class="line-content">after each month in the 12-month sequence is processed. This detail is</td></tr><tr><td class="line-number" value="1955"></td><td class="line-content">important to understand.</td></tr><tr><td class="line-number" value="1956"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1957"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1958"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1959"></td><td class="line-content">Once the network is trained, we can predict passenger counts for the final</td></tr><tr><td class="line-number" value="1960"></td><td class="line-content">twelve months, and compare them to the known counts. Notice we have two</td></tr><tr><td class="line-number" value="1961"></td><td class="line-content">options for testing. We can take the <span class="html-tag">&lt;i&gt;</span>known<span class="html-tag">&lt;/i&gt;</span> 12-month sequences,</td></tr><tr><td class="line-number" value="1962"></td><td class="line-content">or we can take a combination of the <span class="html-tag">&lt;i&gt;</span>known<span class="html-tag">&lt;/i&gt;</span> and <span class="html-tag">&lt;i&gt;</span>predicted<span class="html-tag">&lt;/i&gt;</span></td></tr><tr><td class="line-number" value="1963"></td><td class="line-content">values to form a 12-month sequence.</td></tr><tr><td class="line-number" value="1964"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1965"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1966"></td><td class="line-content">\[</td></tr><tr><td class="line-number" value="1967"></td><td class="line-content">\begin{array}{r l l}</td></tr><tr><td class="line-number" value="1968"></td><td class="line-content">t_1 &amp;=\;\; \{ p_{121}, p_{122}, \ldots, p_{132} \} &amp;\rightarrow \;\; l_1\\</td></tr><tr><td class="line-number" value="1969"></td><td class="line-content">t_2 &amp;=\;\; \{ p_{122}, p_{123}, \ldots, p_{133} \} &amp;\rightarrow \;\; l_2\\</td></tr><tr><td class="line-number" value="1970"></td><td class="line-content">&amp;\qquad\qquad\,\cdots&amp;\;\\</td></tr><tr><td class="line-number" value="1971"></td><td class="line-content">t_{12} &amp;=\;\; \{ p_{132}, p_{133}, \ldots, p_{143} \} &amp;\rightarrow \;\; l_{12}\\</td></tr><tr><td class="line-number" value="1972"></td><td class="line-content">\\</td></tr><tr><td class="line-number" value="1973"></td><td class="line-content">&amp;\;\;\;\textrm{versus}\\</td></tr><tr><td class="line-number" value="1974"></td><td class="line-content">\\</td></tr><tr><td class="line-number" value="1975"></td><td class="line-content">t &amp;=\;\; \{ p_{121}, p_{122}, \ldots, p_{132} \} &amp;\rightarrow \;\; l_1\\</td></tr><tr><td class="line-number" value="1976"></td><td class="line-content">t &amp;=\;\; \{ p_{122}, p_{123}, \ldots, l_{1} \} &amp;\rightarrow \;\; l_2\\</td></tr><tr><td class="line-number" value="1977"></td><td class="line-content">&amp;\qquad\qquad\,\cdots&amp;\;\\</td></tr><tr><td class="line-number" value="1978"></td><td class="line-content">t &amp;=\;\; \{ p_{132}, l_{1}, \ldots, l_{11} \} &amp;\rightarrow \;\; l_{12}\\</td></tr><tr><td class="line-number" value="1979"></td><td class="line-content">\end{array}</td></tr><tr><td class="line-number" value="1980"></td><td class="line-content">\]</td></tr><tr><td class="line-number" value="1981"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1982"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="1983"></td><td class="line-content">We choose to implement the second approach. This approach would be</td></tr><tr><td class="line-number" value="1984"></td><td class="line-content">required if you are using all of your known data to predict a target</td></tr><tr><td class="line-number" value="1985"></td><td class="line-content">variable different from your predictors. For example, suppose we had</td></tr><tr><td class="line-number" value="1986"></td><td class="line-content">144 months of temperature data, and 156 months of precipitation data.</td></tr><tr><td class="line-number" value="1987"></td><td class="line-content">If we use the full 144 months of temperature data to build our model,</td></tr><tr><td class="line-number" value="1988"></td><td class="line-content">then the final 12 months we predict for testing have no corresponding</td></tr><tr><td class="line-number" value="1989"></td><td class="line-content">temperature data. In this case, we have no choice but to use our</td></tr><tr><td class="line-number" value="1990"></td><td class="line-content">predictions to "fill in" the unavailable temperature data as we</td></tr><tr><td class="line-number" value="1991"></td><td class="line-content">predict the 1<span class="html-tag">&lt;sup&gt;</span>st<span class="html-tag">&lt;/sup&gt;</span>, 2<span class="html-tag">&lt;sup&gt;</span>nd<span class="html-tag">&lt;/sup&gt;</span>, $\ldots$, and</td></tr><tr><td class="line-number" value="1992"></td><td class="line-content">12<span class="html-tag">&lt;sup&gt;</span>th<span class="html-tag">&lt;/sup&gt;</span> precipitation values. The tradeoff is more data during</td></tr><tr><td class="line-number" value="1993"></td><td class="line-content">training versus estimated data that likely contains errors during</td></tr><tr><td class="line-number" value="1994"></td><td class="line-content">testing for accuracy.</td></tr><tr><td class="line-number" value="1995"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="1996"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="1997"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="1998"></td><td class="line-content">#  Predict final twelve months</td></tr><tr><td class="line-number" value="1999"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2000"></td><td class="line-content">model.eval()</td></tr><tr><td class="line-number" value="2001"></td><td class="line-content">fut_pred = 12</td></tr><tr><td class="line-number" value="2002"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2003"></td><td class="line-content">#  Grab last twelve months of data, this will be the sequence used to</td></tr><tr><td class="line-number" value="2004"></td><td class="line-content">#  predict the first test value (remember, 132 training and 12 test</td></tr><tr><td class="line-number" value="2005"></td><td class="line-content">#  values were split at the beginning of the program)</td></tr><tr><td class="line-number" value="2006"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2007"></td><td class="line-content">test_outputs = [ ]</td></tr><tr><td class="line-number" value="2008"></td><td class="line-content">test_inputs = train_data_norm[ -train_window: ].tolist()</td></tr><tr><td class="line-number" value="2009"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2010"></td><td class="line-content">#  Run through all 12 test values</td></tr><tr><td class="line-number" value="2011"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2012"></td><td class="line-content">for i in range( 0, fut_pred ):</td></tr><tr><td class="line-number" value="2013"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="2014"></td><td class="line-content">    #  Convert list of last 12 (normalized) passengers to a tensor</td></tr><tr><td class="line-number" value="2015"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="2016"></td><td class="line-content">    seq = torch.FloatTensor( test_inputs[ -train_window: ] )</td></tr><tr><td class="line-number" value="2017"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="2018"></td><td class="line-content">    #  with torch.no_grad() runs LSTM without calculating gradients, we</td></tr><tr><td class="line-number" value="2019"></td><td class="line-content">    #  can only do this b/c we know we don't need gradients, backwards()</td></tr><tr><td class="line-number" value="2020"></td><td class="line-content">    #  is not called at the end of this training run, b/c we are passing</td></tr><tr><td class="line-number" value="2021"></td><td class="line-content">    #  one single 12-value sequence and only care about the final output</td></tr><tr><td class="line-number" value="2022"></td><td class="line-content">    </td></tr><tr><td class="line-number" value="2023"></td><td class="line-content">    with torch.no_grad():</td></tr><tr><td class="line-number" value="2024"></td><td class="line-content">        model.hidden =\</td></tr><tr><td class="line-number" value="2025"></td><td class="line-content">          ( torch.zeros( 1, 1, model.hidden_layer_size ),</td></tr><tr><td class="line-number" value="2026"></td><td class="line-content">            torch.zeros( 1, 1, model.hidden_layer_size ) )</td></tr><tr><td class="line-number" value="2027"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="2028"></td><td class="line-content">        #  Append output of LSTM to test_inputs, so when we loop again and</td></tr><tr><td class="line-number" value="2029"></td><td class="line-content">        #  grab the last 12 values for input, it includes the output(s)</td></tr><tr><td class="line-number" value="2030"></td><td class="line-content">        #  the LSTM is generating. Also, make sure to save the outputs for</td></tr><tr><td class="line-number" value="2031"></td><td class="line-content">        #  later accuracy calculations</td></tr><tr><td class="line-number" value="2032"></td><td class="line-content">        </td></tr><tr><td class="line-number" value="2033"></td><td class="line-content">        test_inputs.append( model( seq ).item() )</td></tr><tr><td class="line-number" value="2034"></td><td class="line-content">        test_outputs.append( test_inputs[ -1 ] )</td></tr><tr><td class="line-number" value="2035"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2036"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2037"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2038"></td><td class="line-content">Finally, to visually inspect our predictions, we plot the full</td></tr><tr><td class="line-number" value="2039"></td><td class="line-content">144-month sequence of known passenger counts in blue, then show the</td></tr><tr><td class="line-number" value="2040"></td><td class="line-content">last twelve months of predicted passenger counts in orange.</td></tr><tr><td class="line-number" value="2041"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2042"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="2043"></td><td class="line-content">#  The LSTM output is normalized on range -1..1, so we need to invert this</td></tr><tr><td class="line-number" value="2044"></td><td class="line-content">#  to get actual passenger numbers, need these to do a proper comparison to</td></tr><tr><td class="line-number" value="2045"></td><td class="line-content">#  known passenger numbers for accuracy calculations</td></tr><tr><td class="line-number" value="2046"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2047"></td><td class="line-content">actual_pred =\</td></tr><tr><td class="line-number" value="2048"></td><td class="line-content">  scaler.inverse_transform( np.array( test_outputs ).reshape( -1, 1 ) )</td></tr><tr><td class="line-number" value="2049"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2050"></td><td class="line-content">#  Plot known values in blue</td></tr><tr><td class="line-number" value="2051"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2052"></td><td class="line-content">fig_size = plt.rcParams[ 'figure.figsize' ]</td></tr><tr><td class="line-number" value="2053"></td><td class="line-content">fig_size[ 0 ] = 15</td></tr><tr><td class="line-number" value="2054"></td><td class="line-content">fig_size[ 1 ] = 5</td></tr><tr><td class="line-number" value="2055"></td><td class="line-content">plt.rcParams[ 'figure.figsize' ] = fig_size</td></tr><tr><td class="line-number" value="2056"></td><td class="line-content">plt.title( 'Months vs Passengers' )</td></tr><tr><td class="line-number" value="2057"></td><td class="line-content">plt.ylabel( 'Total Passengers' )</td></tr><tr><td class="line-number" value="2058"></td><td class="line-content">plt.xlabel( 'Months' )</td></tr><tr><td class="line-number" value="2059"></td><td class="line-content">plt.grid( True )</td></tr><tr><td class="line-number" value="2060"></td><td class="line-content">plt.autoscale( axis='x', tight=True )</td></tr><tr><td class="line-number" value="2061"></td><td class="line-content">plt.plot( flight_data[ 'passengers' ] )</td></tr><tr><td class="line-number" value="2062"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2063"></td><td class="line-content">#  Add in the predicted values in orange</td></tr><tr><td class="line-number" value="2064"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2065"></td><td class="line-content">x = np.arange( 132, 144, 1 )</td></tr><tr><td class="line-number" value="2066"></td><td class="line-content">plt.plot( x, actual_pred )</td></tr><tr><td class="line-number" value="2067"></td><td class="line-content">plt.show()</td></tr><tr><td class="line-number" value="2068"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2069"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2070"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="2071"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/predict.png" rel="noreferrer noopener">figs/predict.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto; max-width: 80%;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2072"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2073"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2074"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2075"></td><td class="line-content">The results are acceptable but not outstanding, although the LSTM was</td></tr><tr><td class="line-number" value="2076"></td><td class="line-content">able to catch the up&amp;ndash;down seasonal variation in the data,</td></tr><tr><td class="line-number" value="2077"></td><td class="line-content">something a basic RNN would probably miss. You can compare this to an</td></tr><tr><td class="line-number" value="2078"></td><td class="line-content">approach where we use only known values during testing. Not</td></tr><tr><td class="line-number" value="2079"></td><td class="line-content">surprisingly, this produces slightly better results.</td></tr><tr><td class="line-number" value="2080"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2081"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2082"></td><td class="line-content"><span class="html-tag">&lt;div&gt;</span></td></tr><tr><td class="line-number" value="2083"></td><td class="line-content">  <span class="html-tag">&lt;img <span class="html-attribute-name">src</span>="<a class="html-attribute-value html-resource-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/figs/predict_known.png" rel="noreferrer noopener">figs/predict_known.png</a>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">display: block; margin: auto; max-width: 80%;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2084"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2085"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2086"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2087"></td><td class="line-content"><span class="html-tag">&lt;h2 <span class="html-attribute-name">id</span>="<span class="html-attribute-value">#exercise</span>"&gt;</span>DNN Practice Exercises<span class="html-tag">&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="2088"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2089"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2090"></td><td class="line-content">Below is an example of the MNIST problem solved using a single-layer</td></tr><tr><td class="line-number" value="2091"></td><td class="line-content">FCN (<span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/notebook/MNIST-FCN.ipynb" rel="noreferrer noopener">./notebook/MNIST-FCN.ipynb</a>"&gt;</span>Jupyter</td></tr><tr><td class="line-number" value="2092"></td><td class="line-content">Notebook<span class="html-tag">&lt;/a&gt;</span>, <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/MNIST-FCN.py" rel="noreferrer noopener">./code/MNIST-FCN.py</a>"&gt;</span>Python file<span class="html-tag">&lt;/a&gt;</span>). Extend</td></tr><tr><td class="line-number" value="2093"></td><td class="line-content">this example to use two hidden layers instead of one. The second</td></tr><tr><td class="line-number" value="2094"></td><td class="line-content">hidden layer should take input from the first, contain 64 nodes, and</td></tr><tr><td class="line-number" value="2095"></td><td class="line-content">use ReLU to transform its output to be continuous. <span class="html-tag">&lt;b&gt;</span>Note:<span class="html-tag">&lt;/b&gt;</span> to</td></tr><tr><td class="line-number" value="2096"></td><td class="line-content">achieve this, you should <span class="html-tag">&lt;b&gt;</span>only<span class="html-tag">&lt;/b&gt;</span> need to make minor changes in the</td></tr><tr><td class="line-number" value="2097"></td><td class="line-content">FCN class's <span class="html-tag">&lt;tt&gt;</span>init()<span class="html-tag">&lt;/tt&gt;</span> and <span class="html-tag">&lt;tt&gt;</span>forward()<span class="html-tag">&lt;/tt&gt;</span> methods.</td></tr><tr><td class="line-number" value="2098"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2099"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2100"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>"&gt;</span></td></tr><tr><td class="line-number" value="2101"></td><td class="line-content">#  Perform all required imports</td></tr><tr><td class="line-number" value="2102"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2103"></td><td class="line-content">import numpy as np</td></tr><tr><td class="line-number" value="2104"></td><td class="line-content">import torch</td></tr><tr><td class="line-number" value="2105"></td><td class="line-content">import torch.nn as nn</td></tr><tr><td class="line-number" value="2106"></td><td class="line-content">import torch.optim as optim</td></tr><tr><td class="line-number" value="2107"></td><td class="line-content">import torchvision.datasets as datasets</td></tr><tr><td class="line-number" value="2108"></td><td class="line-content">import torchvision.transforms as transforms</td></tr><tr><td class="line-number" value="2109"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2110"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2111"></td><td class="line-content">class FCN( nn.Module ):</td></tr><tr><td class="line-number" value="2112"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2113"></td><td class="line-content">#  FCN model to convert image to digit</td></tr><tr><td class="line-number" value="2114"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2115"></td><td class="line-content">    def __init__( self ):</td></tr><tr><td class="line-number" value="2116"></td><td class="line-content">        super( FCN, self ).__init__()</td></tr><tr><td class="line-number" value="2117"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2118"></td><td class="line-content">        self.hidden0 = nn.Linear( 784, 128 )</td></tr><tr><td class="line-number" value="2119"></td><td class="line-content">        self.output = nn.Linear( 128, 10 )</td></tr><tr><td class="line-number" value="2120"></td><td class="line-content">        self.sigmoid = nn.Sigmoid()</td></tr><tr><td class="line-number" value="2121"></td><td class="line-content">        self.softmax = nn.LogSoftmax( dim=1 )</td></tr><tr><td class="line-number" value="2122"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2123"></td><td class="line-content">    #  End function __init__</td></tr><tr><td class="line-number" value="2124"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2125"></td><td class="line-content">    def forward( self, input ):</td></tr><tr><td class="line-number" value="2126"></td><td class="line-content">        hidden = self.hidden0( input )</td></tr><tr><td class="line-number" value="2127"></td><td class="line-content">        hidden = self.sigmoid( hidden )</td></tr><tr><td class="line-number" value="2128"></td><td class="line-content">        output = self.output( hidden )</td></tr><tr><td class="line-number" value="2129"></td><td class="line-content">        output = self.softmax( output )</td></tr><tr><td class="line-number" value="2130"></td><td class="line-content">        return output</td></tr><tr><td class="line-number" value="2131"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2132"></td><td class="line-content">    #  End function forward</td></tr><tr><td class="line-number" value="2133"></td><td class="line-content">#  End class FCN</td></tr><tr><td class="line-number" value="2134"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2135"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2136"></td><td class="line-content">def train( model, epoch, trainloader ):</td></tr><tr><td class="line-number" value="2137"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2138"></td><td class="line-content">#  Training function, use stochastic gradient descent to optimize</td></tr><tr><td class="line-number" value="2139"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2140"></td><td class="line-content">    criterion = nn.NLLLoss()</td></tr><tr><td class="line-number" value="2141"></td><td class="line-content">    optimizer = optim.SGD( model.parameters(), lr=0.003, momentum=0.9 )</td></tr><tr><td class="line-number" value="2142"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2143"></td><td class="line-content">    for e in range( 0, epoch ):</td></tr><tr><td class="line-number" value="2144"></td><td class="line-content">        running_loss = 0</td></tr><tr><td class="line-number" value="2145"></td><td class="line-content">        for images,labels in trainloader:</td></tr><tr><td class="line-number" value="2146"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2147"></td><td class="line-content">            #  Flatten images into a 784-long vector</td></tr><tr><td class="line-number" value="2148"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2149"></td><td class="line-content">            images = images.view( images.shape[ 0 ], -1 )</td></tr><tr><td class="line-number" value="2150"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2151"></td><td class="line-content">            optimizer.zero_grad()</td></tr><tr><td class="line-number" value="2152"></td><td class="line-content">            output = model( images )</td></tr><tr><td class="line-number" value="2153"></td><td class="line-content">            loss = criterion( output, labels )</td></tr><tr><td class="line-number" value="2154"></td><td class="line-content">            loss.backward()</td></tr><tr><td class="line-number" value="2155"></td><td class="line-content">            optimizer.step()</td></tr><tr><td class="line-number" value="2156"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2157"></td><td class="line-content">            running_loss += loss.item()</td></tr><tr><td class="line-number" value="2158"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2159"></td><td class="line-content">        print( 'Epoch {}, training loss: {}'.\</td></tr><tr><td class="line-number" value="2160"></td><td class="line-content">          format( e, running_loss / len( trainloader ) ) )</td></tr><tr><td class="line-number" value="2161"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2162"></td><td class="line-content">#  End function train</td></tr><tr><td class="line-number" value="2163"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2164"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2165"></td><td class="line-content">#  Mainline</td></tr><tr><td class="line-number" value="2166"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2167"></td><td class="line-content">#  Transform incoming greyscale data to a tensor, the mean and stdev of</td></tr><tr><td class="line-number" value="2168"></td><td class="line-content">#  the MNIST images is 0.1307 and 0.3081, so use Normalize to convert</td></tr><tr><td class="line-number" value="2169"></td><td class="line-content">#  this to a range of -1..1 (NB: mean and stdev can take up to 3 arguments,</td></tr><tr><td class="line-number" value="2170"></td><td class="line-content">#  to support different mean/stdev for R,G,B image channels, but a greyscale</td></tr><tr><td class="line-number" value="2171"></td><td class="line-content">#  image only has one channel)</td></tr><tr><td class="line-number" value="2172"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2173"></td><td class="line-content">transform = transforms.Compose(</td></tr><tr><td class="line-number" value="2174"></td><td class="line-content">    [ transforms.ToTensor(),</td></tr><tr><td class="line-number" value="2175"></td><td class="line-content">      transforms.Normalize( (0.5,), (0.5,) )</td></tr><tr><td class="line-number" value="2176"></td><td class="line-content">    ]</td></tr><tr><td class="line-number" value="2177"></td><td class="line-content">)</td></tr><tr><td class="line-number" value="2178"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2179"></td><td class="line-content">#  Download datasets, if not already downloaded</td></tr><tr><td class="line-number" value="2180"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2181"></td><td class="line-content">trainset =\</td></tr><tr><td class="line-number" value="2182"></td><td class="line-content">  datasets.MNIST( './', download=True, train=True, transform=transform )</td></tr><tr><td class="line-number" value="2183"></td><td class="line-content">testset =\</td></tr><tr><td class="line-number" value="2184"></td><td class="line-content">  datasets.MNIST( './', download=True, train=False, transform=transform )</td></tr><tr><td class="line-number" value="2185"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2186"></td><td class="line-content">#  Process data in batches of 64 items, both for training and testing</td></tr><tr><td class="line-number" value="2187"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2188"></td><td class="line-content">trainloader =\</td></tr><tr><td class="line-number" value="2189"></td><td class="line-content">  torch.utils.data.DataLoader( trainset, batch_size=64, shuffle=True )</td></tr><tr><td class="line-number" value="2190"></td><td class="line-content">testloader =\</td></tr><tr><td class="line-number" value="2191"></td><td class="line-content">  torch.utils.data.DataLoader( testset, batch_size=64, shuffle=True )</td></tr><tr><td class="line-number" value="2192"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2193"></td><td class="line-content">model = FCN()</td></tr><tr><td class="line-number" value="2194"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2195"></td><td class="line-content">#  Put the model in training mode, train for 15 epochs</td></tr><tr><td class="line-number" value="2196"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2197"></td><td class="line-content">model.train()</td></tr><tr><td class="line-number" value="2198"></td><td class="line-content">train( model, 15, trainloader )</td></tr><tr><td class="line-number" value="2199"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2200"></td><td class="line-content">#  Evaluate the model on the 10000 test images</td></tr><tr><td class="line-number" value="2201"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2202"></td><td class="line-content">n = 0</td></tr><tr><td class="line-number" value="2203"></td><td class="line-content">correct = 0</td></tr><tr><td class="line-number" value="2204"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2205"></td><td class="line-content">model.eval()</td></tr><tr><td class="line-number" value="2206"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2207"></td><td class="line-content">for images,labels in testloader:</td></tr><tr><td class="line-number" value="2208"></td><td class="line-content">    for i in range( 0, len( labels ) ):</td></tr><tr><td class="line-number" value="2209"></td><td class="line-content">        img = images[ i ].view( 1, 784 )</td></tr><tr><td class="line-number" value="2210"></td><td class="line-content">        prob = model( img )</td></tr><tr><td class="line-number" value="2211"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2212"></td><td class="line-content">        #  Invert probabilities since they are natural log'd</td></tr><tr><td class="line-number" value="2213"></td><td class="line-content">        #  (LogSoftmax), then from a tensor to a list</td></tr><tr><td class="line-number" value="2214"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2215"></td><td class="line-content">        prob = torch.exp( prob )</td></tr><tr><td class="line-number" value="2216"></td><td class="line-content">        prob = prob[ 0 ].tolist()</td></tr><tr><td class="line-number" value="2217"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2218"></td><td class="line-content">        pred_label = prob.index( max( prob ) )</td></tr><tr><td class="line-number" value="2219"></td><td class="line-content">        true_label = labels[ i ].item()</td></tr><tr><td class="line-number" value="2220"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2221"></td><td class="line-content">        if true_label == pred_label:</td></tr><tr><td class="line-number" value="2222"></td><td class="line-content">            correct += 1</td></tr><tr><td class="line-number" value="2223"></td><td class="line-content">        n += 1</td></tr><tr><td class="line-number" value="2224"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2225"></td><td class="line-content">print( f'Images tested: {n}' )</td></tr><tr><td class="line-number" value="2226"></td><td class="line-content">print( f'Accuracy: {correct / n * 100.0:.2f}%' )</td></tr><tr><td class="line-number" value="2227"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2228"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2229"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2230"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">id</span>="<span class="html-attribute-value">MNIST-FCN-accordion</span>" <span class="html-attribute-name">class</span>="<span class="html-attribute-value">detail</span>"&gt;</span></td></tr><tr><td class="line-number" value="2231"></td><td class="line-content">  <span class="html-tag">&lt;h3&gt;</span>MNIST FCN Solution<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="2232"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">background: none; background-color: #e7eefb;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2233"></td><td class="line-content">    <span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">background-color: #f8f8f8;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2234"></td><td class="line-content">class FCN( nn.Module ):</td></tr><tr><td class="line-number" value="2235"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2236"></td><td class="line-content">#  FCN model to convert image to digit</td></tr><tr><td class="line-number" value="2237"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2238"></td><td class="line-content">    def __init__( self ):</td></tr><tr><td class="line-number" value="2239"></td><td class="line-content">        super( FCN, self ).__init__()</td></tr><tr><td class="line-number" value="2240"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2241"></td><td class="line-content">        self.hidden0 = nn.Linear( 784, 128 )</td></tr><tr><td class="line-number" value="2242"></td><td class="line-content">        self.hidden1 = nn.Linear( 128, 64 )</td></tr><tr><td class="line-number" value="2243"></td><td class="line-content">        self.output = nn.Linear( 64, 10 )</td></tr><tr><td class="line-number" value="2244"></td><td class="line-content">        self.sigmoid = nn.Sigmoid()</td></tr><tr><td class="line-number" value="2245"></td><td class="line-content">        self.relu = nn.ReLU()</td></tr><tr><td class="line-number" value="2246"></td><td class="line-content">        self.softmax = nn.LogSoftmax( dim=1 )</td></tr><tr><td class="line-number" value="2247"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2248"></td><td class="line-content">    #  End function __init__</td></tr><tr><td class="line-number" value="2249"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2250"></td><td class="line-content">    def forward( self, input ):</td></tr><tr><td class="line-number" value="2251"></td><td class="line-content">        hidden = self.hidden0( input )</td></tr><tr><td class="line-number" value="2252"></td><td class="line-content">        hidden = self.sigmoid( hidden )</td></tr><tr><td class="line-number" value="2253"></td><td class="line-content">        hidden = self.hidden1( hidden )</td></tr><tr><td class="line-number" value="2254"></td><td class="line-content">        hidden = self.relu( hidden )</td></tr><tr><td class="line-number" value="2255"></td><td class="line-content">        output = self.output( hidden )</td></tr><tr><td class="line-number" value="2256"></td><td class="line-content">        output = self.softmax( output )</td></tr><tr><td class="line-number" value="2257"></td><td class="line-content">        return output</td></tr><tr><td class="line-number" value="2258"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2259"></td><td class="line-content">    #  End function forward</td></tr><tr><td class="line-number" value="2260"></td><td class="line-content">#  End class FCN</td></tr><tr><td class="line-number" value="2261"></td><td class="line-content">    <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2262"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2263"></td><td class="line-content">  <span class="html-tag">&lt;p&gt;</span>Full Python code</td></tr><tr><td class="line-number" value="2264"></td><td class="line-content">  solution: <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/MNIST-FCN.py" rel="noreferrer noopener">./code/MNIST-FCN.py</a>"&gt;</span>MNIST-FCN.py<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2265"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2266"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2267"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2268"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2269"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2270"></td><td class="line-content">Normally, image data is processed with a CNN rather than an FCN. This</td></tr><tr><td class="line-number" value="2271"></td><td class="line-content">worked well here because taking the $2 \times 2$ image and</td></tr><tr><td class="line-number" value="2272"></td><td class="line-content">concatenating rows to create a single vector of greyscale values</td></tr><tr><td class="line-number" value="2273"></td><td class="line-content">formed patterns that distinguished different digits well. Could we do</td></tr><tr><td class="line-number" value="2274"></td><td class="line-content">better with a CNN?  Would you like to find out? If so, try</td></tr><tr><td class="line-number" value="2275"></td><td class="line-content">implementing one.</td></tr><tr><td class="line-number" value="2276"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2277"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2278"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2279"></td><td class="line-content">This is more complicated than the previous example, because you will</td></tr><tr><td class="line-number" value="2280"></td><td class="line-content">need to completely replace the <span class="html-tag">&lt;tt&gt;</span>FCN<span class="html-tag">&lt;/tt&gt;</span> class with a <span class="html-tag">&lt;tt&gt;</span>CNN<span class="html-tag">&lt;/tt&gt;</span></td></tr><tr><td class="line-number" value="2281"></td><td class="line-content">class. The rest of the code can remain unchanged, except for: (1)</td></tr><tr><td class="line-number" value="2282"></td><td class="line-content">using a different <span class="html-tag">&lt;tt&gt;</span>criterion<span class="html-tag">&lt;/tt&gt;</span> better suited for images, (2) no</td></tr><tr><td class="line-number" value="2283"></td><td class="line-content">need to flatten images because a CNN can handle 2D images directly,</td></tr><tr><td class="line-number" value="2284"></td><td class="line-content">unlike an FCN which expects its input to be single 1D vector, and (3)</td></tr><tr><td class="line-number" value="2285"></td><td class="line-content">the evaluation section of the mainline, which will need to account for</td></tr><tr><td class="line-number" value="2286"></td><td class="line-content">the format of the output from the CNN model.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2287"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2288"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2289"></td><td class="line-content">Here is the CNN model structure you should use.</td></tr><tr><td class="line-number" value="2290"></td><td class="line-content"><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2291"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2292"></td><td class="line-content"><span class="html-tag">&lt;ol&gt;</span></td></tr><tr><td class="line-number" value="2293"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>A 2D convolution that takes the MNIST images as input and produces</td></tr><tr><td class="line-number" value="2294"></td><td class="line-content">six filters using a $4 \times 4$ kernel with a stride of $1$.</td></tr><tr><td class="line-number" value="2295"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2296"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>An ReLU activation on the results to scale them to the range $[0</td></tr><tr><td class="line-number" value="2297"></td><td class="line-content">\ldots 1]$.</td></tr><tr><td class="line-number" value="2298"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2299"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>A 2D maxpooling of size $2 \times 2$ to downsample the image from</td></tr><tr><td class="line-number" value="2300"></td><td class="line-content">size $28 \times 28$. <span class="html-tag">&lt;b&gt;</span>Be careful here!<span class="html-tag">&lt;/b&gt;</span> Think hard about what</td></tr><tr><td class="line-number" value="2301"></td><td class="line-content">size of image results you'll get from your 2D covolution step (hint:</td></tr><tr><td class="line-number" value="2302"></td><td class="line-content">it <span class="html-tag">&lt;b&gt;</span>IS NOT<span class="html-tag">&lt;/b&gt;</span> $28 \times 28$, consider what happens when a $4</td></tr><tr><td class="line-number" value="2303"></td><td class="line-content">\times 4$ filter reaches either the right edge or bottom edge of a $28</td></tr><tr><td class="line-number" value="2304"></td><td class="line-content">\times 28$ image). You will need to add <span class="html-tag">&lt;tt&gt;</span>padding<span class="html-tag">&lt;/tt&gt;</span> to</td></tr><tr><td class="line-number" value="2305"></td><td class="line-content">the <span class="html-tag">&lt;tt&gt;</span>MaxPool2D<span class="html-tag">&lt;/tt&gt;</span> operation to account for this.</td></tr><tr><td class="line-number" value="2306"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2307"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Another 2D convolution producing 16 filters using a $2 \times 2$</td></tr><tr><td class="line-number" value="2308"></td><td class="line-content">kernel with a stride of $1$.</td></tr><tr><td class="line-number" value="2309"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2310"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>An ReLU activation on the results.</td></tr><tr><td class="line-number" value="2311"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2312"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>A 2D maxpooling of size $2 \times 2$.</td></tr><tr><td class="line-number" value="2313"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2314"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Flatten the results into a single 1D vector as input for the</td></tr><tr><td class="line-number" value="2315"></td><td class="line-content">follow-on FCN.</td></tr><tr><td class="line-number" value="2316"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2317"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>A linear layer with $120$ nodes taking the results of the</td></tr><tr><td class="line-number" value="2318"></td><td class="line-content">convolution stage as input.</td></tr><tr><td class="line-number" value="2319"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2320"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>An ReLU activation on the results.</td></tr><tr><td class="line-number" value="2321"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2322"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>A linear layer with 10 nodes taking output from the previous</td></tr><tr><td class="line-number" value="2323"></td><td class="line-content">hidden layer and producing probabilities for the ten possible</td></tr><tr><td class="line-number" value="2324"></td><td class="line-content">handwritten letter types.</td></tr><tr><td class="line-number" value="2325"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2326"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Finally, a <span class="html-tag">&lt;tt&gt;</span>LogSoftmax<span class="html-tag">&lt;/tt&gt;</span> operation on the probabilities to</td></tr><tr><td class="line-number" value="2327"></td><td class="line-content">produce normalized logarithmic results.</td></tr><tr><td class="line-number" value="2328"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2329"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Update your <span class="html-tag">&lt;tt&gt;</span>criterion<span class="html-tag">&lt;/tt&gt;</span> to use</td></tr><tr><td class="line-number" value="2330"></td><td class="line-content"><span class="html-tag">&lt;tt&gt;</span>nn.CrossEntropyLoss()<span class="html-tag">&lt;/tt&gt;</span> rather than <span class="html-tag">&lt;tt&gt;</span>nn.NLLLoss()<span class="html-tag">&lt;/tt&gt;</span> in</td></tr><tr><td class="line-number" value="2331"></td><td class="line-content">your <span class="html-tag">&lt;tt&gt;</span>train()<span class="html-tag">&lt;/tt&gt;</span> function.</td></tr><tr><td class="line-number" value="2332"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2333"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>In your <span class="html-tag">&lt;tt&gt;</span>train()<span class="html-tag">&lt;/tt&gt;</span> function, you <span class="html-tag">&lt;b&gt;</span>DO NOT<span class="html-tag">&lt;/b&gt;</span> need to</td></tr><tr><td class="line-number" value="2334"></td><td class="line-content">flatten the images with <span class="html-tag">&lt;code&gt;</span>view()<span class="html-tag">&lt;/code&gt;</span>, since a CNN takes the 2D</td></tr><tr><td class="line-number" value="2335"></td><td class="line-content">$28 \times 28$ greyscale images directly.</td></tr><tr><td class="line-number" value="2336"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2337"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="2338"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2339"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2340"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">id</span>="<span class="html-attribute-value">MNIST-CNN-accordion</span>" <span class="html-attribute-name">class</span>="<span class="html-attribute-value">detail</span>"&gt;</span></td></tr><tr><td class="line-number" value="2341"></td><td class="line-content">  <span class="html-tag">&lt;h3&gt;</span>MNIST CNN Solution<span class="html-tag">&lt;/h3&gt;</span></td></tr><tr><td class="line-number" value="2342"></td><td class="line-content">  <span class="html-tag">&lt;div <span class="html-attribute-name">style</span>="<span class="html-attribute-value">background: none; background-color: #e7eefb;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2343"></td><td class="line-content">    <span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">code-div</span>" <span class="html-attribute-name">style</span>="<span class="html-attribute-value">background-color: #f8f8f8;</span>"&gt;</span></td></tr><tr><td class="line-number" value="2344"></td><td class="line-content">class CNN( nn.Module ):</td></tr><tr><td class="line-number" value="2345"></td><td class="line-content">    def __init__( self ):</td></tr><tr><td class="line-number" value="2346"></td><td class="line-content">        super( CNN, self ).__init__()</td></tr><tr><td class="line-number" value="2347"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2348"></td><td class="line-content">        #  2D convolution, 1 input channel, 6 output channels, 4x4 kernel,</td></tr><tr><td class="line-number" value="2349"></td><td class="line-content">        #  stride of 1, 28x28 image will become a 25x25 result, b/c last</td></tr><tr><td class="line-number" value="2350"></td><td class="line-content">        #  4x4 kernel at right edge and bottom edge produces 1 result for 4</td></tr><tr><td class="line-number" value="2351"></td><td class="line-content">        #  cells, meaning image shrinks by 3 cells</td></tr><tr><td class="line-number" value="2352"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2353"></td><td class="line-content">        self.conv1 = nn.Conv2d( 1, 6, kernel_size=4, stride=1 )</td></tr><tr><td class="line-number" value="2354"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2355"></td><td class="line-content">        #  2D convolution, input size of 6(from previous convolution layer),</td></tr><tr><td class="line-number" value="2356"></td><td class="line-content">        #  output size of 16, this follows a MaxPool2d of size 2x2 with a</td></tr><tr><td class="line-number" value="2357"></td><td class="line-content">        #  padding of 1 to expand image to 26x26 from 25x25, this downsamples</td></tr><tr><td class="line-number" value="2358"></td><td class="line-content">        #  the image to 13x13</td></tr><tr><td class="line-number" value="2359"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2360"></td><td class="line-content">        self.conv2 = nn.Conv2d( 6, 16, kernel_size=2 )</td></tr><tr><td class="line-number" value="2361"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2362"></td><td class="line-content">        #  FCN section of CNN, result of second covolution is again MaxPool2d</td></tr><tr><td class="line-number" value="2363"></td><td class="line-content">        #  at 2x2 resolution with padding of 1, reducing image size from 14x14</td></tr><tr><td class="line-number" value="2364"></td><td class="line-content">        #  to 7x7 over the 16 output filters</td></tr><tr><td class="line-number" value="2365"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2366"></td><td class="line-content">        self.fc1 = nn.Linear( 16 * 7 * 7, 120 )</td></tr><tr><td class="line-number" value="2367"></td><td class="line-content">        self.fc2 = nn.Linear( 120, 10 )</td></tr><tr><td class="line-number" value="2368"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2369"></td><td class="line-content">        self.pool = nn.MaxPool2d( 2, 2, padding=1 )</td></tr><tr><td class="line-number" value="2370"></td><td class="line-content">        self.relu = nn.ReLU()</td></tr><tr><td class="line-number" value="2371"></td><td class="line-content">        self.softmax = nn.LogSoftmax( dim=1 )</td></tr><tr><td class="line-number" value="2372"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2373"></td><td class="line-content">    def forward( self, input ):</td></tr><tr><td class="line-number" value="2374"></td><td class="line-content">        conv = self.conv1( input )</td></tr><tr><td class="line-number" value="2375"></td><td class="line-content">        conv = self.relu( conv )</td></tr><tr><td class="line-number" value="2376"></td><td class="line-content">        conv = self.pool( conv )</td></tr><tr><td class="line-number" value="2377"></td><td class="line-content">        conv = self.conv2( conv )</td></tr><tr><td class="line-number" value="2378"></td><td class="line-content">        conv = self.relu( conv )</td></tr><tr><td class="line-number" value="2379"></td><td class="line-content">        conv = self.pool( conv )</td></tr><tr><td class="line-number" value="2380"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2381"></td><td class="line-content">        conv = conv.view( -1, 16 * 7 * 7 )</td></tr><tr><td class="line-number" value="2382"></td><td class="line-content">        output = self.fc1( conv )</td></tr><tr><td class="line-number" value="2383"></td><td class="line-content">        output = self.relu( output )</td></tr><tr><td class="line-number" value="2384"></td><td class="line-content">        output = self.fc2( output )</td></tr><tr><td class="line-number" value="2385"></td><td class="line-content">        output = self.softmax( output )</td></tr><tr><td class="line-number" value="2386"></td><td class="line-content">        return output</td></tr><tr><td class="line-number" value="2387"></td><td class="line-content">    #  End function forward</td></tr><tr><td class="line-number" value="2388"></td><td class="line-content">#  End class CNN</td></tr><tr><td class="line-number" value="2389"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2390"></td><td class="line-content">...</td></tr><tr><td class="line-number" value="2391"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2392"></td><td class="line-content">def train( model, epoch, trainloader ):</td></tr><tr><td class="line-number" value="2393"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2394"></td><td class="line-content">#  Training function, use stochastic gradient descent to optimize</td></tr><tr><td class="line-number" value="2395"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2396"></td><td class="line-content">    criterion =  nn.CrossEntropyLoss()</td></tr><tr><td class="line-number" value="2397"></td><td class="line-content">    optimizer = optim.SGD( model.parameters(), lr=0.003, momentum=0.9 )</td></tr><tr><td class="line-number" value="2398"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2399"></td><td class="line-content">    for e in range( 0, epoch ):</td></tr><tr><td class="line-number" value="2400"></td><td class="line-content">        running_loss = 0</td></tr><tr><td class="line-number" value="2401"></td><td class="line-content">        for images,labels in trainloader:</td></tr><tr><td class="line-number" value="2402"></td><td class="line-content">            optimizer.zero_grad()</td></tr><tr><td class="line-number" value="2403"></td><td class="line-content">            output = model( images )</td></tr><tr><td class="line-number" value="2404"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2405"></td><td class="line-content">...</td></tr><tr><td class="line-number" value="2406"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2407"></td><td class="line-content">model.eval()</td></tr><tr><td class="line-number" value="2408"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2409"></td><td class="line-content">for images,labels in testloader:</td></tr><tr><td class="line-number" value="2410"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2411"></td><td class="line-content">    # Use trained CNN to convert (batch of 64) images to class probabilities</td></tr><tr><td class="line-number" value="2412"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2413"></td><td class="line-content">    prob = model( images )</td></tr><tr><td class="line-number" value="2414"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2415"></td><td class="line-content">    #  Check all results against known class labels</td></tr><tr><td class="line-number" value="2416"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2417"></td><td class="line-content">    for i in range( 0, len( labels ) ):</td></tr><tr><td class="line-number" value="2418"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2419"></td><td class="line-content">        #  Invert probabilities since they are natural log'd</td></tr><tr><td class="line-number" value="2420"></td><td class="line-content">        #  (LogSoftmax), then from a tensor to a list</td></tr><tr><td class="line-number" value="2421"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2422"></td><td class="line-content">        p = torch.exp( prob[ i ] )</td></tr><tr><td class="line-number" value="2423"></td><td class="line-content">        p = p.tolist()</td></tr><tr><td class="line-number" value="2424"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2425"></td><td class="line-content">        pred_label = p.index( max( p ) )</td></tr><tr><td class="line-number" value="2426"></td><td class="line-content">        true_label = labels[ i ].item()</td></tr><tr><td class="line-number" value="2427"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2428"></td><td class="line-content">        if true_label == pred_label:</td></tr><tr><td class="line-number" value="2429"></td><td class="line-content">            correct += 1</td></tr><tr><td class="line-number" value="2430"></td><td class="line-content">    n += 1</td></tr><tr><td class="line-number" value="2431"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2432"></td><td class="line-content">...</td></tr><tr><td class="line-number" value="2433"></td><td class="line-content">    <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2434"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2435"></td><td class="line-content">  <span class="html-tag">&lt;p&gt;</span>Full Python code</td></tr><tr><td class="line-number" value="2436"></td><td class="line-content">  solution: <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>="<a class="html-attribute-value html-external-link" target="_blank" href="https://www.csc2.ncsu.edu/faculty/healey/msa/dnn/code/MNIST-CNN.py" rel="noreferrer noopener">./code/MNIST-CNN.py</a>"&gt;</span>MNIST-CNN.py<span class="html-tag">&lt;/a&gt;</span><span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2437"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2438"></td><td class="line-content">  <span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2439"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2440"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2441"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>In my testing, results from the CNN were 98% or higher, slightly</td></tr><tr><td class="line-number" value="2442"></td><td class="line-content">better than the FCN example.<span class="html-tag">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2443"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2444"></td><td class="line-content"><span class="html-comment">&lt;!--</span></td></tr><tr><td class="line-number" value="2445"></td><td class="line-content"><span class="html-comment">&lt;h2&gt;Further Study&lt;/h2&gt;</span></td></tr><tr><td class="line-number" value="2446"></td><td class="line-content"><span class="html-comment"><br></span></td></tr><tr><td class="line-number" value="2447"></td><td class="line-content"><span class="html-comment">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2448"></td><td class="line-content"><span class="html-comment">If you want to continue working on CNNs, I suggest you look at</span></td></tr><tr><td class="line-number" value="2449"></td><td class="line-content"><span class="html-comment">a library like &lt;code&gt;fastai&lt;/code&gt;. This library sits on top of</span></td></tr><tr><td class="line-number" value="2450"></td><td class="line-content"><span class="html-comment">&lt;code&gt;pytorch&lt;/code&gt;, and further simplifies creating image-based</span></td></tr><tr><td class="line-number" value="2451"></td><td class="line-content"><span class="html-comment">CNNs. &lt;code&gt;fastai&lt;/code&gt; includes numerous pre-structured CNNs</span></td></tr><tr><td class="line-number" value="2452"></td><td class="line-content"><span class="html-comment">that have been shown to produce excellent performance on standard</span></td></tr><tr><td class="line-number" value="2453"></td><td class="line-content"><span class="html-comment">test datasets like CIFAR10. &lt;code&gt;fastai&lt;/code&gt; documentation and</span></td></tr><tr><td class="line-number" value="2454"></td><td class="line-content"><span class="html-comment">instructions on how to install the library can be found here.</span></td></tr><tr><td class="line-number" value="2455"></td><td class="line-content"><span class="html-comment"><br></span></td></tr><tr><td class="line-number" value="2456"></td><td class="line-content"><span class="html-comment">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2457"></td><td class="line-content"><span class="html-comment">&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;</span></td></tr><tr><td class="line-number" value="2458"></td><td class="line-content"><span class="html-comment">&lt;a href="https://github.com/fastai"&gt;&lt;code&gt;fastai&lt;/code&gt;&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="2459"></td><td class="line-content"><span class="html-comment">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2460"></td><td class="line-content"><span class="html-comment"><br></span></td></tr><tr><td class="line-number" value="2461"></td><td class="line-content"><span class="html-comment">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2462"></td><td class="line-content"><span class="html-comment">Here is an example of using &lt;code&gt;fastai&lt;/code&gt; to train on the</span></td></tr><tr><td class="line-number" value="2463"></td><td class="line-content"><span class="html-comment">CIFAR10 dataset, producing accuracies of approximately 93%.</span></td></tr><tr><td class="line-number" value="2464"></td><td class="line-content"><span class="html-comment">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2465"></td><td class="line-content"><span class="html-comment"><br></span></td></tr><tr><td class="line-number" value="2466"></td><td class="line-content"><span class="html-comment">&lt;p&gt;</span></td></tr><tr><td class="line-number" value="2467"></td><td class="line-content"><span class="html-comment">&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;</span></td></tr><tr><td class="line-number" value="2468"></td><td class="line-content"><span class="html-comment">&lt;a href="https://www.fast.ai/2018/04/30/dawnbench-fastai/"&gt;CIFAR10 fastai CNN&lt;/a&gt;</span></td></tr><tr><td class="line-number" value="2469"></td><td class="line-content"><span class="html-comment">&lt;/p&gt;</span></td></tr><tr><td class="line-number" value="2470"></td><td class="line-content"><span class="html-comment">--&gt;</span></td></tr><tr><td class="line-number" value="2471"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2472"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2473"></td><td class="line-content"><span class="html-comment">&lt;!-- The mod-date span will be updated by code in mod-date.js --&gt;</span></td></tr><tr><td class="line-number" value="2474"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2475"></td><td class="line-content"><span class="html-tag">&lt;hr <span class="html-attribute-name">class</span>="<span class="html-attribute-value">fig_top</span>"&gt;</span></td></tr><tr><td class="line-number" value="2476"></td><td class="line-content"><span class="html-tag">&lt;div <span class="html-attribute-name">class</span>="<span class="html-attribute-value">footer</span>"&gt;</span></td></tr><tr><td class="line-number" value="2477"></td><td class="line-content">  Updated <span class="html-tag">&lt;span <span class="html-attribute-name">id</span>="<span class="html-attribute-value">mod-date</span>"&gt;</span>01-Jan-01<span class="html-tag">&lt;/span&gt;</span></td></tr><tr><td class="line-number" value="2478"></td><td class="line-content"><span class="html-tag">&lt;/div&gt;</span></td></tr><tr><td class="line-number" value="2479"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2480"></td><td class="line-content"><span class="html-tag">&lt;/body&gt;</span></td></tr><tr><td class="line-number" value="2481"></td><td class="line-content"><span class="html-tag">&lt;/html&gt;</span></td></tr><tr><td class="line-number" value="2482"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2483"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="2484"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  ui DNNs PyTorch FCNs convolutional CNNs conda ANNs</span></td></tr><tr><td class="line-number" value="2485"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2486"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  pytorch torchvision McCulloch Rosenblatt DNN GPUs</span></td></tr><tr><td class="line-number" value="2487"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2488"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  Perceptron Widrow MADALINE Papert multilayer FCN</span></td></tr><tr><td class="line-number" value="2489"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2490"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  activations backpropagation Rumelhart PyTorch's im</span></td></tr><tr><td class="line-number" value="2491"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2492"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  dataset MNIST ReLU greyscale CIFAR dsets xforms jk</span></td></tr><tr><td class="line-number" value="2493"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2494"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  ToTensor img numpy plt figsize imshow Jupyter CONV</span></td></tr><tr><td class="line-number" value="2495"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2496"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  accuracies modalities convolve convolved softmax</span></td></tr><tr><td class="line-number" value="2497"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2498"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  neighbours Analytics Healey perceptrons perceptron</span></td></tr><tr><td class="line-number" value="2499"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2500"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  ldots leq cdot sigmoid frac perceptron's MNIST MSE</span></td></tr><tr><td class="line-number" value="2501"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2502"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  forall rightarrow tunable nabla pre datasets Bigg</span></td></tr><tr><td class="line-number" value="2503"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2504"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  fastai backpropegating Backpropegation underbrace</span></td></tr><tr><td class="line-number" value="2505"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2506"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  bmatrix scriptsize vectorization cdots Elementwise</span></td></tr><tr><td class="line-number" value="2507"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2508"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  backpropegation elementwise odot Hadamard Schur Eq</span></td></tr><tr><td class="line-number" value="2509"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2510"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  propegating backpropegated Backpropegate Assymetry</span></td></tr><tr><td class="line-number" value="2511"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2512"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  backpropegate TensorFlow nm Paszke Lua Soumith cpu</span></td></tr><tr><td class="line-number" value="2513"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2514"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  Chintala Chanan Caffe ndarray cuda setosa RNNs RNN</span></td></tr><tr><td class="line-number" value="2515"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2516"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  versicolor virginica LSTMs GRUs RNN's tanh rnn sns</span></td></tr><tr><td class="line-number" value="2517"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2518"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  Programatically FeedForwardDNN feedforward Goro np</span></td></tr><tr><td class="line-number" value="2519"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2520"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  timesteps LSTM Hochreiter Schmidhuber pointwise nn</span></td></tr><tr><td class="line-number" value="2521"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2522"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  textrm seaborn len matplotlib pyplot sklearn init</span></td></tr><tr><td class="line-number" value="2523"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2524"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  preprocessing MinMaxScaler lstm fcn recurse LSTM's</span></td></tr><tr><td class="line-number" value="2525"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2526"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  tw FloatTensor scaler pred f'epoch th nd tradeoff</span></td></tr><tr><td class="line-number" value="2527"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2528"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  eval tolist rcParams ylabel xlabel autoscale optim</span></td></tr><tr><td class="line-number" value="2529"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2530"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  arange LogSoftmax trainloader NLLLoss SGD lr stdev</span></td></tr><tr><td class="line-number" value="2531"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2532"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  trainset testset testloader log'd f'Images relu py</span></td></tr><tr><td class="line-number" value="2533"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2534"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  f'Accuracy maxpooling downsample covolution conv</span></td></tr><tr><td class="line-number" value="2535"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2536"></td><td class="line-content"><span class="html-comment">&lt;!--  LocalWords:  MaxPool Conv downsamples fc CrossEntropyLoss</span></td></tr><tr><td class="line-number" value="2537"></td><td class="line-content"><span class="html-comment"> --&gt;</span></td></tr><tr><td class="line-number" value="2538"></td><td class="line-content"><span class="html-end-of-file"></span></td></tr></tbody></table></body></html>