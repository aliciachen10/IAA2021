<!DOCTYPE html>

<html lang="en">

<head>
<meta http-equiv="X-UA-Compatible" content="IE=9" />
<meta charset="UTF-8" />

<!-- Favicon -->

<link rel="icon" type="image/png" href="./favicon.png">

<!-- JQuery package -->

<link type="text/css" href="../../css/redmond/jquery-ui-1.10.2.css" rel="stylesheet" />
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/jquery-ui-1.10.2.min.js"></script>

<!-- Style header for jQuery accordions -->

<style>
.ui-accordion-header {
  font-family: 'Droid Sans', sans-serif;
  font-style: normal;
}
</style>

<!-- d3 -->

<script src="https://d3js.org/d3.v3.min.js" charset="utf-8"></script>

<!-- Google Code hyphenator -->

<script type="text/javascript" src="../../js/hyphenate.js"></script>

<!-- Google fonts stuff -->

<link rel="stylesheet" type="text/css"
  href="https://fonts.googleapis.com/css?family=Noto+Serif:400" />
<link rel="stylesheet" type="text/css"
  href="https://fonts.googleapis.com/css?family=Noto+Serif:400italic" />
<link rel="stylesheet" type="text/css"
  href="https://fonts.googleapis.com/css?family=Droid+Sans:400" />
<link rel="stylesheet" type="text/css"
  href="https://fonts.googleapis.com/css?family=Droid+Sans:700" />

<!-- MathJax math formatting -->

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML">
</script>
-->

<!-- Course page CSS and JS -->

<link type="text/css" href="../../css/course.css" rel="stylesheet" />
<script type="text/javascript" src="../../js/mod-date.js"></script>
<script type="text/javascript" src="./msa-text.js"></script>

<!-- Overrides for standard course CSS -->

<link type="text/css" href="msa-text.css" rel="stylesheet" />


<title>Text Analytics: Institute for Advanced Analytics</title>
</head>

<body bgcolor="white" class="hyphenate">

<div style="text-align: center;">
<span style="
  position: relative;
  display: inline-block;
  width: 100%;
  max-width: 946px;
  height: 435px;
  background-image: url( 'fig/affinity-graph.png' );
  background-repeat: no-repeat;
">
<div style="
  position: absolute;
  align: right;
  bottom: 10px;
  right: 20px;
  font-family: 'Trebuchet MS', Helvetica, sans-serif;
  font-size: 16pt;
  font-weight: normal;
  color: #646464;
">
  <div style="line-height: 90%;">Text Analytics</div>
  <div style="font-size: 10pt;">
    <a target="_blank" href="http://www.csc.ncsu.edu/faculty/healey">
    <i>Christopher G. Healey</i>
    </a>
  </div>
</div>
</span>
</div>

<div style="
  position: absolute;
  align: right;
  top: 10px;
  left: 10px;
">
  <a target="_blank" href="https://www.ncsu.edu">
    <img src="fig/nc-state-logo-blue.png"
     style="
       border-style: none;
       -moz-box-shadow: 1px 1px 8px #646464;
       -webkit-box-shadow: 1px 1px 8px #646464;
       box-shadow: 1px 1px 8px #646464;
     ">
  </a>
</div>

<!--Spacer after image of one "line"-->

<div style="height: 1em;"></div>

<!-- Navigation toolbar -->

<div id="navWrap">
  <div id="nav">
    <ul id="nav-list">
      <li id="nav-intro">Introduction
      <li id="nav-rep">Representation
      <li id="nav-termvec">Term Vectors
      <li id="nav-sim">Similarity
      <li id="nav-cluster">Clustering
      <li id="nav-sum">Summarization
      <li id="nav-sentiment">Sentiment
      <li id="nav-viz">Visualization
    </ul>
  </div>

  <div id="nav-footer">
  </div>
</div>


<h2 id="intro">Introduction</h2>

<p>
This module will provide an introduction to methods for analyzing
text. Text analytics is a complicated topic, covering a wide range of
problems and solutions. For example, consider the following simple
questions. How should text data be structured? What types of analysis
are appropriate for text data? How can these analyses be performed? How
should results from text analytics be presented to a viewer?
</p>

<p>
We will focus on simple approaches to address each of these
questions. In particular, we will discuss different ways to represent
text, and present the use of term frequency to structure a document's
text. We will then present algorithms that use this approach to
measure the similarity between text, to cluster text into topics, to
estimate the sentiment in text, and to present text and text analytics
results to a viewer.
</p>


<h3>Why Text Analytics?</h3>

<p>
Much of the data now being captured and stored
is <i>semi-structured</i> or <i>unstructured:</i> it is not being
saved in a structured format, for example, in a database or data
repository with a formal framework. Text data often makes up a large
part of these unstructured collections: email, web pages, news
articles, books, social network postings, and so on. Text analytics
and text mining are meant to be applied to this type of data to
</p>

<div class="code-div" style="font-family: 'Noto Serif', Times, serif;">
<div style="margin-top: 0.5em; margin-bottom: 0.5em;">
"...find <b>interesting</b> regularities in large textual datasets..."
(Fayad)
</div>

<div style="margin-top: 0.5em; margin-bottom: 0.5em;">
"...find <b>semantic</b> and <b>abstract</b> information from the
surface form of textual data..." (Grobelnik & Mladenic)
</div>
</div>

<p>
where <b>interesting</b> means non-trivial, hidden, previously unknown,
and potentially useful.
</p>

<p>
Text analytics has many potential applications. These can be
<i>discovery driven</i>, where we try to answer specific questions
through text mining, or <i>data driven</i>, where we take a large
collection of text data and try to derive useful information by
analyzing it.
</p>

<p>
There are many reasons why analyzing text is difficult.
</p>

<ul>

<li><b>Abstract.</b> The concepts contained in text are often
difficult to identify and represent (<i>e.g.</i>, sentiment or
sarcasm, "I loved the way you put that. No, really, I LOVED it.")</li>

<li><b>Relationships.</b> Subtle, complex relationships between
concepts must be extracted (<i>e.g.</i>, negation, "I thought I'd
enjoy that movie, but in the end it just didn't work out the way I
expected.")</li>

<li><b>Scale.</b> A document can contain many thousands of words, and
a document collection can contain thousands or hundreds of thousands
of documents.</li>

</ul>

<a name="text_assn"><h3>Assignment</h3></a>

<p>
Each homework group will complete a text analytics assignment. This
involves choosing a problem of interest to address, identifying and
collecting appropriate text data to support the problem, then using
text analysis techniques discussed during this module to analyze your
data and form conclusions and results that will be presented in class.
</p>

<dl>
  <dt>Sept. 21, 11:59pm</dt>
  <dd>
  Submit a draft proposal via the Moodle section on Text Mining (Text
  Mining Project Proposal Submission) that describes the problem you
  will investigate. As taught in communications, the proposal must be
  a Word document so we can return comments through change
  tracking. The proposal <b>must</b> include the following items.

  <ul class="sub">
    <li>A problem statement explaining the issue you want to study and
    the goals you plan to achieve.</li>

    <li>A description of the dataset(s) you will use, detailing either
    where the data is available, or how it will be collected. Provide
    enough specificity for us to confirm that the data will be
    available in a timely manner.</li>

    <li>A description of the types of analysis you plan to apply to
    your datasets to achieve the goals listed in your problem
    statement.</li>

    <li>A detailed justification of <em>why</em> the data you plan to
    collect will support the problem and analysis you propose to
    complete. Projects that we do not feel have a high probability of
    success will be returned and will need to be improved or replaced
    with a different proposal.</li>

    <li>A list of the results you will present at the end of your
    analysis.</li>
  </ul>

  The draft proposal should be approximately one page in length. We
  have provided <a target="_blank" href="./proposal_doc.html">an
  example draft proposal</a> to give you an idea of its length,
  content, and format. Kate Bagley and Andrea Villanes will schedule
  office hours on the 23rd to provide both communications and
  technical feedback as necessary.
  </dd>

  <dt>Sept. 24, 11:59pm</dt>
  <dd>
  Submit a revised proposal through the Moodle section on Text Mining
  (Text Mining Project Revised Submission) that addresses comments
  and/or concerns made by the instructors on your draft proposal. The
  revised proposal represents a template for what we expect to see
  during your final presentation.
  </dd>

  <dt>Oct. 12, 9:00am&ndash;12:00pm (orange), 1:00pm&ndash;4:00pm (blue)</dt>
  <dd>
  Present your project and its results to the class. Each group will
  be allotted 10 minutes: 8 minutes to present, and 2 minutes to
  answer one or two questions. Because of our tight schedule, <b>each
  group must provide their slides to Andrea by 12pm on Oct. 11</b> through
  the Moodle section on Text Mining (Text Mining Project
  Submission). This will allow us to review the presentations
  ahead of time. During presentations, groups will screen share their
  slides, so you are expected to be prepared and ready to being
  presenting immediately when your group is scheduled. Each group
  will be strictly limited to 10 minutes for their presentations
  (<i>i.e.</i>, 4&ndash;5 slides for the average presentation). Please
  plan your presentations accordingly. For example, consider having
  only 1 or 2 groups members present your slides, then have the entire
  team available for questions at the end.<br><br>

  You must attend all of the presentations for your cohort. Attending
  presentations for the opposite cohort is optional, but certainly
  encouraged if you want to see the work they did.</li>
  </dd>
</dl>

<!--
<p>
As an example of potential sources of text data, Dr. Xie has made
available the <a target="_blank"
href="http://research.csc.ncsu.edu/ase/courses/analytics/textmining/textdata2011.html">proposed
text sources from the 2011 Analytics class</a>.
</p>
-->


<h3>NLTK, Gensim, and Scikit-Learn</h3>

<p>
Throughout this module we will provide code examples in Python using
the <a href="http://www.nltk.org" target="_blank">Natural Language
Toolkit (NLTK)</a>. NLTK is designed to support natural language
processing and analysis of human language data. It includes the
ability to perform many different language processing operations,
including all of the text analytics techniques we will be discussing.
</p>

<p>
For techniques beyond the scope of NLTK, we will provide Python
examples that use either <a href="http://radimrehurek.com/gensim/"
target="_blank">Gensim</a>, a more sophisticated text analysis package
that includes the text similarity algorithms we will discuss during
the module. We will also discuss the text analytics preprocessing
capabilities built into <a href="http://scikit-learn.org/stable/"
target="_blank">scikit-learn</a>.
</p>


<h3>Tweet Capture Tool</h3>

<p>
Capturing data from a social network site
like <a href="https://dev.twitter.com/overview/documentation">Twitter</a>
or <a href="https://developers.facebook.com/">Facebook</a> normally
requires using a programming language (e.g., Python) to talk to the
application programming interface (API) the site provides to allow you
to access its data.
</p>

<p>
In order to simplify this process, we are
providing <a href="./TweetCapture.exe">a Tweet Capture tool</a> that
requests tweets by keyword from Twitter's real-time tweet stream. To
use the tool, you will need to register for application credentials
from the Twitter Developer site. Instructions for how to request and
use your keys are included in the Help section of the program.
</p>


<h2 id="rep">Text Representations</h2>

<p>
There are many different ways to represent text. We describe some of
the most common approaches, discussing briefly their advantages and
disadvantages.
</p>

<h3>Character</h3>

<p>
The simplest representation views text as an ordered collection of
characters. A text document is described by the frequency of sequences
of characters of different lengths. For example, consider the
following text data.
</p>

<div class="code-div">
To be or not to be
</div>

<p>
This could be represented by a set of single character frequencies
<i>fq</i> (1-grams), a set of two-character frequencies (2-grams), and
so on.
</p>

<div style="overflow: hidden;">

<div style="width: 50%; float: left;">
<table style="margin-left: auto; margin-right: 4em;">
<tr>
  <th></th>
  <th>b</th>
  <th>e</th>
  <th>n</th>
  <th>o</th>
  <th>r</th>
  <th>t</th>
</tr>

<tr class="even">
  <td><i>fq</i></td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">4</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">3</td>
</tr>
</table>
</div>

<div style="width: 50%; float: left;">
<table style="margin-left: 4em; margin-right: auto;">
<tr>
  <th></th>
  <th>be</th>
  <th>no</th>
  <th>or</th>
  <th>ot</th>
  <th>to</th>
</tr>

<tr class="even">
  <td><i>fq</i></td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
</tr>
</table>
</div>

</div>

<p>
Character representations have the advantage of being simple and
fairly unambiguous, since they avoid many common language issues
(homonyms, synonyms, etc.) They also allow us to compress large
amounts of text into a relative compact representation. Unfortunately,
character representations provide almost no access to the semantic
properties of text data, making them a poor representation for
analyzing <i>meaning</i> in the data.
</p>

<h3>Word</h3>

<p>
A very common representation of text is to convert a document into
individual words or terms. Our example sentence represented as words
would look something like this.
</p>


<table style="margin-left: auto; margin-right: auto;">
<tr>
  <th></th>
  <th>be</th>
  <th>not</th>
  <th>or</th>
  <th>to</th>
</tr>

<tr class="even">
  <td><i>fq</i></td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
</tr>
</table>

<p>
Words strike a good balance between simplicity and semantics. At this
level various ambiguities begin to arise, however.
</p>

<ul>

<li><b>Homonym.</b> Words with identical form but different meaning
(<i>e.g.</i>, lie: to recline versus lie: not telling the truth; or
bass: a musical instrument versus bass: a fish).</li>

<li><b>Synonym.</b> Words with different form but the same or similar
meaning (<i>e.g.</i>, ambiguous, confusing, opaque, uncertain,
unclear, vague).</li>

<li><b>Polysemy.</b> Words with the same form and multiple related
meanings (<i>e.g.</i>, bank: a financial institution and bank: to rely
upon, as in "You can bank on me").</li>

<li><b>Hyponym.</b> Words that are a semantic subclass of another
word, forming a <i>type-of</i> relationship (<i>e.g.</i> salmon
is a hyponym of fish).</li>

</ul>

<p>
It's also the case that some words may be more useful than others, due
to their commonality. Suppose we're trying to determine the similarity
between the text of two documents that discuss the financial
crisis. Would comparing the frequency of the word like "an" be as
useful as comparing the frequency of a word like "investment" or
"collapse"?
</p>


<h3>Phrase</h3>

<p>
Combining words together forms phrases, which are often called
<i>n</i>-grams when <i>n</i> words are combined. Phrases may be
contiguous, "Mary switches her table lamp off" &rArr; "table lamp
off", or non-contiguous, "Mary switches her table lamp off" &rArr;
{&thinsp;lamp, off, switches&thinsp;}. An important advantage of
phrase representations is that they often give a better meaning to the
semantics of sense of the individual words in a phrase (<i>e.g.</i>,
"lie down" versus "lie shamelessly").
</p>

<p>
Google has published a number of online applications and datasets
related to <i>n</i>-grams. One example is the <a target="_blank"
href="https://books.google.com/ngrams">Ngram Viewer</a> which allows
you to compare the occurrence of <i>n</i>-grams in books Google has
indexed over a range of years. The Ngram Viewer allows not only
explicit phrase searches, but <a target="_blank"
href="https://books.google.com/ngrams/info">also searches that include
wildcards, inflection, or part-of-speech</a>. They have
also <a target="_blank"
href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">made
their underlying <i>n</i>-gram datasets available</a> to interested
researchers.
</p>


<h3>Part of Speech</h3>

<p>
Words can be further enriched by performing part-of-speech
tagging. Common parts of speech in English include nouns, verbs,
adjectives, adverbs, and so on. Part-of-speech tagging is often used
to filter a document, allowing us to restrict analysis to "information
rich" parts of the text like nouns and verbs or noun phrases. The
Cognitive Computation Group at UIUC provides <a target="_blank"
href="http://cogcomp.cs.illinois.edu/demo/pos/">a comprehensive
part-of-speech tagger</a> as a web-based application.
</p>


<h3>WordNet</h3>

<p>
<a target="_blank" href="http://wordnet.princeton.edu">WordNet</a> is a
lexical database of English words. In its simplest form, WordNet
contains four databases of nouns, verbs, adjectives, and adverbs.
Like a thesaurus, WordNet groups synonymous words into <i>synsets</i>,
sets of words with similar meanings. Unlike a thesaurus, however,
WordNet forms <i>conceptual relations</i> between synsets based on
semantics. For example, for the noun database the following relations
are defined.
</p>

<table class="center">
<tr>
  <th>Relation</th>
  <th>Explanation</th>
  <th>Example</th>
</tr>

<tr class="even">
  <td style="white-space: nowrap;">hyponym</td>
  <td>From lower to higher level <i>type-of</i> concept, <i>X</i> is a
  hyponym of <i>Y</i> if <i>X</i> is a type of <i>Y</i></td>
  <td style="white-space: nowrap;"><i>dalmatian</i> is a hyponym
  of <i>dog</i></td>
</tr>

<tr class="odd">
  <td style="white-space: nowrap;">hypernym</td>
  <td>From higher to lower level <i>subordinate</i> concept, <i>X</i>
  is a hypernym of <i>Y</i> if <i>Y</i> is a type of <i>X</i></td>
  <td style="white-space: nowrap;"><i>canine</i> is a hypernym
  of <i>dog</i></td>
</tr>

<tr class="even">
  <td style="white-space: nowrap;">meronym</td>
  <td><i>Has-member</i> concept from group to members, <i>X</i> is a
  meronym of <i>Y</i> if <i>X</i>s are members of <i>Y</i></td>
  <td style="white-space: nowrap;"><i>professor</i> is a meronym
  of <i>faculty</i></td>
</tr>

<tr class="odd">
  <td style="white-space: nowrap;">holonym</td>
  <td><i>Is-member</i> concept from members to group, <i>X</i> is a
  holonym of <i>Y</i> if <i>Y</i>s are members of <i>X</i></td>
  <td style="white-space: nowrap;"><i>grapevine</i> is a holonym
  of <i>grape</i></td>
</tr>

<tr class="even">
  <td style="white-space: nowrap;">part meronym</td>
  <td><i>Has-part</i> concept from composite to part</td>
  <td style="white-space: nowrap;"><i>leg</i> is a part meronym
  of <i>table</i></td>
</tr>

<tr class="odd">
  <td style="white-space: nowrap;">part holonym</td>
  <td><i>Is-part</i> concept from part to composite</td>
  <td style="white-space: nowrap;"><i>human</i> is a part holonym
  of <i>foot</i></td>
</tr>
</table>

<p>
WordNet also includes general to specific <i>troponym</i> relations
between verbs: communicate&ndash;talk&ndash;whisper,
move&ndash;jog&ndash;run, or like&ndash;love&ndash;idolize; and
<i>antonym</i> relations between verbs and adjectives: wet&ndash;dry,
young&ndash;old or like&ndash;hate. Finally, WordNet provides
a brief explanation (or <i>gloss</i>) and example sentences for
each of its synsets.
</p>

<p>
WordNet is extremely useful for performing tasks like part-of-speech
tagging or word disambiguation. <a target="_blank"
href="http://wordnetweb.princeton.edu/perl/webwn">WordNet's databases
can be searched</a> through an online web application. The databases
<a target="_blank"
href="http://wordnet.princeton.edu/wordnet/download/">can also be
downloaded</a> for use by researchers and practitioners.
</p>


<h3>Text Representation Analytics</h3>

<p>
Dr. Peter Norvig, a leading artificial intelligence researcher and
Director of Research at Google, recently
<a href="http://norvig.com/mayzner.html" target="_blank">complied a
set of statistics</a> about character, <i>n</i>-gram, and word
frequencies based on the <a href="http://books.google.com"
target="_blank">Google Books archive</a>. His results showed some
interesting similarities and differences to
the <a href="https://books.google.com/books/about/Tables_of_Single_letter_and_Digram_Frequ.html?id=FI7BHgAACAAJ"
target="_blank">seminal work of Mark Mayzner</a>, who studied the
original frequency of letters in the English language in the
1960s. The video below provides an interesting overview of
Dr. Norvig's findings.
</p>

<p style="margin-top: 1.5em; text-align: center;">
<iframe width="560" height="315" style="border: 2px solid gray;"
src="https://www.youtube.com/embed/7XQRduB6oTM" frameborder="0"
allowfullscreen></iframe>
</p>


<h3>Practice Problem 1</h3>

<p>Take the following except from John Steinbeck's famous novel, <i>Of
Mice and Men</i>.</p>

<div class="code-div">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  Two men, dressed in denim jackets and trousers and wearing "black,
  shapeless hats," walk single-file down a path near the pool. Both
  men carry blanket rolls — called bindles — on their shoulders. The
  smaller, wiry man is George Milton. Behind him is Lennie Small, a
  huge man with large eyes and sloping shoulders, walking at a gait
  that makes him resemble a huge bear.<br><br>

  When Lennie drops near the pool's edge and begins to drink like a
  hungry animal, George cautions him that the water may not be
  good. This advice is necessary because Lennie is retarded and
  doesn't realize the possible dangers. The two are on their way to a
  ranch where they can get temporary work, and George warns Lennie not
  to say anything when they arrive. Because Lennie forgets things very
  quickly, George must make him repeat even the simplest instructions.
  <br><br>

  Lennie also likes to pet soft things. In his pocket, he has a dead
  mouse which George confiscates and throws into the weeds beyond the
  pond. Lennie retrieves the dead mouse, and George once again catches
  him and gives Lennie a lecture about the trouble he causes when he
  wants to pet soft things (they were run out of the last town because
  Lennie touched a girl's soft dress, and she screamed). Lennie offers
  to leave and go live in a cave, causing George to soften his
  complaint and tell Lennie perhaps they can get him a puppy that can
  withstand Lennie's petting.<br><br>

  As they get ready to eat and sleep for the night, Lennie asks George
  to repeat their dream of having their own ranch where Lennie will be
  able to tend rabbits. George does so and then warns Lennie that, if
  anything bad happens, Lennie is to come back to this spot and hide
  in the brush. Before George falls asleep, Lennie tells him they must
  have many rabbits of various colors.
  </code>
  </div>
</div>

<p>Convert this text into four separate text representations:</p>

<ul>
<li>character</li>
<li>term</li>
<li>bigram, pairs of adjacent terms
<li>term with appropriate part-of-speech disambiguation
</ul>

<p>You can use Python's <code>nltk</code> library to assign part-of-speech
tags to terms in a term list.</p>

<div class="code-div">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  >>> import nltk<br>
  >>> nltk.download( 'averaged_perceptron_tagger' )<br><br>

  >>> text = 'And now for something completely different'<br>
  >>> tok = text.split( ' ' )<br>
  >>> POS = nltk.pos_tag( tok )<br>
  >>> print( POS )<br>
  [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]
  </code>
  </div>
</div>

<p>The various part-of-speech tags like <code>CC</code> (coordinating
conjunction), <code>RB</code> (adverb), and <code>IN</code>
(preposition) are part of
the <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
target="_blank">Penn Treebank part-of-speech tag list</a>, which is
available online.</p>

<div id="prac-prob-1" class="detail">
  <h3>Practice Problem 1 Solutions</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>The following four snippets of Python code will perform
    character frequency, term frequency, contiguous bigram frequency,
    and term-POS frequency. Results are return as a printed list, and
    in some cases (for your edification,) as a pyplot bar graph.</p>

    <p>Note that the solutions assume you copy text to a sequence of
    Jupyter Notebook cells, so definitions from the initial solutions
    are available in later code.</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-01-img" class="code-div-img"></span>
        <div id="code-01">
        >>> import re<br>
        >>> import matplotlib.pyplot as plt<br>
        >>> import numpy as np<br>
        >>><br>
        >>> txt = 'Two men, dressed in denim jackets and trousers and wearing "black, shapeless hats," walk single-file down a path near the pool. Both men carry blanket rolls — called bindles — on their shoulders. The smaller, wiry man is George Milton. Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear. When Lennie drops near the pool\'s edge and begins to drink like a hungry animal, George cautions him that the water may not be good. This advice is necessary because Lennie is retarded and doesn\'t realize the possible dangers. The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive. Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions. Lennie also likes to pet soft things. In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond. Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl\'s soft dress, and she screamed). Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie\'s petting. As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits. George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush. Before George falls asleep, Lennie tells him they must have many rabbits of various colors.'<br>
  
        >>> <br>
        >>> def print_dict( d ):<br>
        ... <span class="tab-1">"""Print frequency dictionary. Key is 'representation', value is<br>
        ... <span class="tab-1">frequency of representation.</span><br>
        ... <br>
        ... <span class="tab-1">Args:</span><br>
        ... <span class="tab-1">&nbsp d (dict):  Dictionary of (rep,freq) pairs</span><br>
        ... <span class="tab-1">"""</span><br>
        ... <br>
        ... <span class="tab-1">keys = list( d.keys() )</span><br>
        ... <span class="tab-1">keys.sort()</span><br>
        ... <br>
        ... <span class="tab-1">for k in keys:</span><br>
        ... <span class="tab-2">print( f'{k}: {d[ k ]}; ', end='' )</span><br>
        ... <span class="tab-1">print( '' )</span><br>
        >>> <br>
        >>> <br>
        >>> #  Character representation, both w/ and w/o punctuation<br>
        >>><br>
        >>> #  Convert to lower case, use regex to create a version with no punctuation<br>
        >>><br>
        >>> t = txt.lower()<br>
        >>> t_no_punc = re.sub( r'[^\w\s]', '', t )<br>
        >>><br>
        >>> #  Create punc, no punc dictionaries to hold character frequencies<br>
        >>><br>
        >>> char_dict = { }<br>
        >>> char_dict_no_punc = { }<br>
        >>><br>
        >>> #  Count characters in text w/punctuation<br>
        >>><br>
        >>> for c in t:<br>
        ... <span class="tab-1">char_dict[ c ] = ( 1 if c not in char_dict else char_dict[ c ] + 1 )</span><br>
        >>><br>   
        >>> #  Print results<br>
        >>><br>
        >>> print( 'Character frequency' )<br>
        >>> print_dict( char_dict )<br>
        >>><br>   
        >>> for c in t_no_punc:<br>
        ... <span class="tab-1">char_dict_no_punc[ c ] = ( 1 if c not in char_dict_no_punc else char_dict_no_punc[ c ] + 1 )</span><br>
        >>><br>
        >>> print( 'Character frequency w/o punctuation' )<br>
        >>> print_dict( char_dict_no_punc )<br>
        >>><br>
        >>> #  Plot as bar graph<br>
        >>><br>
        >>> char = list( char_dict.keys() )<br>
        >>> char.sort()<br>
        >>><br>
        >>> freq = [ ]<br>
        >>> for c in char:<br>
        ... <span class="tab-1">freq = freq + [ char_dict[ c ] ]</span><br>
         >>><br>   
        ... <span class="tab-1">#  Add any character in punctuation dict but not in no punctuation dict w/freq of zero</span><br>
        ... <span class="tab-1"></span><br>  
        ... <span class="tab-1">if c not in char_dict_no_punc:</span><br>
        ... <span class="tab-2">char_dict_no_punc[ c ] = 0</span><br>
        ... <span class="tab-1"></span><br>
        >>> char_no_punc = list( char_dict_no_punc.keys() )<br>
        >>> char_no_punc.sort()<br>
        >>> <br>
        >>> freq_no_punc = [ ]<br>
        >>> for c in char_no_punc:<br>
        ... <span class="tab-1">freq_no_punc = freq_no_punc + [ char_dict_no_punc[ c ] ]</span><br>
        >>> <br>
        >>> X = np.arange( len( freq ) )<br>
        >>> w = 0.35<br>
        >>><br>
        >>> fig = plt.figure( figsize=(10,5) )<br>
        >>> ax = fig.add_axes( [ 0, 0, 1, 1 ] )<br>
        >>> ax.bar( X + 0.00, freq, color='b', width=w, label='w/punc' )<br>
        >>> ax.bar( X + 0.33, freq_no_punc, color='orange', width=w, label='w/o punc' )<br>
        >>><br>
        >>> plt.ylabel( 'Frequency' )<br>
        >>> plt.xlabel( 'Character' )<br>
        >>> plt.xticks( X + w / 2, char )<br>
        >>> plt.legend( loc='best' )<br>
        >>> plt.show()<br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p>Enumerate and report term frequencies.</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-02-img" class="code-div-img"></span>
        <div id="code-02">
        >>> #  Term frequencies<br>
        >>><br>
        >>> #  Convert text to lower-case term tokens<br>
        >>><br>
        >>> t = re.sub( r'[^\w\s]', '', txt )<br>
        >>> tok = t.lower().split()<br>
        >>><br>
        >>> #  Count term frequencies<br>
        >>><br>
        >>> d = { }<br>
        >>> for term in tok:<br>
        ... <span class="tab-1">d[ term ] = ( 1 if term not in d else d[ term ] + 1 )</span><br>
        >>><br>   
        >>> #  Print results<br>
        >>><br>
        >>> print( 'Term frequencies' )<br>
        >>> print_dict( d )<br>
        >>><br>
        >>> #  Plot as bar graph<br>
        >>><br>
        >>> term = list( d.keys() )<br>
        >>> term.sort()<br>
        >>><br>
        >>> freq = [ ]<br>
        >>> for t in term:<br>
        ... <span class="tab-1">freq = freq + [ d[ t ] ]</span><br>
        >>><br>
        >>> x_pos = range( len( term ) )<br>
        >>> fig = plt.figure( figsize=(15,40) )<br>
        >>> ax = fig.add_axes( [ 0, 0, 1, 1 ] )<br>
        >>> ax.barh( x_pos, freq, color='b' )<br>
        >>><br>
        >>> plt.ylabel( 'Term' )<br>
        >>> plt.xlabel( 'Frequency' )<br>
        >>> plt.yticks( x_pos, term )<br>
        >>> plt.show()<br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p>Enumerate and report bigram frequencies.</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-03-img" class="code-div-img"></span>
        <div id="code-03">
        >>> #  Bigram frequencies<br>
        >>><br>
        >>> #  Convert text to lower-case term tokens<br>
        >>><br>
        >>> t = re.sub( r'[^\w\s]', '', txt )<br>
        >>> tok = t.lower().split()<br>
        >>><br>
        >>> #  Build bigrams, count frequencies<br>
        >>><br>
        >>> d = { }<br>
        >>> for i in range( 1, len( tok ) ):<br>
        ... <span class="tab-1">bigram = (tok[ i - 1 ],tok[ i ] )</span><br>
        ... <span class="tab-1">d[ bigram ] = ( 1 if bigram not in d else d[ bigram ] + 1 )</span><br>
        >>><br>   
        >>> #  Print results<br>
        >>><br>
        >>> print( 'Bigram frequencies' )<br>
        >>> print_dict( d )<br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p>Enumerate and report (term,POS) frequencies, then plot the
    frequency of each part of speech.</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-04-img" class="code-div-img"></span>
        <div id="code-04">
        >>> import nltk<br>
        >>> <br>
        >>> def POS_expand( term_POS ):<br>
        ... <span class="tab-1">"""Convert second elements of tuple, shortened POS tag, to expanded POS description</span><br>
        ... <br>   
        ... <span class="tab-1">Args:</span><br>
        ... <span class="tab-1">  term_POS (tuple):  Tuple of (term,POS-tag)</span><br>
        ... <br>
        ... <span class="tab-1">Returns:</span><br>
        ... <span class="tab-1">(tuple):  Tuple of (term,POS-tag-plus-description)</span><br>
        ... <span class="tab-1">"""</span><br>
        ... <br>
        ... <span class="tab-1">tag = term_POS[ 1 ]</span><br>
        ... <span class="tab-1">if tag == 'CC':</span><br>
        ... <span class="tab-2">exp = 'coordinating conjunction'</span><br>
        ... <span class="tab-1">elif tag == 'CD':</span><br>
        ... <span class="tab-2">exp = 'cardinal number'</span><br>
        ... <span class="tab-1">elif tag == 'DT':</span><br>
        ... <span class="tab-2">exp = 'determiner'</span><br>
        ... <span class="tab-1">elif tag == 'EX':</span><br>
        ... <span class="tab-2">exp = 'existential there'</span><br>
        ... <span class="tab-1">elif tag == 'FW':</span><br>
        ... <span class="tab-2">exp = 'foreign word'</span><br>
        ... <span class="tab-1">elif tag == 'IN':</span><br>
        ... <span class="tab-2">exp = 'preposition'</span><br>
        ... <span class="tab-1">elif tag == 'JJ':</span><br>
        ... <span class="tab-2">exp = 'adjective'</span><br>
        ... <span class="tab-1">elif tag == 'JJR':</span><br>
        ... <span class="tab-2">exp = 'comparative adjective'</span><br>
        ... <span class="tab-1">elif tag == 'JJS':</span><br>
        ... <span class="tab-2">exp = 'superlative adjective'</span><br>
        ... <span class="tab-1">elif tag == 'LS':</span><br>
        ... <span class="tab-2">exp = 'list item marker'</span><br>
        ... <span class="tab-1">elif tag == 'MD':</span><br>
        ... <span class="tab-2">exp = 'modal'</span><br>
        ... <span class="tab-1">elif tag == 'NN':</span><br>
        ... <span class="tab-2">exp = 'noun'</span><br>
        ... <span class="tab-1">elif tag == 'NNS':</span><br>
        ... <span class="tab-2">exp = 'plural noun'</span><br>
        ... <span class="tab-1">elif tag == 'NNP':</span><br>
        ... <span class="tab-2">exp = 'proper noun'</span><br>
        ... <span class="tab-1">elif tag == 'NNPS':</span><br>
        ... <span class="tab-2">exp = 'plural proper noun'</span><br>
        ... <span class="tab-1">elif tag == 'PDT':</span><br>
        ... <span class="tab-2">exp = 'predeterminer'</span><br>
        ... <span class="tab-1">elif tag == 'POS':</span><br>
        ... <span class="tab-2">exp = 'possessive ending'</span><br>
        ... <span class="tab-1">elif tag == 'PRP':</span><br>
        ... <span class="tab-2">exp = 'personal pronoun'</span><br>
        ... <span class="tab-1">elif tag == 'PRP$':</span><br>
        ... <span class="tab-2">exp = 'possessive pronoun'</span><br>
        ... <span class="tab-1">elif tag == 'RB':</span><br>
        ... <span class="tab-2">exp = 'adverb'</span><br>
        ... <span class="tab-1">elif tag == 'RBR':</span><br>
        ... <span class="tab-2">exp = 'comparative adverb'</span><br>
        ... <span class="tab-1">elif tag == 'RBS':</span><br>
        ... <span class="tab-2">exp = 'superlative adverb'</span><br>
        ... <span class="tab-1">elif tag == 'RP':</span><br>
        ... <span class="tab-2">exp = 'particle'</span><br>
        ... <span class="tab-1">elif tag == 'SYM':</span><br>
        ... <span class="tab-2">exp = 'symbol'</span><br>
        ... <span class="tab-1">elif tag == 'TO':</span><br>
        ... <span class="tab-2">exp = 'to'</span><br>
        ... <span class="tab-1">elif tag == 'UH':</span><br>
        ... <span class="tab-2">exp = 'interjection'</span><br>
        ... <span class="tab-1">elif tag == 'VB':</span><br>
        ... <span class="tab-2">exp = 'verb'</span><br>
        ... <span class="tab-1">elif tag == 'VBD':</span><br>
        ... <span class="tab-2">exp = 'past tense verb'</span><br>
        ... <span class="tab-1">elif tag == 'VBG':</span><br>
        ... <span class="tab-2">exp = 'gerund verb'</span><br>
        ... <span class="tab-1">elif tag == 'VBN':</span><br>
        ... <span class="tab-2">exp = 'past participle verb'</span><br>
        ... <span class="tab-1">elif tag == 'VBP':</span><br>
        ... <span class="tab-2">exp = 'non-3rd person verb'</span><br>
        ... <span class="tab-1">elif tag == 'VBZ':</span><br>
        ... <span class="tab-2">exp = '3rd person verb'</span><br>
        ... <span class="tab-1">elif tag == 'WDT':</span><br>
        ... <span class="tab-2">exp = 'wh-determiner'</span><br>
        ... <span class="tab-1">elif tag == 'WP':</span><br>
        ... <span class="tab-2">exp = 'wh-pronoun'</span><br>
        ... <span class="tab-1">elif tag == 'WP$':</span><br>
        ... <span class="tab-2">exp = 'possessive wh-pronoun'</span><br>
        ... <span class="tab-1">elif tag == 'WRB':</span><br>
        ... <span class="tab-2">exp = 'wh-adverb'</span><br>
        ... <span class="tab-1">else:</span><br>
        ... <span class="tab-2">exp = 'default'</span><br>
        ... <br>    
        ... <span class="tab-1">return (term_POS[ 0 ],tag + ' ' + exp)</span><br>
        >>><br>
        >>> #  (term,part-of-speech) frequencies<br>
        >>><br>
        >>> #  Convert text to lower-case term tokens, POS tag them<br>
        >>><br>
        >>> t = re.sub( r'[^\w\s]', '', txt )<br>
        >>> tok = t.lower().split()<br>
        >>> tok = nltk.pos_tag( tok )<br>
        >>><br>
        >>> #  Build (term,POS) and POS dictionaries, count frequencies<br>
        >>><br>
        >>> d = { }<br>
        >>> d_POS = { }<br>
        >>> <br>
        >>> for term_POS in tok:<br>
        ... <span class="tab-1">term_POS = POS_expand( term_POS )</span><br>
        ... <span class="tab-1">d[ term_POS ] = ( 1 if term_POS not in d else d[ term_POS ] + 1 )</span><br>
        ... <br>
        ... <span class="tab-1">POS = term_POS[ 1 ]</span><br>
        ... <span class="tab-1">d_POS[ POS ] = ( 1 if POS not in d_POS else d_POS[ POS ] + 1 )</span><br>
        >>><br>   
        >>> #  Print results<br>
        >>><br>
        >>> print( '(term,POS) frequencies' )<br>
        >>> print_dict( d )<br>
        >>> print( 'POS frequencies' )<br>
        >>> print_dict( d_POS )<br>
        >>><br>
        >>>#  Plot POS frequencies as bar graph<br>
        >>><br>
        >>> POS = list( d_POS.keys() )<br>
        >>> POS.sort()<br>
        >>><br>
        >>> freq = [ ]<br>
        >>> for p in POS:<br>
        ... <span class="tab-1">freq = freq + [ d_POS[ p ] ]</span><br>
        >>><br>
        >>> x_pos = range( len( POS ) )<br>
        >>> fig = plt.figure( figsize=(10,10) )<br>
        >>> ax = fig.add_axes( [ 0, 0, 1, 1 ] )<br>
        >>> ax.barh( x_pos, freq, color='b' )<br>
        >>><br>
        >>> plt.ylabel( 'POS' )<br>
        >>> plt.xlabel( 'Frequency' )<br>
        >>> plt.yticks( x_pos, POS )<br>
        >>> plt.show()<br>
        </div>
      </td>
      </tr>
      </table>
    </div>

  </div>
</div>


<h2 id="termvec">Term Vectors</h2>

<p>
As discussed above, perhaps the most common method of representing
text is by individual words or terms. Syntactically, this approach
converts the text in document
<i>D</i> into a term vector <i>D<sub>j</sub></i>. Each entry in
<i>D<sub>j</sub></i> corresponds to a specific
term <i>t<sub>i</sub></i>, and its value defines the frequency of
<i>t<sub>i</sub></i> &isin; <i>D<sub>j</sub></i>. Other possible
approaches include language modelling, which tries to predict the
probabilities of specific sequences of terms, and natural language
processing (NLP), which converts text into <i>parse trees</i> that
include parts of speech and a hierarchical breakdown of phrases and
sentences based on rules of grammar. Although useful for a variety of
tasks (<i>e.g.</i>, optical character recognition, spell checking, or
language understanding), language modelling and NLP are normally too
specific or too complex for our purposes.
</p>

<p>
As an example of term vectors, suppose we had the following four
documents.
</p>

<div class="code-div">
  <div><b>Document 1</b></div>

  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  It is a far, far better thing I do, than I have ever done
  </code>
  </div>

  <div><b>Document 2</b></div>

  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  Call me Ishmael
  </code>
  </div>

  <div><b>Document 3</b></div>

  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  Is this a dagger I see before me?
  </code>
  </div>

  <div><b>Document 4</b></div>

  <div style="margin-left: 1em;">
  <code>
  O happy dagger
  </code>
  </div>
</div>

<p>
Taking the union of the documents' unique terms, the documents
produce the following term vectors.
</p>

<table class="center">
<tr style="font-size: 0.8em;">
  <th></th>
  <th>a</th>
  <th>before</th>
  <th>better</th>
  <th>call</th>
  <th>dagger</th>
  <th>do</th>
  <th>done</th>
  <th>ever</th>
  <th>far</th>
  <th>happy</th>
  <th>have</th>
  <th>i</th>
  <th>is</th>
  <th>ishmael</th>
  <th>it</th>
  <th>me</th>
  <th>o</th>
  <th>see</th>
  <th>than</th>
  <th>thing</th>
  <th>this</th>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>1</sub></i></b></td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>2</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>3</sub></i></b></td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>4</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>
</table>

<p>
Intuitively, the overlap between term vectors might provide some clues
about the similarity between documents. In our example, there is no
overlap between <i>D<sub>1</sub></i> and <i>D<sub>2</sub></i>, but a
three-term overlap between <i>D<sub>1</sub></i>
and <i>D<sub>3</sub></i>, and a one-term overlap
between <i>D<sub>3</sub></i> and <i>D<sub>4</sub></i>.
</p>

<div id="term-vec-nltk" class="detail">
  <h3>NLTK Term Vectors</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>The following Python NLTK code snippet will create the same
    four documents from our example as a Python list, strip
    punctuation characters from the documents, tokenize them into four
    separate token (or term) vectors, then print the term vectors.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">import gensim
import nltk
import re
import string

# Create initial documents list

doc = [ ]
doc.append( 'It is a far, far better thing I do, than I have every done' )
doc.append( 'Call me Ishmael' )
doc.append( 'Is this a dagger I see before me?' )
doc.append( 'O happy dagger' )

# Remove punctuation, then tokenize documents

punc = re.compile( '[%s]' % re.escape( string.punctuation ) )
term_vec = [ ]

for d in doc:
    d = d.lower()
    d = punc.sub( '', d )
    term_vec.append( nltk.word_tokenize( d ) )

# Print resulting term vectors

for vec in term_vec:
    print vec
</code>

    </div>

    <p>Running this code in Python produces a list of term vectors
    identical to the table shown above.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">['it', 'is', 'a', 'far', 'far', 'better', 'thing', 'i', 'do', 'than', 'i', 'have', 'ever', 'done']
['call', 'me', 'ishmael']
['is', 'this', 'a', 'dagger', 'i', 'see', 'before', 'me']
['o', 'happy', 'dagger']
</code>

    </div>
  </div>
</div>


<h3>Stop Words</h3>

<p>
A common preprocessing step during text analytics is to remove
<i>stop words</i>, words that are common in text but that do not
provide any useful context or semantics. Removing stop words is
simple, since it can be performed in a single pass over the text.
There is no single, definitive stop word list. Here is one fairly
extensive example.
<p>

<div class="donthyphenate code-div">
<!--
a about above after again against all am an and any are aren't as
at be because been before being below between both but by can't cannot
could couldn't did didn't do does doesn't doing don't down during each
few for from further had hadn't has hasn't have haven't having he he'd
he'll he's her here here's hers herself him himself his how how's i
i'd i'll i'm i've if in into is isn't it it's its itself let's me more
most mustn't my myself no nor not of off on once only or other ought
our ours ourselves out over own same shan't she she'd she'll she's
should shouldn't so some such than that that's the their theirs them
themselves then there there's these they they'd they'll they're
they've this those through to too under until up very was wasn't we
we'd we'll we're we've were weren't what what's when when's where
where's which while who who's whom why why's with won't would wouldn't
you you'd you'll you're you've your yours yourself yourselves
-->
a about above after again against all am an and any are as at be
because been before being below between both but by can did do does
doing don down during each few for from further had has have having he
her here hers herself him himself his how i if in into is it its
itself just me more most my myself no nor not now of off on once only
or other our ours ourselves out over own s same she should so some
such t than that the their theirs them themselves then there these
they this those through to too under until up very was we were what
when where which while who whom why will with you your yours yourself
yourselves
</div>

<p>
Applying stop word removal to our initial four document example would
significantly shorten their term vectors.
</p>

<table class="center">
<tr style="font-size: 0.8em;">
  <th></th>
  <th>better</th>
  <th>call</th>
  <th>dagger</th>
  <th>done</th>
  <th>ever</th>
  <th>far</th>
  <th>happy</th>
  <th>ishmael</th>
  <th>o</th>
  <th>see</th>
  <th>thing</th>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>1</sub></i></b></td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>2</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>3</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>4</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>
</table>

<p>
Notice that the overlap between <i>D<sub>1</sub></i>
and <i>D<sub>3</sub></i>, which was based on stop words, has
vanished. The only remaining overlap is between <i>D<sub>3</sub></i>
and <i>D<sub>4</sub></i>.

<p>
As with all operations, removing stop words is normally appropriate,
but not always. The classic example is the sentence "To be or not to
be." Removing stop words eliminates the entire sentence, which could
be problematic. Consider a search engine that performs stop word
removal prior to search to improve performance. Searching on the
sentence "To be or not to be." using this strategy would fail.
</p>


<div id="stop-word-nltk" class="detail">
  <h3>NLTK Stop Words</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>Continuing our NLTK example, the following code snippet removes
    stop words from the document term vectors.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;"># Remove stop words from term vectors

stop_words = nltk.corpus.stopwords.words( 'english' )

for i in range( 0, len( term_vec ) ):
    term_list = [ ]

    for term in term_vec[ i ]:
        if term not in stop_words:
            term_list.append( term )

    term_vec[ i ] = term_list

# Print term vectors with stop words removed

for vec in term_vec:
    print vec
</code>

    </div>

    <p>Running this code in Python produces a list of term vectors
    identical to the table shown above.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">['far', 'far', 'better', 'thing', 'ever', 'done']
['call', 'ishmael']
['dagger', 'see']
['o', 'happy', 'dagger']
</code>
    </div>
  </div>
</div>


<h3>Stemming</h3>

<p>
Stemming removes suffixes from words, trimming them down
to <i>conflate</i> them into a single, common term. For example, the
terms
</p>

<div class="code-div">
  connect, connected, connecting, connection, connections
</div>

<p>
could be stemmed to a single term <code>connect</code>. There are a
number of potential advantages to stemming terms in a document. The
two most obvious are: (1) it reduces the total number of terms,
improving efficiency, and (2) it better captures the content of a
document by aggregating terms that are semantically similar.
</p>

<p>
Researchers in IR quickly realized it would be useful to develop
automatic stemming algorithms. One of the first algorithms for English
terms was published by Julie Beth Lovins in 1968 (Lovins,
J. B. <a target="_blank"
href="http://mt-archive.info/MT-1968-Lovins.pdf">Development of a
Stemming Algorithm</a>, <i>Mechanical Translation and Computational
Linguistics 11</i>, 1&ndash;2, 1968, 22&ndash;31.)  Lovins's algorithm
used 294 <i>endings</i>, 29 <i>conditions</i>, and
35 <i>transformation rules</i> to stem terms. Conditions and endings
are paired to define when endings can be removed from terms. For
example
</p>

<div class="code-div">
  <div><b>Conditions</b></div>

  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  A No restrictions on stem<br>
  B Minimum stem length = 3<br>
  &middot;&middot;&middot;<br>
  BB Minimum stem length = 3 and do not remove ending after <i>met</i>
  or <i>ryst</i><br>
  </code>
  </div>

  <div><b>Endings</b></div>

  <div style="margin-left: 1em;">
  <code>
  ATIONALLY B<br>
  IONALLY A<br>
  &middot;&middot;&middot;<br>
  </code>
  </div>
</div>

<p>
Consider the term <code>NATIONALLY</code>. This term ends
in <code>ATIONALLY</code> but condition <code>B</code> restricts its
application to terms whose minimum stem length (after stemming) is 3
characters or longer, so it cannot be applied. The term also ends in
<code>IONALLY</code>, however, and it satisfies
condition <code>A</code> (no restriction on stem), so this
ending <i>can</i> be removed, producing
<code>NAT</code>.
</p>

<p>
Lovins's transformation rules handle issues like letter doubling
(<code>SITTING</code> &rarr; <code>SITT</code>
&rarr; <code>SIT</code>), odd pluralization (<code>MATRIX</code>
as <code>MATRICES</code>), and other irregularities
(<code>ASSUME</code> and <code>ASSUMPTION</code>).
</p>

<p>
The order that rules are applied is important. In Lovins's algorithm,
the longest ending that satisfies its condition is found and applied.
Next, each of the 35 transformation rules are tested in turn.
</p>

<p>
Lovins's algorithm is a good example of trading space for coverage and
performance. The number of endings, conditions, and rules is fairly
extensive, but many special cases are handled, and the algorithm runs
in just two major steps: removing a suffix, and handling
language-specific transformations.
</p>


<h3>Porter Stemming</h3>

<p>
Perhaps the most popular stemming algorithm was developed by Michael
Porter in 1980 (Porter, M. F. <a target="_blank"
href="http://search.proquest.com/docview/218285582/fulltext?accountid=12725">An
Algorithm for Suffix Stripping</a>,
<i>Program 14</i>, 3, 1980, 130&ndash;137.) Porter's algorithm
attempted to improve on Lovins's in a number of ways. First, it is
much simpler, containing many fewer endings and conditions. Second,
unlike Lovins's approach of using stem length and the stem's ending
character as a condition, Porter uses the number of consonant-vowel
pairs that occur before the ending, to better represent syllables in a
stem. The algorithm begins by defining consonants and vowels.
</p>

<ul>

<li><b>Consonant.</b> A letter other than A, E, I, O, U, or Y preceded
by a consonant.</li>

<li><b>Vowel.</b> A letter than is not a consonant.</li>

</ul>

<p>
A sequence of consonants ccc... of length &gt; 0 is denoted C. A list
of vowels vvv... of length &gt; 0 is denoted V. Therefore, any term
has four forms: CVCV...C, CVCV...V, VCVC...C, or VCVC...V. Using
square brackets [C] to denote arbitrary presence and parentheses
(VC)<sup>m</sup> to denote m repetitions, this can be simplified to
<p>

<blockquote>
[C] (VC)<sup>m</sup> [V]
</blockquote>

<p>
m is the <i>measure</i> of the term. Here are some examples of
different terms and their measures, denoted using Porter's definitions.
</p>

<table class="center">
<tr>
  <th>Measure</th>
  <th>Term</th>
  <th>Def'n</th>
</tr>

<tr class="even">
  <td style="text-align: center;">m = 0</td>
  <td>tree &rArr; [tr] [ee]</td>
  <td>C (VC)<sup>0</sup> V</td>
</tr>

<tr class="odd">
  <td style="text-align: center;">m = 1</td>
  <td>trouble &rArr; [tr] (ou bl) [e]</td>
  <td>C (VC)<sup>1</sup> V</td>
</tr>

<tr class="even">
  <td style="text-align: center;">m = 1</td>
  <td>oats &rArr; [ ] (oa ts) [ ]</td>
  <td>(VC)<sup>1</sup></td>
</tr>

<tr class="odd">
  <td style="text-align: center;">m = 2</td>
  <td>private &rArr; [pr] (i v a t) [e]</td>
  <td>C (VC)<sup>2</sup> V</td>
</tr>

<tr class="even">
  <td style="text-align: center;">m = 2</td>
  <td>orrery &rArr; [ ] (o rr e r) [y]</td>
  <td>(VC)<sup>2</sup> V</td>
</tr>
</table>

<p>
Once terms are converted into consonant&ndash;vowel descriptions,
rules are defined by a conditional and a suffix transformation.
</p>

<blockquote>
(condition) S1 &rarr; S2
</blockquote>

<p>
The rule states that if a term ends in S1, and if the stem before S1
satisfies the condition, then S1 should be replaced by S2. The condition
is often specified in terms of m.
</p>

<blockquote>
(m &gt; 1) EMENT &rarr;
</blockquote>

<p>
This rule replaces a suffix EMENT with nothing if the remainder of the
term has measure of 2 or greater. For example, REPLACEMENT would be
stemmed to REPLAC, since REPLAC &rArr; [R] (E PL A C) [ ] with m = 2.
PLACEMENT, on the other hand, would not be stemmed, since PLAC &rArr;
[PL] (A C) [ ] has m = 1. Conditions can also contain more
sophisticated requirements.
</p>

<table class="center">
<tr>
  <th>Condition</th>
  <th>Explanation</th>
</tr>

<tr class="even">
  <td style="text-align: center;">*S</td>
  <td>stem must end in S (any letter can be specified)</td>
</tr>

<tr class="odd">
  <td style="text-align: center;">*v*</td>
  <td>stem must contain a vowel</td>
</tr>

<tr class="even">
  <td style="text-align: center;">*d</td>
  <td>stem must end in a double consonant</td>
</tr>

<tr class="odd">
  <td style="text-align: center;">*o</td>
  <td>stem must end in CVC, and the second C must not be W, X, or Y</td>
</tr>
</table>

<p>
Conditions can also include boolean operators, for example, (m > 1 and
(*S or *T)) for a stem with a measure of 2 or more that ends in S or
T, or (*d and not (*L or *S or *Z)) for a stem that ends in a double
consonant but does not end in L or S or Z.
</p>

<p>
Porter defines bundles of conditions that form eight rule sets. Each
rule set is applied in order, and within a rule set the matching rule
with the longest S1 is applied. The first three rules deal with
plurals and past participles (a verb in the past tense, used to modify
a noun or noun phrase). The next three rules reduce or strip suffixes.
The final two rules clean up trailing characters on a term. Here are
some examples from the first, second, fourth, and seventh rule sets.
</p>

<table class="center">
<tr>
  <th>Rule Set</th>
  <th>Rule</th>
  <th>Example</th>
</tr>

<tr class="even">
  <td style="text-align: center;">1</td>
  <td>
  SSES &rarr; SS<br>
  IES &rarr; I<br>
  S &rarr;
  </td>
  <td>
  CARESSES &rarr; CARESS<br>
  PONIES &rarr; PONI<br>
  CATS &rarr; CAT
  </td>
</tr>

<tr class="odd">
  <td style="text-align: center;">2</td>
  <td>
  (m > 0) EED &rarr; EE<br>
  (*v*) ED &rarr;<br>
  (*v*) ING &rarr;
  </td>
  <td>
  AGREED &rarr; AGREE<br>
  PLASTERED &rarr; PLASTER<br>
  MOTORING &rarr; MOTOR, SING &rarr; SING
  </td>
</tr>

<tr class="even">
  <td style="text-align: center;">4</td>
  <td>
  (m > 0) ATIONAL &rarr; ATE<br>
  (m > 0) ATOR &rarr; ATE<br>
  (m > 0) OUSLI &rarr; OUS<br>
  . . .
  </td>
  <td>
  RELATIONAL &rarr; RELATE<br>
  OPERATOR &rarr; OPERATE<br>
  ANALOGOUSLI &rarr; ANALOGOUS
  </td>
</tr>

<tr class="odd">
  <td style="text-align: center;">7</td>
  <td>
  (m > 1) E &rarr;
  </td>
  <td>
  PROBATE &rarr; PROBAT, RATE &rarr; RATE
  </td>
</tr>
</table>

<p>
<a target="_blank" href="https://text-processing.com/demo/stem">This
web site</a> provides an online demonstration of text being stemmed
using Porter's algorithm. Stemming our four document example produces
the following result, with <code>happy</code> stemming
to <code>happi</code>.
</p>

<table class="center">
<tr style="font-size: 0.8em;">
  <th></th>
  <th>better</th>
  <th>call</th>
  <th>dagger</th>
  <th>done</th>
  <th>ever</th>
  <th>far</th>
  <th>happi</th>
  <th>ishmael</th>
  <th>o</th>
  <th>see</th>
  <th>thing</th>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>1</sub></i></b></td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">2</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>2</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="even">
  <td style="text-align: center;"><b><i>D<sub>3</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
</tr>

<tr class="odd">
  <td style="text-align: center;"><b><i>D<sub>4</sub></i></b></td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1</td>
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">0</td>
</tr>
</table>


<div id="porter-stem-nltk" class="detail">
  <h3>NLTK Porter Stemming</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>Completing our initial NLTK example, the following code snippet
    Porter stems each term in our term vectors.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;"># Porter stem remaining terms

porter = nltk.stem.porter.PorterStemmer()

for i in range( 0, len( term_vec ) ):
    for j in range( 0, len( term_vec[ i ] ) ):
        term_vec[ i ][ j ] = porter.stem( term_vec[ i ][ j ] )

# Print term vectors with stop words removed

for vec in term_vec:
    print vec
</code>
    </div>

    <p>Running this code in Python produces a list of stemmed term
    vectors identical to the table shown above.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">['far', 'far', 'better', 'thing', 'ever', 'done']
['call', 'ishmael']
['dagger', 'see']
['o', 'happi', 'dagger']
</code>

    </div>
  </div>
</div>


<h3>Practice Problem 2</h3>

<p>Take the following three excepts from John Steinbeck's
<i>Of Mice and Men</i>, William Golding's <i>Lord of the Flies</i>,
and George Orwell's <i>1984</i>, and use stop word removal and Porter
stemming to produce a \(3 \times n\) term&ndash;frequency matrix.</p>

<p style="margin-left: 1.5in;"><b>Of Mice and Men</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  Two men, dressed in denim jackets and trousers and wearing "black,
  shapeless hats," walk single-file down a path near the pool. Both
  men carry blanket rolls — called bindles — on their shoulders. The
  smaller, wiry man is George Milton. Behind him is Lennie Small, a
  huge man with large eyes and sloping shoulders, walking at a gait
  that makes him resemble a huge bear.<br><br>

  When Lennie drops near the pool's edge and begins to drink like a
  hungry animal, George cautions him that the water may not be
  good. This advice is necessary because Lennie is retarded and
  doesn't realize the possible dangers. The two are on their way to a
  ranch where they can get temporary work, and George warns Lennie not
  to say anything when they arrive. Because Lennie forgets things very
  quickly, George must make him repeat even the simplest instructions.
  <br><br>

  Lennie also likes to pet soft things. In his pocket, he has a dead
  mouse which George confiscates and throws into the weeds beyond the
  pond. Lennie retrieves the dead mouse, and George once again catches
  him and gives Lennie a lecture about the trouble he causes when he
  wants to pet soft things (they were run out of the last town because
  Lennie touched a girl's soft dress, and she screamed). Lennie offers
  to leave and go live in a cave, causing George to soften his
  complaint and tell Lennie perhaps they can get him a puppy that can
  withstand Lennie's petting.<br><br>

  As they get ready to eat and sleep for the night, Lennie asks George
  to repeat their dream of having their own ranch where Lennie will be
  able to tend rabbits. George does so and then warns Lennie that, if
  anything bad happens, Lennie is to come back to this spot and hide
  in the brush. Before George falls asleep, Lennie tells him they must
  have many rabbits of various colors.
  </code>
  </div>
</div>
</p>

<p style="margin-left: 1.5in;"><b>Lord of the Flies</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  A fair-haired boy lowers himself down some rocks toward a lagoon on
  a beach. At the lagoon, he encounters another boy, who is chubby,
  intellectual, and wears thick glasses. The fair-haired boy
  introduces himself as Ralph and the chubby one introduces himself as
  Piggy. Through their conversation, we learn that in the midst of a
  war, a transport plane carrying a group of English boys was shot
  down over the ocean. It crashed in thick jungle on a deserted
  island. Scattered by the wreck, the surviving boys lost each other
  and cannot find the pilot.<br><br>

  Ralph and Piggy look around the beach, wondering what has become of
  the other boys from the plane. They discover a large pink and
  cream-colored conch shell, which Piggy realizes could be used as a
  kind of makeshift trumpet. He convinces Ralph to blow through the
  shell to find the other boys. Summoned by the blast of sound from
  the shell, boys start to straggle onto the beach. The oldest among
  them are around twelve; the youngest are around six. Among the group
  is a boys’ choir, dressed in black gowns and led by an older boy
  named Jack. They march to the beach in two parallel lines, and Jack
  snaps at them to stand at attention. The boys taunt Piggy and mock
  his appearance and nickname.<br><br>

  The boys decide to elect a leader. The choirboys vote for Jack, but
  all the other boys vote for Ralph. Ralph wins the vote, although
  Jack clearly wants the position. To placate Jack, Ralph asks the
  choir to serve as the hunters for the band of boys and asks Jack to
  lead them. Mindful of the need to explore their new environment,
  Ralph chooses Jack and a choir member named Simon to explore the
  island, ignoring Piggy’s whining requests to be picked. The three
  explorers leave the meeting place and set off across the island.<br><br>

  The prospect of exploring the island exhilarates the boys, who feel
  a bond forming among them as they play together in the
  jungle. Eventually, they reach the end of the jungle, where high,
  sharp rocks jut toward steep mountains. The boys climb up the side
  of one of the steep hills. From the peak, they can see that they are
  on an island with no signs of civilization. The view is stunning,
  and Ralph feels as though they have discovered their own land. As
  they travel back toward the beach, they find a wild pig caught in a
  tangle of vines. Jack, the newly appointed hunter, draws his knife
  and steps in to kill it, but hesitates, unable to bring himself to
  act. The pig frees itself and runs away, and Jack vows that the next
  time he will not flinch from the act of killing. The three boys make
  a long trek through dense jungle and eventually emerge near the
  group of boys waiting for them on the beach.
  </code>
  </div>
</div>

<p style="margin-left: 1.5in;"><b>1984</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  On a cold day in April of 1984, a man named Winston Smith returns to
  his home, a dilapidated apartment building called Victory
  Mansions. Thin, frail, and thirty-nine years old, it is painful for
  him to trudge up the stairs because he has a varicose ulcer above
  his right ankle. The elevator is always out of service so he does
  not try to use it. As he climbs the staircase, he is greeted on each
  landing by a poster depicting an enormous face, underscored by the
  words "BIG BROTHER IS WATCHING YOU."<br><br>

  Winston is an insignificant official in the Party, the totalitarian
  political regime that rules all of Airstrip One &ndash; the land
  that used to be called England &ndash; as part of the larger state
  of Oceania. Though Winston is technically a member of the ruling
  class, his life is still under the Party's oppressive political
  control. In his apartment, an instrument called a telescreen &ndash;
  which is always on, spouting propaganda, and through which the
  Thought Police are known to monitor the actions of citizens &ndash;
  shows a dreary report about pig iron. Winston keeps his back to the
  screen. From his window he sees the Ministry of Truth, where he
  works as a propaganda officer altering historical records to match
  the Party’s official version of past events. Winston thinks about
  the other Ministries that exist as part of the Party's governmental
  apparatus: the Ministry of Peace, which wages war; the Ministry of
  Plenty, which plans economic shortages; and the dreaded Ministry of
  Love, the center of the Inner Party's loathsome activities.<br><br>

  WAR IS PEACE<br>
  FREEDOM IS SLAVERY<br>
  IGNORANCE IS STRENGTH<br><br>

  From a drawer in a little alcove hidden from the telescreen, Winston
  pulls out a small diary he recently purchased. He found the diary in
  a secondhand store in the proletarian district, where the very poor
  live relatively unimpeded by Party monitoring. The proles, as they
  are called, are so impoverished and insignificant that the Party
  does not consider them a threat to its power. Winston begins to
  write in his diary, although he realizes that this constitutes an
  act of rebellion against the Party. He describes the films he
  watched the night before. He thinks about his lust and hatred for a
  dark-haired girl who works in the Fiction Department at the Ministry
  of Truth, and about an important Inner Party member named O'Brien
  &ndash; a man he is sure is an enemy of the Party. Winston remembers
  the moment before that day’s Two Minutes Hate, an assembly during
  which Party orators whip the populace into a frenzy of hatred
  against the enemies of Oceania. Just before the Hate began, Winston
  knew he hated Big Brother, and saw the same loathing in O'Brien's
  eyes.<br><br>

  Winston looks down and realizes that he has written "DOWN WITH BIG
  BROTHER" over and over again in his diary. He has committed
  thoughtcrime—the most unpardonable crime—and he knows that the
  Thought Police will seize him sooner or later. Just then, there is a
  knock at the door.
  </code>
  </div>
</div>


<div id="prac-prob-2" class="detail">
  <h3>Practice Problem 2 Solution</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>The following snippet of Python code will produce a
    term--document frequency matrix. For simplicity of display, the
    matrix is transposed, but by standard definition rows represent
    documents and columns represents terms. Each cell the matrix is
    the frequency \(t_{i,j}\) of term \(t_i\) in document \(D_j\).</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-05-img" class="code-div-img"></span>
        <div id="code-05">
        >>> import nltk<br>
        >>> import re<br>
        >>> import string<br>
        >>><br>
        >>> nltk.download( 'stopwords' )<br>
        >>><br>
        >>> txt = [<br>
        ... <span class="tab-1">'Two men, dressed in denim jackets and trousers and wearing "black, shapeless hats," walk single-file down a path near the pool. Both men carry blanket rolls — called bindles — on their shoulders. The smaller, wiry man is George Milton. Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear. When Lennie drops near the pool\'s edge and begins to drink like a hungry animal, George cautions him that the water may not be good. This advice is necessary because Lennie is retarded and doesn\'t realize the possible dangers. The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive. Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions. Lennie also likes to pet soft things. In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond. Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl\'s soft dress, and she screamed). Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie\'s petting. As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits. George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush. Before George falls asleep, Lennie tells him they must have many rabbits of various colors.',</span><br>
        ... <span class="tab-1">'A fair-haired boy lowers himself down some rocks toward a lagoon on a beach. At the lagoon, he encounters another boy, who is chubby, intellectual, and wears thick glasses. The fair-haired boy introduces himself as Ralph and the chubby one introduces himself as Piggy. Through their conversation, we learn that in the midst of a war, a transport plane carrying a group of English boys was shot down over the ocean. It crashed in thick jungle on a deserted island. Scattered by the wreck, the surviving boys lost each other and cannot find the pilot. Ralph and Piggy look around the beach, wondering what has become of the other boys from the plane. They discover a large pink and cream-colored conch shell, which Piggy realizes could be used as a kind of makeshift trumpet. He convinces Ralph to blow through the shell to find the other boys. Summoned by the blast of sound from the shell, boys start to straggle onto the beach. The oldest among them are around twelve; the youngest are around six. Among the group is a boys’ choir, dressed in black gowns and led by an older boy named Jack. They march to the beach in two parallel lines, and Jack snaps at them to stand at attention. The boys taunt Piggy and mock his appearance and nickname. The boys decide to elect a leader. The choirboys vote for Jack, but all the other boys vote for Ralph. Ralph wins the vote, although Jack clearly wants the position. To placate Jack, Ralph asks the choir to serve as the hunters for the band of boys and asks Jack to lead them. Mindful of the need to explore their new environment, Ralph chooses Jack and a choir member named Simon to explore the island, ignoring Piggy\'s whining requests to be picked. The three explorers leave the meeting place and set off across the island. The prospect of exploring the island exhilarates the boys, who feel a bond forming among them as they play together in the jungle. Eventually, they reach the end of the jungle, where high, sharp rocks jut toward steep mountains. The boys climb up the side of one of the steep hills. From the peak, they can see that they are on an island with no signs of civilization. The view is stunning, and Ralph feels as though they have discovered their own land. As they travel back toward the beach, they find a wild pig caught in a tangle of vines. Jack, the newly appointed hunter, draws his knife and steps in to kill it, but hesitates, unable to bring himself to act. The pig frees itself and runs away, and Jack vows that the next time he will not flinch from the act of killing. The three boys make a long trek through dense jungle and eventually emerge near the group of boys waiting for them on the beach.',</span><br>
        ... <span class="tab-1">'On a cold day in April of 1984, a man named Winston Smith returns to his home, a dilapidated apartment building called Victory Mansions. Thin, frail, and thirty-nine years old, it is painful for him to trudge up the stairs because he has a varicose ulcer above his right ankle. The elevator is always out of service so he does not try to use it. As he climbs the staircase, he is greeted on each landing by a poster depicting an enormous face, underscored by the words "BIG BROTHER IS WATCHING YOU." Winston is an insignificant official in the Party, the totalitarian political regime that rules all of Airstrip One – the land that used to be called England – as part of the larger state of Oceania. Though Winston is technically a member of the ruling class, his life is still under the Party\'s oppressive political control. In his apartment, an instrument called a telescreen – which is always on, spouting propaganda, and through which the Thought Police are known to monitor the actions of citizens – shows a dreary report about pig iron. Winston keeps his back to the screen. From his window he sees the Ministry of Truth, where he works as a propaganda officer altering historical records to match the Party’s official version of past events. Winston thinks about the other Ministries that exist as part of the Party’s governmental apparatus: the Ministry of Peace, which wages war; the Ministry of Plenty, which plans economic shortages; and the dreaded Ministry of Love, the center of the Inner Party’s loathsome activities. WAR IS PEACE FREEDOM IS SLAVERY IGNORANCE IS STRENGTH From a drawer in a little alcove hidden from the telescreen, Winston pulls out a small diary he recently purchased. He found the diary in a secondhand store in the proletarian district, where the very poor live relatively unimpeded by Party monitoring. The proles, as they are called, are so impoverished and insignificant that the Party does not consider them a threat to its power. Winston begins to write in his diary, although he realizes that this constitutes an act of rebellion against the Party. He describes the films he watched the night before. He thinks about his lust and hatred for a dark-haired girl who works in the Fiction Department at the Ministry of Truth, and about an important Inner Party member named O\'Brien – a man he is sure is an enemy of the Party. Winston remembers the moment before that day’s Two Minutes Hate, an assembly during which Party orators whip the populace into a frenzy of hatred against the enemies of Oceania. Just before the Hate began, Winston knew he hated Big Brother, and saw the same loathing in O’Brien’s eyes. Winston looks down and realizes that he has written "DOWN WITH BIG BROTHER" over and over again in his diary. He has committed thoughtcrime—the most unpardonable crime—and he knows that the Thought Police will seize him sooner or later. Just then, there is a knock at the door.'</span><br>
        ... ]<br>
        >>> <br>
        >>> def porter_stem( txt ):<br>
        ... <span class="tab-1">"""Porter stem terms in text block</span><br>
        ... <br>
        ... <span class="tab-1">Args:</span><br>
        ... <span class="tab-1">&nbsp txt (list of string):  Text block as list of individual terms</span><br>
        ... <br>
        ... <span class="tab-1">Returns:</span><br>
        ... <span class="tab-1">&nbsp (list of string):  Text block with terms Porter stemmed</span><br>
        ... <span class="tab-1">"""</span><br>
        ... <br>
        ... <span class="tab-1">porter = nltk.stem.porter.PorterStemmer()</span><br>
        ... <br>
        ... <span class="tab-1">for i in range( 0, len( txt ) ):</span><br>
        ... <span class="tab-2">txt[ i ] = porter.stem( txt[ i ] )</span><br>
        ... <br>
        ... <span class="tab-1">return txt</span><br>
        >>> <br>
        >>> <br>
        >>> def remove_stop_word( txt ):<br>
        ... <span class="tab-1">"""Remove all stop words from text block
        ... <br>
        ... <span class="tab-1">Args:</span><br>
        ... <span class="tab-1">&nbsp txt (list of string):  Text block as list of individual terms</span><br>
        ... <br>
        ... <span class="tab-1">Returns:</span><br>
        ... <span class="tab-1">&nbsp (list of string):  Text block with stop words removed</span><br>
        ... <span class="tab-1">"""</span><br>
        ... <br>
        ... <span class="tab-1">term_list = [ ]</span><br>
        ... <span class="tab-1">stop_word = nltk.corpus.stopwords.words( 'english' )</span><br>
        ... <br>
        ... <span class="tab-1">for term in txt:</span><br>
        ... <span class="tab-2">term_list += ( [ ] if term in stop_word else [ term ] )</span><br>
        >>> <br>
        ... <span class="tab-1">return term_list</span><br>
        >>><br>
        >>><br>
        >>> #  Mainline<br>
        >>><br>
        >>> #  Remove punctuation except hyphen<br>
        >>><br>
        >>> punc = string.punctuation.replace( '-', '' )<br>
        >>> for i in range( 0, len( txt ) ):<br>
        ... <span class="tab-1">txt[ i ] = re.sub( '[' + punc + ']+', '', txt[ i ] )</span><br>
        >>><br>
        >>> #  Lower-case and tokenize text<br>
        >>><br>
        >>> for i in range( 0, len( txt ) ):<br>
        ... <span class="tab-1">txt[ i ] = txt[ i ].lower().split()</span><br>
        >>><br>
        >>> #  Stop word remove w/nltk stop word list, then Porter stem<br>
        >>><br>
        >>> for i in range( 0, len( txt ) ):<br>
        ... <span class="tab-1">txt[ i ] = remove_stop_word( txt[ i ] )</span><br>
        ... <span class="tab-1">txt[ i ] = porter_stem( txt[ i ] )</span><br>
        >>><br>
        >>> #  Create list of all (unique) stemmed terms<br>
        >>><br>
        >>> term_list = set( txt[ 0 ] )<br>
        >>> for i in range( 1, len( txt ) ):<br>
        ... <span class="tab-1">term_list = term_list.union( txt[ i ] )</span><br>
        >>> term_list = sorted( term_list )<br>
        >>><br>
        >>> #  Count occurrences of unique terms in each document<br>
        >>><br>
        >>> n = len( term_list )<br>
        >>> freq = [ ]<br>
        >>> for i in range( 0, len( txt ) ):<br>
        ... <span class="tab-1">freq.append( [ 0 ] * n )</span><br>
        ... <span class="tab-1">for term in txt[ i ]:</span><br>
        ... <span class="tab-2">pos = term_list.index( term )</span><br>
        ... <span class="tab-2">freq[ -1 ][ pos ] += 1</span><br>
        >>><br>
        >>> #  Print transposed term-frequency list for easier viewing<br>
        >>> print( '....................mice..lord..1984' )<br>
        >>> for i in range( 0, len( term_list ) ):<br>
        ... <span class="tab-1">print( f'{term_list[ i ]: <{20}}', end='' )</span><br>
        ... <span class="tab-1">for j in range( 0, len( txt ) ):</span><br>
        ... <span class="tab-2">print( f'{freq[ j ][ i ]:4d}  ', end='' )</span><br>
        ... <span class="tab-1">print( '' )</span><br>
        </div>
      </td>
      </tr>
      </table>
    </div>

  </div>
</div>


<h2 id="similarity">Similarity</h2>

<p>
Once documents have been converted into term vectors, vectors can be
compared to estimate the similarity between pairs or sets of
documents. Many algorithms weight the vectors' term frequencies to
better distinguish documents from one another, then use the cosine of
the angle between a pair of document vectors to compute the documents'
similarity.
</p>


<h3>Term Frequency&ndash;Inverse Document Frequency</h3>

<p>
A well known document similarity algorithm is term
frequency&ndash;inverse document frequency, or TF-IDF (Salton, G. and
Yang, C. S. <a target="_blank"
href="https://ecommons.cornell.edu/handle/1813/6016">On the
Specification of Term Values in Automatic Indexing</a>, <i>Journal of
Documentation 29</i>, 4, 351&ndash;372, 1973). Here, individual terms
in a document's term vector are weighted by their frequency in the
document (the term frequency), and by their frequency over the entire
document collection (the document frequency).
</p>

<p>
Consider an <i>m</i>&times;<i>n</i> matrix <i>X</i> representing
<i>m</i> unique terms <i>t<sub>i</sub></i> as rows of <i>X</i>
and <i>n</i> documents <i>D<sub>j</sub></i> as columns
of <i>X</i>. The weight
<i>X</i>[<i>i,&thinsp;j</i>] = <i>w<sub>i,j</sub></i> for
<i>t<sub>i</sub></i> &isin; <i>D<sub>j</sub></i> is defined
as <i>w<sub>i,j</sub></i> = tf<i><sub>i,j</sub></i> &times;
idf<i><sub>i</sub></i>, where tf<i><sub>i,j</sub></i> is the number of
occurrences of <i>t<sub>i</sub></i> &isin; <i>D<sub>j</sub></i>,
and idf<i><sub>i</sub></i> is the log of inverse fraction of
documents <i>n<sub>i</sub></i> that contain at least one occurrence of
<i>t<sub>i</sub></i>, idf<i><sub>i</sub></i> = ln( <i>n</i>
/ <i>n<sub>i</sub></i> ).
</p>

<p>
The left matrix below shows our four document example transposed to
place the <i>m</i>=11 terms in rows and the <i>n</i>=4 documents in
columns. The center matrix weights each term count using TF-IDF. The
right matrix normalizes each document column, to remove the influence
of document length from the TF-IDF weights.
</p>


<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>

  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>

  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
</tr>

<tr>
  <td rowspan="11" class="matrix" style="vertical-align: middle;">
    <i>X</i> &nbsp; =
  </td>
  <td class="matrix">better</td>
  <td rowspan="11" class="matrix-left"></td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="11" class="matrix-right"></td>

  <td rowspan="11" class="matrix"
   style="vertical-align: middle; padding-left: 2em;">
    =
  </td>
  <td class="matrix">better</td>
  <td rowspan="11" class="matrix-left"></td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="11" class="matrix-right"></td>

  <td rowspan="11" class="matrix"
   style="vertical-align: middle; padding-left: 2em;">
    =
  </td>
  <td class="matrix">better</td>
  <td rowspan="11" class="matrix-left"></td>
  <td class="matrix">0.35</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="11" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix">call</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">call</td>
  <td class="matrix">0</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">call</td>
  <td class="matrix">0</td>
  <td class="matrix">0.71</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>

  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.69</td>
  <td class="matrix">0.69</td>

  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.44</td>
  <td class="matrix">0.33</td>
</tr>

<tr>
  <td class="matrix">done</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">done</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">done</td>
  <td class="matrix">0.35</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">ever</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">ever</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">ever</td>
  <td class="matrix">0.35</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">far</td>
  <td class="matrix">2</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">far</td>
  <td class="matrix">2.77</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">far</td>
  <td class="matrix">0.71</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">happi</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>

  <td class="matrix">happi</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1.39</td>

  <td class="matrix">happi</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.67</td>
</tr>

<tr>
  <td class="matrix">ishmael</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">ishmael</td>
  <td class="matrix">0</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">ishmael</td>
  <td class="matrix">0</td>
  <td class="matrix">0.71</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">o</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>

  <td class="matrix">o</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1.39</td>

  <td class="matrix">o</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.67</td>
</tr>

<tr>
  <td class="matrix">see</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>

  <td class="matrix">see</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>

  <td class="matrix">see</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.90</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">thing</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">thing</td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">thing</td>
  <td class="matrix">0.35</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

</table>


<p>
Most of the weights in the center matrix's columns are 1.39. These
correspond to single frequency occurrences of terms
(tf<i><sub>i,j</sub></i> = 1) that exist in only one document
(idf<i><sub>i</sub></i> = ln(4 / 1) = 1.39). Single frequency
occurrences of <code>dagger</code> in
<i>D</i><sub>3</sub> and <i>D</i><sub>4</sub> have weights of 0.69,
because idf<sub><code>dagger</code></sub> = ln(4 / 2) = 0.69.
Finally, the weight for <code>far</code> in <i>D</i><sub>1</sub> is
2.77 because its term frequency is tf<sub><code>far</code>,1</sub> =
2.
</p>

<p>
Once documents are converted into normalized TF-IDF vectors, the
similarity between two documents is the dot product of their vectors.
In our example, the only documents that share a common term with a
non-zero weight are <i>D</i><sub>3</sub>
and <i>D</i><sub>4</sub>. Their similarity is <i>D</i><sub>3</sub>
&middot; <i>D</i><sub>4</sub> = 0.44 &times; 0.33 = 0.15.
</p>

<p>
Mathematically, recall that cos&thinsp;&theta; = <i>D<sub>i</sub></i>
&middot; <i>D<sub>j</sub></i> / |<i>D<sub>i</sub></i>|
|<i>D<sub>j</sub></i>|. Since the document vectors are normalized,
this reduces to cos&thinsp;&theta; = <i>D<sub>i</sub></i>
&middot; <i>D<sub>j</sub></i>. Dot product similarity measures the
cosine of the angle between two document vectors. The more similar the
direction of the vectors, the more similar the documents.
</p>

<p>
Intuitively, TF-IDF implies the following. In any
document <i>D<sub>j</sub></i>, if a term <i>t<sub>i</sub></i> occurs
frequently, it's an important term for
characterizing <i>D<sub>j</sub></i>. Moreover, if <i>t<sub>i</sub></i>
does not occur in many other documents, it's an important term for
distinguishing <i>D<sub>j</sub></i> from other documents. This is why
<i>t<sub>i</sub></i>'s weight in <i>D<sub>j</sub></i> increases based
on term frequency and inverse document frequency. If two documents
share terms with high term frequency and low document frequency, they
are assumed to be similar. The dot product captures exactly this
situation in its sum of the product of individual term weights.
</p>


<div id="tfidf-gensim" class="detail">
  <h3>TF-IDF</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>Unfortunately, NLTK does not provide a TF-IDF
    implementation. To generate TF-IDF vectors and use them to
    calculate pairwise document similarity, we can use the Gensim
    Python library.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">#  Convert term vectors into gensim dictionary

dict = gensim.corpora.Dictionary( term_vec )

corp = [ ]
for i in range( 0, len( term_vec ) ):
    corp.append( dict.doc2bow( term_vec[ i ] ) )

#  Create TFIDF vectors based on term vectors bag-of-word corpora

tfidf_model = gensim.models.TfidfModel( corp )

tfidf = [ ]
for i in range( 0, len( corp ) ):
    tfidf.append( tfidf_model[ corp[ i ] ] )

#  Create pairwise document similarity index

n = len( dict )
index = gensim.similarities.SparseMatrixSimilarity( tfidf_model[ corp ], num_features = n )

#  Print TFIDF vectors and pairwise similarity per document

for i in range( 0, len( tfidf ) ):
    s = 'Doc ' + str( i + 1 ) + ' TFIDF:'

    for j in range( 0, len( tfidf[ i ] ) ):
        s = s + ' (' + dict.get( tfidf[ i ][ j ][ 0 ] ) + ','
        s = s + ( '%.3f' % tfidf[ i ][ j ][ 1 ] ) + ')'

    print s

for i in range( 0, len( corp ) ):
    print 'Doc', ( i + 1 ), 'sim: [ ',

    sim = index[ tfidf_model[ corp[ i ] ] ]
    for j in range( 0, len( sim ) ):
        print '%.3f ' % sim[ j ],

    print ']'
</code>

    </div>

    <p>Running this code produces a list of normalized TF-IDF vectors
    for the Porter stemmed terms in each document, and a list of
    pairwise similarities for each document compared to all the other
    documents in our four document collection.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">Doc 1 TFIDF: (better,0.354) (done,0.354) (ever,0.354) (far,0.707) (thing,0.354)
Doc 2 TFIDF: (call,0.707) (ishmael,0.707)
Doc 3 TFIDF: (dagger,0.447) (see,0.894)
Doc 4 TFIDF: (dagger,0.333) (happi,0.667) (o,0.667)
Doc 1 sim: [  1.000  0.000  0.000  0.000  ]
Doc 2 sim: [  0.000  1.000  0.000  0.000  ]
Doc 3 sim: [  0.000  0.000  1.000  0.149  ]
Doc 4 sim: [  0.000  0.000  0.149  1.000  ]
</code>
    </div>

    <p>Alternatively, we can use scikit-learn to perform TFIDF
    directly. The following code performs all operations we have
    discussed thus far, including TF-IDF weighting in sklearn.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">import nltk
import numpy
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import stop_words

text = [\
  "It is a far, far better thing I do, than I have every done before",\
  "Call me Ishmael",\
  "Is this a dagger I see before me?",\
  "O happy dagger"\
]

#  Remove punctuation

punc = re.compile( '[%s]' % re.escpae( string.punctuation ) )
for i, doc in enumerate( text ):
    text[ i ] = punc.sub( '', doc.lower() )

#  TF-IDF vectorize documents w/sklearn, remove English stop words

vect = TfidfVectorizer( stop_words='english' )
xform = vect.fit_transform( text )

#  Grab remaining terms (keys), stem, if different replace w/stem

porter = nltk.stem.porter.PorterStemmer()
for term in list( vect.vocabulary_.keys() ):
    if term == porter.stem( term )
        continue

    v = vect.vocabulary_[ term ]
    del vect.vocabulary_[ term ]
    vect.vocabulary_[ porter.stem( term ) ] = v

#  Get final key/value lists

key = list( vect.vocabulary_.keys() )
val = list( vect.vocabulary_.values() )

# Print out formatted TF-IDF scores per term per document

row, col = xform.nonzero()

cur_doc = 0
s = 'Doc 1 TFIDF: '

for i, c in enumerate( col ):
    term = key[ val.index( c ) ]
    tfidf = xform[ row[ i ], c ]

    if row[ i ] != cur_doc:        #  New document?
        print( s )                 #  Print prev doc's terms/TFIDF weights

        cur_doc = row[ i ]         #  Record new doc's ID
        s = 'Doc ' + str( cur_doc + 1 ) + ' TFIDF:'

    s = s + ' (' + term + ','      #  Add current term/TFIDF pair
    s = s + ( f'{tfidf:.03f}' + ')' )

print( s )                         #  Print final doc's terms/TFIDF weights

# Print document similarity matrix

dense = xform.todense()

for i in range( len( dense ) ):
    s = 'Doc' + str( i + 1 ) + 'sim: '
    x = dense[ i ].tolist()[ 0 ]

    s = s + '[  '
    for j in range( len( dense ) ):
      y = dense[ j ].tolist()[ 0 ]
      prod = numpy.multiply( x, y ).sum()
      s = s + f'{prod:.03f}' + '  '
    print( s + ']' )
</code>
    </div>

    <p>Notice that this produces slightly different TF-IDF scores and
    a corresponding similarity matrix.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">Doc 1 TFIDF: (thing,0.408) (better,0.408) (far,0.816)
Doc 2 TFIDF: (ishmael,1.000)
Doc 3 TFIDF: (dagger,1.000)
Doc 4 TFIDF: (happi,0.618),(o,0.618),(dagger,0.487)
Doc 1 sim: [  1.000  0.000  0.000  0.000  ]
Doc 2 sim: [  0.000  1.000  0.000  0.000  ]
Doc 3 sim: [  0.000  0.000  1.000  0.487  ]
Doc 4 sim: [  0.000  0.000  0.487  1.000  ]
</code>
    </div>

    <p>The reason for this discrepancy is due entirely to the different
    stop word lists used by sklearn versus NLTK.</p>

  </div>
</div>


<h3>Latent Semantic Analysis</h3>

<p>
Latent semantic analysis (LSA) reorganizes a set of documents using a
<i>semantic</i> space derived from implicit structure contained in the
text of the documents (Dumais, S. T., Furnas, G. W., Landauer, T. K.,
Deerwester, S. and Harshman, R. <a target="_blank"
href="http://dl.acm.org/citation.cfm?id=57214">Using Latent Semantic
Analysis to Improve Access to Textual Information</a>,
<i>Proceedings of the Conference on Human Factors in Computing Systems
(CHI '88)</i>, 281&ndash;286, 1988). LSA uses the same
<i>m</i>&times;<i>n</i> term-by-document matrix <i>X</i> corresponding
to <i>m</i> unique terms across <i>n</i> documents. Each frequency
<i>X</i>[<i>i,&thinsp;j</i>] for term <i>t<sub>i</sub></i> in
document <i>D<sub>j</sub></i> is normally weighted, for example, using
TF-IDF.
</p>

<table class="center">
<tr>
  <td style="border-width: 0px;"></td>
  <td style="padding: 0px; border-width: 0px;"></td>
  <td style="border-width: 0px;"></td>
  <td style="border-width: 0px;"><i>D<sub>j</sub></i></td>
  <td style="border-width: 0px;"></td>
  <td style="padding: 0px; border-width: 0px;"></td>
  <td style="border-width: 0px;"></td>
</tr>

<tr>
  <td rowspan="3" style="vertical-align: middle; border-width: 0px;">
    <i>t<sub>i</sub></i>
  </td>
  <td class="matrix-left" rowspan="3"></td>
  <td style="border-width: 0px;"><i>x</i><sub>1,1</sub></td>
  <td style="border-width: 0px;"><i>&#8943;</td>
  <td style="border-width: 0px;"><i>x</i><sub>1,<i>n</i></sub></td>
  <td class="matrix-right" rowspan="3"></td>
  <td rowspan="3" style="vertical-align: middle; border-width: 0px;">
    &nbsp; = &nbsp; <i>X</i>
  </td>
</tr>

<tr>
  <td style="border-width: 0px; text-align: center;">&#8942;</td>
  <td style="border-width: 0px; text-align: center;">&#8945;</td>
  <td style="border-width: 0px; text-align: center;">&#8942;</td>
</tr>

<tr>
  <td style="border-width: 0px;"><i>x</i><sub><i>m</i>,1</sub></td>
  <td style="border-width: 0px;"><i>&#8943;</td>
  <td style="border-width: 0px;"><i>x</i><sub><i>m,n</i></sub></td>
</tr>
</table>

<p>
Each row in <i>X</i> is a term vector <i>t<sub>i</sub></i> =
[&thinsp;<i>x</i><sub><i>i</i>,1</sub>
&#8943; <i>x<sub>i,n</sub></i>&thinsp;] defining the frequency
of <i>t<sub>i</sub></i> in each document <i>D<sub>j</sub></i>. The dot
product of two term vectors <i>t<sub>p</sub></i>
&middot; <i>t<sub>q</sub></i><sup>T</sup> defines a correlation
between the distribution of the two terms across the documents.
Similarly, the dot product <i>D<sub>p</sub></i><sup>T</sup>
&middot; <i>D<sub>q</sub></i> of two columns of <i>X</i> corresponding
to the term frequencies for two documents defines a similarity between
the documents.
</p>

<p>
Given <i>X</i>, we perform a singular value decomposition (SVD) to
produce <i>X</i> = <i>U</i>&Sigma;</i>V</i><sup>T</sup>,
where <i>U</i> and <i>V</i> are orthonormal matrices (a matrix whose
rows and columns are unit length, and the dot product of any pair of
rows or columns is 0) and &Sigma; is a diagonal
matrix. Mathematically,
<i>U</i> contains the eigenvectors of <i>XX</i><sup>T</sup>
(the <i>t<sub>p</sub></i>&ndash;<i>t<sub>q</sub></i>
correlations), <i>V</i> contains the eigenvectors
of <i>X</i><sup>T</sup><i>X</i>
(the <i>D<sub>p</sub></i>&ndash;<i>D<sub>q</sub></i> similarities),
and &Sigma;&Sigma;<sup>T</sup> contains the eigenvalues for <i>U</i>
and <i>V</i>, which are identical. Mathematically, SVD can be seen as
providing three related functions.
</p>

<ol>
  <li>A method to transform correlated variables into uncorrelated
  variables that better expose relationships among the original data
  items.</li>

  <li>A method to identify and order dimensions along which the data
  items exhibit the most variation.</li>

  <li>A method to best approximate the data items with fewer
  dimensions.</li>
</ol>

<p>
To use SVD for text similarity, we first select the <i>k</i> largest
singular values &sigma; from &Sigma;, together with their
corresponding eigenvectors from <i>U</i> and <i>V</i>. This forms a
rank-<i>k</i> approximation of <i>X</i>, <i>X<sub>k</sub></i> =
<i>U<sub>k</sub></i>&Sigma;<i><sub>k</sub></i><i>V<sub>k</sub></i><sup>T</sup>. The
columns <i>c<sub>i</sub></i> of <i>U<sub>k</sub></i>
represent <i>concepts</i>, linear combinations of the original
terms. The columns <i>D<sub>j</sub></i>
of <i>V<sub>k</sub></i><sup>T</sup> represent the documents defined
based on which concepts (and how much of each concept) they
contain. Consider the following example documents.
</p>

<ul>

<li><i>D</i><sub>1</sub>. Romeo and Juliet</li>
<li><i>D</i><sub>2</sub>. Juliet, O happy dagger!</li>
<li><i>D</i><sub>3</sub>. Romeo died by a dagger</li>
<li><i>D</i><sub>4</sub>. "Live free or die", that's the New Hampshire
motto</li>
<li><i>D</i><sub>5</sub>. Did you know New Hampshire is in New
England</li>

</ul>

<p>
We choose a subset of the terms in these documents, then construct an
initial term&ndash;document matrix <i>X</i>.


<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>

<tr>
  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    <i>X</i> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td rowspan="6" class="matrix-left"></td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="6" class="matrix-right"></td>

  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    &nbsp;&nbsp;&nbsp;&nbsp;<i>X</i><sub>TF&#8209;IDF</sub> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td rowspan="6" class="matrix-left"></td>
  <td class="matrix">707</td>
  <td class="matrix">0</td>
  <td class="matrix">0.578</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="6" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix">juliet</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">juliet</td>
  <td class="matrix">0.707</td>
  <td class="matrix">0.532</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">happy</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">happy</td>
  <td class="matrix">0</td>
  <td class="matrix">0.66</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">0.532</td>
  <td class="matrix">0.577</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">die</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>
  <td class="matrix">0</td>

  <td class="matrix">die</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.578</td>
  <td class="matrix">0.707</td>
  <td class="matrix">0</td>
</tr>

<tr>
  <td class="matrix">hampshire</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>

  <td class="matrix">hampshire</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.707</td>
  <td class="matrix">1.0</td>
</tr>
</table>

<p>
Applying SVD to <i>X</i><sub>TF&#8209;IDF</sub> produces the following
decomposition.
</p>

<table class="center">
<tr>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>c</i><sub>1</sub></td>
  <td class="matrix"><i>c</i><sub>2</sub></td>
  <td class="matrix"><i>c</i><sub>3</sub></td>
  <td class="matrix"><i>c</i><sub>4</sub></td>
  <td class="matrix"><i>c</i><sub>5</sub></td>
  <td class="matrix"><i>c</i><sub>6</sub></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>

<tr>
  <td class="matrix" rowspan="6" style="vertical-align: middle;">
    <i>U</i> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td class="matrix-left" rowspan="6"></td>
  <td class="matrix">-0.34</td>
  <td class="matrix">0.32</td>
  <td class="matrix">0.04</td>
  <td class="matrix">-0.55</td>
  <td class="matrix">0.54</td>
  <td class="matrix">-0.42</td>
  <td class="matrix-right" rowspan="6"></td>

  <td class="matrix"
      rowspan="6" style="vertical-align: middle; padding-left: 10px;">
    &Sigma; &nbsp; =
  </td>
  <td class="matrix-left" rowspan="6"></td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix-right" rowspan="6"></td>

  <td class="matrix"
      rowspan="5" style="vertical-align: middle; padding-left: 10px;">
    <i>V</i><sup>T</sup> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="5"></td>
  <td class="matrix">-0.36</td>
  <td class="matrix">-0.32</td>
  <td class="matrix">-0.52</td>
  <td class="matrix">-0.56</td>
  <td class="matrix">-0.43</td>
  <td class="matrix-right" rowspan="5"></td>
</tr>

<tr>
  <td class="matrix">juliet</td>
  <td class="matrix">-0.50</td>
  <td class="matrix">-0.12</td>
  <td class="matrix">0.55</td>
  <td class="matrix">-0.30</td>
  <td class="matrix">-0.59</td>
  <td class="matrix">0</td>

  <td class="matrix">0</td>
  <td class="matrix">1.26</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">0.49</td>
  <td class="matrix">0.46</td>
  <td class="matrix">0.28</td>
  <td class="matrix">-0.44</td>
  <td class="matrix">-0.52</td>
</tr>

<tr>
  <td class="matrix">happy</td>
  <td class="matrix">-0.60</td>
  <td class="matrix">-0.66</td>
  <td class="matrix">-0.36</td>
  <td class="matrix">0.17</td>
  <td class="matrix">0.22</td>
  <td class="matrix">0</td>

  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.86</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix">-0.08</td>
  <td class="matrix">-0.63</td>
  <td class="matrix">0.63</td>
  <td class="matrix">0.15</td>
  <td class="matrix">-0.42</td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">-0.15</td>
  <td class="matrix">0.24</td>
  <td class="matrix">-0.48</td>
  <td class="matrix">-0.45</td>
  <td class="matrix">-0.14</td>
  <td class="matrix">0.68</td>

  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.78</td>
  <td class="matrix">0</td>

  <td class="matrix">0.77</td>
  <td class="matrix">-0.53</td>
  <td class="matrix">-0.26</td>
  <td class="matrix">-0.12</td>
  <td class="matrix">0.22</td>
</tr>

<tr>
  <td class="matrix">die</td>
  <td class="matrix">-0.31</td>
  <td class="matrix">0.47</td>
  <td class="matrix">-0.46</td>
  <td class="matrix">0.34</td>
  <td class="matrix">-0.43</td>
  <td class="matrix">-0.42</td>

  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.39</td>

  <td class="matrix">-0.17</td>
  <td class="matrix">-0.08</td>
  <td class="matrix">0.44</td>
  <td class="matrix">-0.68</td>
  <td class="matrix">0.56</td>
</tr>

<tr>
  <td class="matrix">hampshire</td>
  <td class="matrix">-0.40</td>
  <td class="matrix">0.41</td>
  <td class="matrix">0.36</td>
  <td class="matrix">0.51</td>
  <td class="matrix">0.34</td>
  <td class="matrix">0.42</td>

  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>
</table>

<p>
We choose the <i>k</i> = 2 largest singular values, producing the
following reduced matrices.
</p>

<table class="center">
<tr>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>c</i><sub>1</sub></td>
  <td class="matrix"><i>c</i><sub>2</sub></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>

<tr>
  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    <i>U</i><sub>2</sub> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td class="matrix-left" rowspan="6"></td>
  <td class="matrix">-0.34</td>
  <td class="matrix">0.32</td>
  <td class="matrix-right" rowspan="6"></td>

  <td class="matrix"
      rowspan="2" style="vertical-align: middle; padding-left: 10px;">
    &Sigma;<sub>2</sub> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">1.39</td>
  <td class="matrix">0</td>
  <td class="matrix-right" rowspan="2"></td>

  <td class="matrix"
      rowspan="2" style="vertical-align: middle; padding-left: 10px;">
    <i>V</i><sub>2</sub><sup>T</sup> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">-0.36</td>
  <td class="matrix">-0.32</td>
  <td class="matrix">-0.52</td>
  <td class="matrix">-0.56</td>
  <td class="matrix">-0.43</td>
  <td class="matrix-right" rowspan="2"></td>
</tr>

<tr>
  <td class="matrix">juliet</td>
  <td class="matrix">-0.50</td>
  <td class="matrix">-0.12</td>

  <td class="matrix">0</td>
  <td class="matrix">1.26</td>

  <td class="matrix">0.49</td>
  <td class="matrix">0.46</td>
  <td class="matrix">0.28</td>
  <td class="matrix">-0.44</td>
  <td class="matrix">-0.52</td>
</tr>

<tr>
  <td class="matrix">happy</td>
  <td class="matrix">-0.60</td>
  <td class="matrix">-0.66</td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">-0.15</td>
  <td class="matrix">0.24</td>
</tr>

<tr>
  <td class="matrix">die</td>
  <td class="matrix">-0.31</td>
  <td class="matrix">0.47</td>
</tr>

<tr>
  <td class="matrix">hampshire</td>
  <td class="matrix">-0.40</td>
  <td class="matrix">0.41</td>
</tr>
</table>

<p>
If we wanted to reconstruct a version of the original
term&ndash;document matrix using our concepts from the rank-2
approximation, we would dot-product <i>U</i><sub>2</sub> &middot;
&Sigma;<sub>2</sub> &middot; <i>V</i><sub>2</sub><sup>T</sup> to
produce <i>X</i><sub>2</sub> based on the largest <i>k</i>=2 singular
values. We have also normalized the document columns to allow for dot
product-based cosine similarity.
</p>

<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;">|<i>D</i><sub>1</sub>|</td>
  <td class="matrix" style="padding-bottom: 6px;">|<i>D</i><sub>2</sub>|</td>
  <td class="matrix" style="padding-bottom: 6px;">|<i>D</i><sub>3</sub>|</td>
  <td class="matrix" style="padding-bottom: 6px;">|<i>D</i><sub>4</sub>|</td>
  <td class="matrix" style="padding-bottom: 6px;">|<i>D</i><sub>5</sub>|</td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
</tr>

<tr>
  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    <i>X</i><sub>2</sub> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td rowspan="6" class="matrix-left"></td>
  <td class="matrix">0.46</td>
  <td class="matrix">0.46</td>
  <td class="matrix">0.45</td>
  <td class="matrix">0.09</td>
  <td class="matrix">-0.01</td>
  <td rowspan="6" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix">juliet</td>
  <td class="matrix">0.22</td>
  <td class="matrix">0.21</td>
  <td class="matrix">0.40</td>
  <td class="matrix">0.48</td>
  <td class="matrix">0.43</td>
</tr>

<tr>
  <td class="matrix">happy</td>
  <td class="matrix">-0.13</td>
  <td class="matrix">-0.16</td>
  <td class="matrix">0.25</td>
  <td class="matrix">0.87</td>
  <td class="matrix">0.89</td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">0.28</td>
  <td class="matrix">0.28</td>
  <td class="matrix">0.24</td>
  <td class="matrix">-0.02</td>
  <td class="matrix">-0.14</td>
</tr>

<tr>
  <td class="matrix">die</td>
  <td class="matrix">0.56</td>
  <td class="matrix">0.56</td>
  <td class="matrix">0.48</td>
  <td class="matrix">-0.02</td>
  <td class="matrix">-0.14</td>
</tr>

<tr>
  <td class="matrix">hampshire</td>
  <td class="matrix">0.57</td>
  <td class="matrix">0.57</td>
  <td class="matrix">0.54</td>
  <td class="matrix">0.09</td>
  <td class="matrix">-0.03</td>
</tr>
</table>

<p>
So what advantage does LSA provide over using a term&ndash;document
matrix directly, as we do in TF-IDF? First, it hopefully provides some
insight into why concepts might be more useful that independent
terms. Consider the term frequencies contained in <i>X</i>
versus <i>X</i><sub>2</sub> for <i>D</i><sub>1</sub>. The original
frequencies in <i>X</i> were 1 for <code>romeo</code>
and <code>juliet</code>, and 0 for all other terms. The two largest
LSA frequencies in <i>X</i><sub>2</sub> are 0.56 for <code>die</code>
and 0.57 for <code>hampshire</code>. Why is there a large positive
frequency for <code>die</code>? LSA has inferred this connection based
on the fact that
<i>D</i><sub>3</sub> associates <code>dagger</code> with
<code>romeo</code>. On the other hand, <code>hampshire</code> has the
largest contribution of 0.57. It is true that <code>hampshire</code>
in <i>D</i><sub>4</sub> associates with <code>die</code>, which
associates with <code>die</code> in <i>D</i><sub>3</sub>, which
associates with <code>romeo</code> in <i>D</i><sub>1</sub>, but this
doesn't seem as strong the <code>romeo</code>&ndash;<code>die</code>
relationship from <i>D</i><sub>3</sub> directly. This highlights one
issue with LSA, the inability to identify "intuitive" meanings for the
concepts it extracts.
</p>

<p>
These associations affect document similarities. For example, in the
original <i>X</i> the similarities
between <i>D</i><sub>1</sub>&ndash;<i>D</i><sub>4</sub> and
<i>D</i><sub>1</sub>&ndash;<i>D</i><sub>5</sub> are both 0. If you
used the <i>X</i><sub>2</sub> term&ndash;document matrix, however, the
similarities are 0.06 and -0.15. The term <code>die</code>
in <i>D</i><sub>4</sub> associates to <code>romeo</code>, defining a
similarity between <i>D</i><sub>1</sub> and <i>D</i><sub>4</sub>. No
such association exists between <i>D</i><sub>1</sub>
and <i>D</i><sub>5</sub>. Human readers with an understanding of the
context of Romeo and Juliet would likely identify the same weak
presence or lack of similarity.
</p>

<p>
Second, using a rank-<i>k</i> approximation can reduce computation
during pairwise document similarity calculations. If we wanted to know
the similarity between <i>D</i><sub>1</sub> and <i>D</i><sub>2</sub>,
normally we would not recreate <i>X</i><sub>2</sub> and use its
columns for cosine similarity. Instead, we would
use <i>V</i><sub>2</sub><sup>T</sup> directly, since this encodes the
original documents and the amount of each <i>concept</i> the documents
contain. There is no need to step back to the terms themselves, since
they aren't needed for the similarity
calculation. Using <i>D</i><sub>1</sub> and <i>D</i><sub>2</sub>
in <i>V</i><sub>2</sub><sup>T</sup>, we would first normalize the
2&times;1 columns, then dot product them to generate a result.
</p>

<table class="center">
<tr>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix">&vert;<i>D</i><sub>1</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>2</sub>&vert;</td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
</tr>

<tr>
  <td rowspan="2" class="matrix" style="vertical-align: middle;">
    <i>V</i><sub>2</sub><sup>T</sup> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">-0.42</td>
  <td class="matrix">-0.57</td>
  <td class="matrix-right" rowspan="2"></td>

  <td rowspan="2" class="matrix" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>V</i><sub>2</sub><sup>T</sup>&vert; &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">-0.88</td>
  <td class="matrix">-0.76</td>
  <td class="matrix-right" rowspan="2"></td>

  <td class="matrix"
      rowspan="2" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>D</i><sub>1</sub>&vert; &middot; &vert;<i>D</i><sub>2</sub>&vert; &nbsp; = 0.978</td>
  </td>
</tr>

<tr>
  <td class="matrix">0.23</td>
  <td class="matrix">0.49</td>

  <td class="matrix">0.48</td>
  <td class="matrix">0.65</td>
</tr>

</table>

<!--
<p>
Foltz, P. W., Laham, D., and Landauer, T. K. <a target="_blank"
href="http://imej.wfu.edu/articles/1999/2/04/">The Intelligent Essay
Assessor: Applications to Educational Technology</a>, <i>Interactive
Multimedia Electronic Journal of Computer Enhanced Learning 1</i>, 2,
1999.
</p>
-->

<p>Notice that if you are using concept amounts from algorithms like
LSA, there is a subtle but important consideration when computing
cosine similarity. For raw term frequencies or TF-IDF weighted term
frequences, the values are always positive, because you cannot have a
negative number of terms. For LSA, however, the amount of a concept
contained in a document <i>can</i> be negative. Consider the
similarity between <i>D</i><sub>2</sub> and <i>D</i><sub>5</sub>.

<table class="center">
<tr>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix">&vert;<i>D</i><sub>2</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>5</sub>&vert;</td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
</tr>

<tr>
  <td rowspan="2" class="matrix" style="vertical-align: middle;">
    <i>V</i><sub>2</sub><sup>T</sup> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">-0.57</td>
  <td class="matrix">-0.06</td>
  <td class="matrix-right" rowspan="2"></td>

  <td rowspan="2" class="matrix" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>V</i><sub>2</sub><sup>T</sup>&vert; &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">-0.88</td>
  <td class="matrix">-0.16</td>
  <td class="matrix-right" rowspan="2"></td>

  <td class="matrix"
      rowspan="2" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>D</i><sub>2</sub>&vert; &middot; &vert;<i>D</i><sub>5</sub>&vert; &nbsp; = -0.52</td>
  </td>
</tr>

<tr>
  <td class="matrix">0.49</td>
  <td class="matrix">-0.37</td>

  <td class="matrix">0.65</td>
  <td class="matrix">-0.99</td>
</tr>

</table>

<p>The cosine similarity between <i>D</i><sub>2</sub>
and <i>D</i><sub>5</sub> is negative. We saw that the cosine
similarity using <i>X</i><sub>2</sub> between <i>D</i><sub>1</sub>
and <i>D</i><sub>5</sub> was also negative. What does it mean to have
a "negative" similarity between two documents? The key is to recall
that, for normalized vectors, cos(&thinsp;\(\theta\)&thinsp;)
= <i>D</i><sub>2</sub>&thinsp;&middot;&thinsp;<i>D</i><sub>5</sub> and
since the range of \(\theta\) is [0&thinsp;&hellip;&thinsp;180], this
produces a range of [1&thinsp;&hellip;&thinsp;-1] for
\(\cos(\theta)\). To address this, we can use \(^{1+\cos(\theta)} /
_{2}\) to shift the range of cosine similarities back to
[1&thinsp;&hellip;&thinsp;0]. If we do this,
|<i>D</i><sub>2</sub>|&thinsp;&middot;&thinsp;|<i>D</i><sub>5</sub>| =
0.24. We would also need to apply this correction to
|<i>D</i><sub>1</sub>|&thinsp;&middot;&thinsp;|<i>D</i><sub>2</sub>| =
0.989 to ensure all similarities are directly comparable. Doing this
for all pairs of documents produces the following similarity matrix
&Sigma;<sub>2</sub>.</p>

<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
</tr>

<tr>
  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    &Sigma;<sub>2</sub> &nbsp; =
  </td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td rowspan="6" class="matrix-left"></td>
  <td class="matrix"></td>
  <td class="matrix">0.99</td>
  <td class="matrix">0.81</td>
  <td class="matrix">0.42</td>
  <td class="matrix">0.33</td>
  <td rowspan="6" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.72</td>
  <td class="matrix">0.32</td>
  <td class="matrix">0.24</td>
</tr>

<tr>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.84</td>
  <td class="matrix">0.76</td>
</tr>

<tr>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.99</td>
</tr>

<tr>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
</tr>

</table>


<h2 id="cluster">Clustering</h2>

<p>
Once similarities between pairs of documents have been calculated,
they can be used to cluster the documents into groups. This is often
called <i>topic clustering</i>, since each group represents a set of
documents with similar content, and by assumption that discuss a
similar topic or topics.
</p>

<p>
Any similarity-based clustering algorithm can be used for topic
clustering, for example, <i>k</i>-means, closest-neighbour
agglomerative, density-based, and so on. We present a graph-based
clustering algorithm that uses threshold similarities to partition the
document collection into clusters. Varying the threshold similarity
produces a hierarchical clustering at different levels of granularity.
</p>


<h3>Minimum Spanning Tree Clustering</h3>

<p>
Any similarity algorithm (<i>e.g.</i>, TF-IDF or LSA) can be used to
construct a pairwise document similarity matrix &Sigma;, where
&sigma;<i>i,j</i> &isin; &Sigma; defines the similarity between
documents <i>D<sub>i</sub></i> and <i>D<sub>j</sub></i>. Since
&sigma;<i>i,j</i> = &sigma;<i>j,i</i>, only the upper or lower half of
&Sigma; is normally defined. The TF-IDF matrix <i>X</i> and the
similarity matrix &Sigma; for the five document example in the LSA
section contains the following values.
</p>

<!--
<table class="center">
<tr>
  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix">&vert;<i>D</i><sub>1</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>2</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>3</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>4</sub>&vert;</td>
  <td class="matrix">&vert;<i>D</i><sub>5</sub>&vert;</td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>

  <td class="matrix"></td>
  <td class="matrix" style="padding: 0px;"></td>
  <td class="matrix"><i>D</i><sub>1</sub></td>
  <td class="matrix"><i>D</i><sub>2</sub></td>
  <td class="matrix"><i>D</i><sub>3</sub></td>
  <td class="matrix"><i>D</i><sub>4</sub></td>
  <td class="matrix"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px;"></td>
</tr>

<tr>
  <td rowspan="2" class="matrix" style="vertical-align: middle;">
    <i>X</i><sub>2</sub> &nbsp; =
  </td>
  <td class="matrix-left" rowspan="2"></td>
  <td class="matrix">0.88</td>
  <td class="matrix">0.76</td>
  <td class="matrix">0.92</td>
  <td class="matrix">0.34</td>
  <td class="matrix">0.16</td>
  <td class="matrix-right" rowspan="2"></td>

  <td rowspan="5" class="matrix" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>V</i><sub>2</sub><sup>T</sup>&vert; &nbsp; =
  </td>
  <td class="matrix-left" rowspan="5"></td>
  <td class="matrix">0.48</td>
  <td class="matrix">0.65</td>
  <td class="matrix">0.39</td>
  <td class="matrix">0.94</td>
  <td class="matrix">0.99</td>
  <td class="matrix-right" rowspan="5"></td>

  <td rowspan="5" class="matrix" style="vertical-align: middle; padding-left: 30px;">
    &vert;<i>V</i><sub>2</sub><sup>T</sup>&vert; &nbsp; =
  </td>
  <td class="matrix-left" rowspan="5"></td>
  <td class="matrix">0.88</td>
  <td class="matrix">0.76</td>
  <td class="matrix-right" rowspan="5"></td>
  </td>
</tr>

<tr>
  <td class="matrix">0.48</td>
  <td class="matrix">0.65</td>
  <td class="matrix">0.39</td>
  <td class="matrix">0.94</td>
  <td class="matrix">0.99</td>
  <td class="matrix-right" rowspan="5"></td>

  <td class="matrix">-0.76</td>
  <td class="matrix">0.65</td>
  <td class="matrix">0.65</td>
  <td class="matrix">0.65</td>
  <td class="matrix">0.65</td>
</tr>

</table>
-->

<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>

  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>

  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
</tr>

<tr>
  <td rowspan="6" class="matrix" style="vertical-align: middle;">
    <i>X</i> &nbsp; =
  </td>
  <td class="matrix">romeo</td>
  <td rowspan="6" class="matrix-left"></td>
  <td class="matrix">0.71</td>
  <td class="matrix">0</td>
  <td class="matrix">0.58</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="6" class="matrix-right"></td>

  <td class="matrix"
      rowspan="5" style="vertical-align: middle; padding-left: 10px">
    <i>&Sigma;</i> &nbsp; =
  </td>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>1</sub></td>
  <td rowspan="5" class="matrix-left"></td>
  <td class="matrix"></td>
  <td class="matrix">0.31</td>
  <td class="matrix">0.41</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td rowspan="5" class="matrix-right"></td>

  <td class="matrix"
      rowspan="5" style="vertical-align: middle; padding-left: 10px">
    <i>&Delta;</i> &nbsp; =
  </td>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>1</sub></td>
  <td rowspan="5" class="matrix-left"></td>
  <td class="matrix"></td>
  <td class="matrix">0.69</td>
  <td class="matrix">0.59</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>
  <td rowspan="5" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix">juliet</td>
  <td class="matrix">0.71</td>
  <td class="matrix">0.44</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>2</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.26</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>2</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.74</td>
  <td class="matrix">1</td>
  <td class="matrix">1</td>
</tr>

<tr>
  <td class="matrix">happy</td>
  <td class="matrix">0</td>
  <td class="matrix">0.78</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>3</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.41</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>3</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.59</td>
  <td class="matrix">1</td>
</tr>

<tr>
  <td class="matrix">dagger</td>
  <td class="matrix">0</td>
  <td class="matrix">0.44</td>
  <td class="matrix">0.58</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>4</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.71</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>4</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.29</td>
</tr>

<tr>
  <td class="matrix">die</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.58</td>
  <td class="matrix">0.71</td>
  <td class="matrix">0</td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>5</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>

  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>5</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
</tr>

<tr>
  <td class="matrix">hampshire</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0</td>
  <td class="matrix">0.71</td>
  <td class="matrix">1</td>

  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>

  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
</tr>
</table>


<p>
We want to use values in &Sigma; to build a graph with documents as
nodes and weighted edges defining the similarity between documents,
with similar documents close to one another. To do this, we must
weight the edges with <i>dissimilarities</i> &Delta; = 1 - &Sigma;.
Now, two documents <i>D<sub>i</sub></i> and <i>D<sub>j</sub></i> with
&sigma;<i><sub>i,j</sub></i> = 1 will have an edge weight of
&delta;<i><sub>i,j</sub></i> = 1 - &sigma;<i><sub>i,j</sub></i> = 0,
so they will overlap. Two documents <i>D<sub>i</sub></i>
and <i>D<sub>j</sub></i> with &sigma;<i><sub>i,j</sub></i> = 0 will
have an edge weight of &delta;<i><sub>i,j</sub></i> = 1 -
&sigma;<i><sub>i,j</sub></i> = 1, so they will be a maximum possible
distance from one another.
</p>

<p>
Once &Delta; is built, it is used to construct a complete graph
with <i>n</i> nodes representing the <i>n</i> documents, and <i>nm</i>
edges representing the similarities between all pairs of documents.
Each edge connecting <i>D<sub>i</sub></i> and <i>D<sub>j</sub></i> is
weighted with &delta;<i><sub>i,j</sub></i>. Kruskal's minimum spanning
tree (MST) algorithm is run to find a minimum-weight tree that includes
all <i>n</i> documents.
</p>

<div class="alg">
<table>
<tr>
  <td class="alg-num">1</td>
  <td colspan="3" class="alg"><i>F</i> &leftarrow; <i>n</i> nodes</td>
</tr>

<tr>
  <td class="alg-num">2</td>
  <td colspan="3" class="alg"><i>E</i> &leftarrow; <i>nm</i> edges</td>
</tr>

<tr>
  <td class="alg-num">3</td>
  <td colspan="3" class="alg"><b>while</b> <i>E</i> not empty
  &amp;&amp; <i>F</i> not spanning <b>do</b></td>
</tr>

<tr>
  <td class="alg-num">4</td>
  <td rowspan="4" class="alg-left"><div class="alg-left"></div></td>
  <td colspan="2" class="alg">find <i>e<sub>i,j</sub></i>
  &isin; <i>E</i> with minimum <i>w<sub>i,j</sub></i>
</tr>

<tr>
  <td class="alg-num">5</td>
  <td colspan="2" class="alg">remove <i>e<sub>i,j</sub></i> from <i>E</i>
</tr>

<tr>
  <td class="alg-num">6</td>
  <td colspan="2" class="alg"><b>if</b> <i>e<sub>i,j</sub></i>
  connects two separate trees in <i>F</i> <b>then</b>
</tr>

<tr>
  <td class="alg-num">7</td>
  <td class="alg-left"><div class="alg-left"></div></td>
  <td class="alg">add <i>e<sub>i,j</sub></i> to <i>F</i></td>
</tr>
</table>
</div>


<!--  d3.js force graph of document similarity example -->

<div id="tfidf-div" class="donthyphenate"
  style="margin-left: auto; margin-right: auto; margin-top: 2em;
         margin-bottom: 2em; border: 1px solid #e0e0e0; width: 600px;
         height: 350px;">
  <svg id="tfidf-svg"
    xmlns="http://www.w3.org/2000/svg" width="600" height="350">
  </svg>
</div>

<style>
.node {
  stroke: #fff;
  stroke-width: 1.5px;
}
</style>


<p>
This force-directed graph shows the five document example with the
Euclidean distance between nodes roughly equal to the dissimilarity
between the corresponding documents. MST edges are drawn in red and
labelled with their &delta;<i><sub>i,j</sub></i>.
</p>

<p>
Once the MST is constructed, topic clusters are formed by removing all
edges <i>e<sub>i,j</sub></i> in the MST whose
&delta;<i><sub>i,j</sub></i> &geq; &tau; for a threshold dissimilarity
&tau;. This produces one or more disconnected subgraphs, each of which
represents a topic cluster. The larger the value of &tau;, the more
dissimilarity we allow between documents in a common cluster. For
example, suppose we chose &tau; = 0.5 in the above TF-IDF
dissimilarity graph. This would produce four topic clusters:

<i>C</i><sub>1</sub> = {&nbsp;<i>D</i><sub>4</sub>,
<i>D</i><sub>5</sub>&nbsp;},
<i>C</i><sub>2</sub> = {&nbsp;<i>D</i><sub>1</sub>&nbsp;},
<i>C</i><sub>3</sub> = {&nbsp;<i>D</i><sub>2</sub>&nbsp;}, and
<i>C</i><sub>4</sub> = {&nbsp;<i>D</i><sub>3</sub>&nbsp;}.

Increasing &tau; to 0.6 produces two topic clusters:
<i>C</i><sub>1</sub> = {&nbsp;<i>D</i><sub>1</sub>,
<i>D</i><sub>3</sub>, <i>D</i><sub>4</sub>, <i>D</i><sub>5</sub>&nbsp;}, and
<i>C</i><sub>2</sub> = {&nbsp;<i>D</i><sub>2</sub>&nbsp;}.
</p>

<p>
Semantically, we might expect to see two clusters:
<i>C</i><sub>1</sub> = {&nbsp;<i>D</i><sub>1</sub>,
<i>D</i><sub>2</sub>, <i>D</i><sub>3</sub>&nbsp;}, and
<i>C</i><sub>2</sub> = {&nbsp;<i>D</i><sub>1</sub>,
<i>D</i><sub>5</sub>&nbsp;}.

However, because &delta;<sub>1,2</sub> = &delta;<sub>3,4</sub> = 0.69,
it is not possible to subdivide the MST in a way that combines
<i>D</i><sub>1</sub> with <i>D</i><sub>3</sub>, but separates
<i>D</i><sub>3</sub> from <i>D</i><sub>4</sub>. This is a consequence
of the fact that TF-IDF has no knowledge of context. It only has
access to the terms in the documents to make its similarity estimates.
</p>

<p>
An alternative is to use LSA to derive a dissimilarity matrix that
includes term correspondences in its results. The matrix
&Delta;<sub>2</sub> below shows dissimilarities for the five document
example derived from <i>X</i><sub>2</sub>, the LSA rank-2 estimate of
the original term&ndash;frequency matrix <i>X</i>.
</p>


<table class="center">
<tr>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>1</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>2</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>3</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>4</sub></td>
  <td class="matrix" style="padding-bottom: 6px;"><i>D</i><sub>5</sub></td>
  <td class="matrix" style="padding: 0px 0px 6px 0px;"></td>
</tr>

<tr>
  <td class="matrix"
      rowspan="5" style="vertical-align: middle; padding-left: 10px">
    <i>&Delta;</i><sub>2</sub> &nbsp; =
  </td>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>1</sub></td>
  <td rowspan="5" class="matrix-left"></td>
  <td class="matrix"></td>
  <td class="matrix">0</td>
  <td class="matrix">0.05</td>
  <td class="matrix">0.47</td>
  <td class="matrix">0.37</td>
  <td rowspan="5" class="matrix-right"></td>
</tr>

<tr>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>2</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.05</td>
  <td class="matrix">0.49</td>
  <td class="matrix">0.59</td>
</tr>

<tr>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>3</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.26</td>
  <td class="matrix">0.36</td>
</tr>

<tr>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>4</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix">0.01</td>
</tr>

<tr>
  <td class="matrix" style="padding-right: 1em;"><i>D</i><sub>5</sub></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
</tr>

<tr>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
  <td class="matrix"></td>
</tr>
</table>


<!--  d3.js force graph of document similarity example -->

<div id="lsa-div" class="donthyphenate"
  style="margin-left: auto; margin-right: auto; margin-top: 2em;
         margin-bottom: 2em; border: 1px solid #e0e0e0; width: 600px;
         height: 350px;">
  <svg id="lsa-svg"
    xmlns="http://www.w3.org/2000/svg" width="600" height="350">
  </svg>
</div>

<style>
.node {
  stroke: #fff;
  stroke-width: 1.5px;
}
</style>

<script src="./js/force-graph.js"></script>


<p>
In this graph, choosing &tau; = 0.2 removes only the edge
between <i>D</i><sub>3</sub> and <i>D</i><sub>4</sub>, producing two
clusters: <i>C</i><sub>1</sub> =
{&nbsp;<i>D</i><sub>1</sub>, <i>D</i><sub>2</sub>, <i>D</i><sub>3</sub>&nbsp;}
and <i>C</i><sub>2</sub> =
{&nbsp;<i>D</i><sub>4</sub>,<i>D</i><sub>4</sub>&nbsp;}. This matches
our intuitive idea of how the five documents should cluster.
</p>


<h3>Practice Problem 3</h3>

<p>Take the following four excerpts from Haruki
Murakami's <a href="https://en.wikipedia.org/wiki/Norwegian_Wood_(novel)"
target="_blank">Norwegian Wood</a>, Yasunari
Kawabata's <a href="https://en.wikipedia.org/wiki/Snow_Country"
target="_blank">Snow Country</a>, Natsume Soseki's
<a href="https://en.wikipedia.org/wiki/Kokoro"
target="_blank">Kokoro</a>, and Kazuo
Ishiguro's <a href="https://en.wikipedia.org/wiki/The_Remains_of_the_Day"
target="_blank">The Remains of the Day</a>.</p>

<p style="margin-left: 1.5in;"><b>Norwegian Wood</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  I was thirty-seven then, strapped in my seat as the huge 747 plunged
  through dense cloud cover on approach to the Hamburg airport. Cold
  November rains drenched the earth and lent everything the gloomy air
  of a Flemish landscape: the ground crew in rain gear, a flag atop a
  squat airport building, a BMW billboard. So Germany again.<br><br>

  Once the plane was on the ground, soft music began to flow from the
  ceiling speakers: a sweet orchestral cover version of the Beatles'
  "Norwegian Wood." The melody never failed to send a shudder through
  me, but this time it hit me harder than ever.<br><br>

  I bent forward in my seat, face in hands to keep my skull from
  splitting open. Before long one of the German stewardesses
  approached and asked in English if I were sick. "No," I said, "just
  dizzy."<br><br>

  "Are you sure?"<br><br>

  "Yes, I'm sure. Thanks."<br><br>

  She smiled and left, and the music changed to a Billy Joel tune. I
  straightened up and looked out the plane window at the dark clouds
  hanging over the North Sea, thinking of what I had lost in the
  course of my life: times gone forever, friends who had died or
  disappeared, feelings I would never know again.
  </code>
  </div>
</div>
</p>

<p style="margin-left: 1.5in;"><b>Snow Country</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  The train came out of the long tunnel into the snow country. The
  earth lay white under the night sky. The train pulled up at a signal
  stop.<br><br>
 
  A girl who had been sitting on the other side of the car came over
  and opened the window in front of Shimamura. The snowy cold poured
  in. Leaning far out the window, the girl called to the station
  master as though he were a great distance away.<br><br>
 
  The station master walked slowly over the snow, a lantern in his
  hand. His face was buried to the nose in a muffler, and the flaps of
  his cap were turned down over his ears.<br><br>
 
  It's that cold, is it, thought Shimamura. Low, barracklike buildings
  that might have been railway dormitories were scattered here and
  there up the frozen slope of the mountain. The white of the snow
  fell away into the darkness some distance before it reached them.
  </code>
  </div>
</div>

<p style="margin-left: 1.5in;"><b>Kokoro</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  I always called him "Sensei." I shall therefore refer to him simply
  as "Sensei," and not by his real name. It is not because I consider
  it more discreet, but it is because I find it more natural that I do
  so. Whenever the memory of him comes back to me now, I find that I
  think of him as "Sensei" still. And with pen in hand, I cannot bring
  myself to write of him in any other way.<br><br>

  It was at Kamakura, during the summer holidays, that I first met
  Sensei. I was then a very young student. I went there at the
  insistence of a friend of mine, who had gone to Kamakura to swim. We
  were not together for long. It had taken me a few days to get
  together enough money to cover the necessary expenses, and it was
  only three days after my arrival that my friend received a telegram
  from home demanding his return. His mother, the telegram explained,
  was ill. My friend, however, did not believe this. For some time his
  parents had been trying to persuade him, much against his will, to
  marry a certain girl. According to our modern outlook, he was really
  too young to marry. Moreover, he was not in the least fond of the
  girl. It was in order to avoid an unpleasant situation that instead
  of going home, as he normally would have done, he had gone to the
  resort near Tokyo to spend his holidays. He showed me the telegram,
  and asked me what he should do. I did not know what to tell him. It
  was, however, clear that if his mother was truly ill, he should go
  home. And so he decided to leave after all. I, who had taken so much
  trouble to join my friend, was left alone.<br><br>

  There were many days left before the beginning of term, and I was
  free either to stay in Kamakura or to go home. I decided to stay. My
  friend was from a wealthy family in the Central Provinces, and had
  no financial worries. But being a young student, his standard of
  living was much the same as my own. I was therefore not obliged,
  when I found myself alone, to change my lodgings.<br><br>

  My inn was in a rather out-of-the-way district of Kamakura, and if
  one wished to indulge in such fashionable pastimes as playing
  billiards and eating ice cream, one had to walk a long way across
  rice fields. If one went by rickshaw, it cost twenty sen. Remote as
  the district was, however, many rich families had built their villas
  there. It was quite near the sea also, which was convenient for
  swimmers such as myself.<br><br>

  I walked to the sea every day, between thatched cottages that were
  old and smoke-blackened. The beach was always crowded with men and
  women, and at times the sea, like a public bath, would be covered
  with a mass of black heads. I never ceased to wonder how so many
  city holiday-makers could squeeze themselves into so small a
  town. Alone in this noisy and happy crowd, I managed to enjoy
  myself, dozing on the beach or splashing about in the water.<br><br>

  It was in the midst of this confusion that I found Sensei. In those
  days, there were two tea houses on the beach. For no particular
  reason, I had come to patronize one of them. Unlike those people
  with their great villas in the Hase area who had their own bathing
  huts, we in our part of the beach were obliged to make use of these
  tea houses which served also as communal changing rooms. In them the
  bathers would drink tea, rest, have their bathing suits rinsed, wash
  the salt from their bodies, and leave their hats and sunshades for
  safe-keeping. I owned no bathing suit to change into, but I was
  afraid of being robbed, and so I regularly left my things in the tea
  house before going into the water.
  </code>
  </div>
</div>

<p style="margin-left: 1.5in;"><b>The Remains of the Day</b><br>
<div class="code-div" style="margin-top: 0.25em;">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  It seems increasingly likely that I really will undertake the
  expedition that has been preoccupying my imagination now for some
  days. An expedition, I should say, which I will undertake alone, in
  the comfort of Mr Farraday’s Ford; an expedition which, as I foresee
  it, will take me through much of the finest countryside of England
  to the West Country, and may keep me away from Darlington Hall for
  as much as five or six days. The idea of such a journey came about,
  I should point out, from a most kind suggestion put to me by Mr
  Farraday himself one afternoon almost a fortnight ago, when I had
  been dusting the portraits in the library. In fact, as I recall, I
  was up on the step-ladder dusting the portrait of Viscount Wetherby
  when my employer had entered carrying a few volumes which he
  presumably wished returned to the shelves. On seeing my person, he
  took the opportunity to inform me that he had just that moment
  finalized plans to return to the United States for a period of five
  weeks between August and September. Having made this announcement,
  my employer put his volumes down on a table, seated himself on the
  chaise-longue, and stretched out his legs.<br><br>

  'You realize, Stevens, I don't expect you to be locked up here in
  this house all the time I'm away. Why don't you take the car and
  drive off somewhere for a few days? You look like you could make
  good use of a break.'<br><br>

  Coming out of the blue as it did, I did not quite know how to reply
  to such a suggestion. I recall thanking him for his consideration,
  but quite probably I said nothing very definite for my employer went
  on:<br><br>

  'I’m serious, Stevens. I really think you should take a break. I'll
  foot the bill for the gas. You fellows, you’re always locked up in
  these big houses helping out, how do you ever get to see around this
  beautiful country of yours?'<br><br>

  This was not the first time my employer had raised such a question;
  indeed, it seems to be something which genuinely troubles him. On
  this occasion, in fact, a reply of sorts did occur to me as I stood
  up there on the ladder; a reply to the effect that those of our
  profession, although we did not see a great deal of the country in
  the sense of touring the countryside and visiting picturesque sites,
  did actually 'see' more of England than most, placed as we were in
  houses where the greatest ladies and gentlemen of the land
  gathered. Of course, I could not have expressed this view to Mr
  Farraday without embarking upon what might have seemed a
  presumptuous speech. I thus contented myself by saying
  simply:<br><br>

  'It has been my privilege to see the best of England over the years,
  sir, within these very walls.'
  </code>
  </div>
</div>

<p>Subdivide each document \(D_i\) into sentences \(s_{i,j}\). Perform
stop word removal and stemming, then convert the sentences into a
sentence-term matrix. Weight terms in the matrix using TF-IDF and
normalize each row to correct for sentence length. Finally, use cosine
similarity to compute a pairwise sentence similarity matrix, convert
this to a dissimilarity matrix, then use <i>k</i>-means clustering to
cluster the sentences into ten clusters.</p>


<div id="prac-prob-3" class="detail">
  <h3>Practice Problem 3 Solution</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>The following snippet of Python code will produce a
    term--document frequency matrix. For simplicity of display, the
    matrix is transposed, but by standard definition rows represent
    documents and columns represents terms. Each cell the matrix is
    the frequency \(t_{i,j}\) of term \(t_i\) in document \(D_j\).</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-06-img" class="code-div-img"></span>
        <div id="code-06">
        >>> import nltk<br>
        >>> import re<br>
        >>> import string<br>
        >>><br>
        >>> from sklearn.feature_extraction.text import TfidfVectorizer<br>
        >>> from sklearn.metrics.pairwise import cosine_similarity<br>
        >>> from sklearn.cluster import KMeans<br>
        >>><br>
        >>> nltk.download( 'stopwords' )<br>
        >>><br>
        >>> txt = [<br>
        ... <span class="tab-1">'I was thirty-seven then, strapped in my seat as the huge 747 plunged through dense cloud cover on approach to the Hamburg airport. Cold November rains drenched the earth and lent everything the gloomy air of a Flemish landscape: the ground crew in rain gear, a flag atop a squat airport building, a BMW billboard. So Germany again. Once the plane was on the ground, soft music began to flow from the ceiling speakers: a sweet orchestral cover version of the Beatles\' "Norwegian Wood." The melody never failed to send a shudder through me, but this time it hit me harder than ever. I bent forward in my seat, face in hands to keep my skull from splitting open. Before long one of the German stewardesses approached and asked in English if I were sick. "No," I said, "just dizzy." "Are you sure?" "Yes, I\'m sure. Thanks." She smiled and left, and the music changed to a Billy Joel tune. I straightened up and looked out the plane window at the dark clouds hanging over the North Sea, thinking of what I had lost in the course of my life: times gone forever, friends who had died or disappeared, feelings I would never know again.',</span><br>
        ... <span class="tab-1">'The train came out of the long tunnel into the snow country. The earth lay white under the night sky. The train pulled up at a signal stop. A girl who had been sitting on the other side of the car came over and opened the window in front of Shimamura. The snowy cold poured in. Leaning far out the window, the girl called to the station master as though he were a great distance away. The station master walked slowly over the snow, a lantern in his hand. His face was buried to the nose in a muffler, and the flaps of his cap were turned down over his ears. It\'s that cold, is it, thought Shimamura. Low, barracklike buildings that might have been railway dormitories were scattered here and there up the frozen slope of the mountain. The white of the snow fell away into the darkness some distance before it reached them.',</span><br>
        ... <span class="tab-1">'I always called him "Sensei." I shall therefore refer to him simply as "Sensei," and not by his real name. It is not because I consider it more discreet, but it is because I find it more natural that I do so. Whenever the memory of him comes back to me now, I find that I think of him as "Sensei" still. And with pen in hand, I cannot bring myself to write of him in any other way. It was at Kamakura, during the summer holidays, that I first met Sensei. I was then a very young student. I went there at the insistence of a friend of mine, who had gone to Kamakura to swim. We were not together for long. It had taken me a few days to get together enough money to cover the necessary expenses, and it was only three days after my arrival that my friend received a telegram from home demanding his return. His mother, the telegram explained, was ill. My friend, however, did not believe this. For some time his parents had been trying to persuade him, much against his will, to marry a certain girl. According to our modern outlook, he was really too young to marry. Moreover, he was not in the least fond of the girl. It was in order to avoid an unpleasant situation that instead of going home, as he normally would have done, he had gone to the resort near Tokyo to spend his holidays. He showed me the telegram, and asked me what he should do. I did not know what to tell him. It was, however, clear that if his mother was truly ill, he should go home. And so he decided to leave after all. I, who had taken so much trouble to join my friend, was left alone. There were many days left before the beginning of term, and I was free either to stay in Kamakura or to go home. I decided to stay. My friend was from a wealthy family in the Central Provinces, and had no financial worries. But being a young student, his standard of living was much the same as my own. I was therefore not obliged, when I found myself alone, to change my lodgings. My inn was in a rather out-of-the-way district of Kamakura, and if one wished to indulge in such fashionable pastimes as playing billiards and eating ice cream, one had to walk a long way across rice fields. If one went by rickshaw, it cost twenty sen. Remote as the district was, however, many rich families had built their villas there. It was quite near the sea also, which was convenient for swimmers such as myself. I walked to the sea every day, between thatched cottages that were old and smoke-blackened. The beach was always crowded with men and women, and at times the sea, like a public bath, would be covered with a mass of black heads. I never ceased to wonder how so many city holiday-makers could squeeze themselves into so small a town. Alone in this noisy and happy crowd, I managed to enjoy myself, dozing on the beach or splashing about in the water. It was in the midst of this confusion that I found Sensei. In those days, there were two tea houses on the beach. For no particular reason, I had come to patronize one of them. Unlike those people with their great villas in the Hase area who had their own bathing huts, we in our part of the beach were obliged to make use of these tea houses which served also as communal changing rooms. In them the bathers would drink tea, rest, have their bathing suits rinsed, wash the salt from their bodies, and leave their hats and sunshades for safe-keeping. I owned no bathing suit to change into, but I was afraid of being robbed, and so I regularly left my things in the tea house before going into the water.',</span><br>
        ... <span class="tab-1">'It seems increasingly likely that I really will undertake the expedition that has been preoccupying my imagination now for some days. An expedition, I should say, which I will undertake alone, in the comfort of Mr Farraday\'s Ford; an expedition which, as I foresee it, will take me through much of the finest countryside of England to the West Country, and may keep me away from Darlington Hall for as much as five or six days. The idea of such a journey came about, I should point out, from a most kind suggestion put to me by Mr Farraday himself one afternoon almost a fortnight ago, when I had been dusting the portraits in the library. In fact, as I recall, I was up on the step-ladder dusting the portrait of Viscount Wetherby when my employer had entered carrying a few volumes which he presumably wished returned to the shelves. On seeing my person, he took the opportunity to inform me that he had just that moment finalized plans to return to the United States for a period of five weeks between August and September. Having made this announcement, my employer put his volumes down on a table, seated himself on the chaise-longue, and stretched out his legs. "You realize, Stevens, I don\'t expect you to be locked up here in this house all the time I\'m away. Why don\'t you take the car and drive off somewhere for a few days? You look like you could make good use of a break." Coming out of the blue as it did, I did not quite know how to reply to such a suggestion. I recall thanking him for his consideration, but quite probably I said nothing very definite for my employer went on: "I\'m serious, Stevens. I really think you should take a break. I\'ll foot the bill for the gas. You fellows, you\'re always locked up in these big houses helping out, how do you ever get to see around this beautiful country of yours?" This was not the first time my employer had raised such a question; indeed, it seems to be something which genuinely troubles him. On this occasion, in fact, a reply of sorts did occur to me as I stood up there on the ladder; a reply to the effect that those of our profession, although we did not see a great deal of the country in the sense of touring the countryside and visiting picturesque sites, did actually \'see\' more of England than most, placed as we were in houses where the greatest ladies and gentlemen of the land gathered. Of course, I could not have expressed this view to Mr Farraday without embarking upon what might have seemed a presumptuous speech. I thus contented myself by saying simply: "It has been my privilege to see the best of England over the years, sir, within these very walls."'</span><br>
        ... ]<br>
        >>> <br>
        >>> #  Split text blocks into sentences<br>
        >>> <br>
        >>> full_sent = [ ]<br>
        >>> for i in range( 0, len( txt ) ):<br>
        ... <span class="tab-1">sent = re.sub( r'[\.!\?]"', '"', txt[ i ] )</span><br>
        ... <span class="tab-1">full_sent += re.split( '[\.!\?]', sent )</span><br>
        >>> full_sent = [sent.strip() for sent in full_sent]<br>
        >>><br>
        >>> #  Remove empty sentences<br>
        >>> <br>
        >>> i = 0<br>
        >>> while i < len( full_sent ):<br>
        ... <span class="tab-1">if len( full_sent[ i ] ) == 0:</span><br>
        ... <span class="tab-2">del full_sent[ i ]</span><br>
        ... <span class="tab-1">else:</span><br>
        ... <span class="tab-2">i += 1</span><br>
        >>> <br>
        >>> #  Remove punctuation<br>
        >>> <br>
        >>> sent = [ ]<br>
        >>> <br>
        >>> punc = string.punctuation.replace( '-', '' )<br>
        >>> for i in range( 0, len( full_sent ) ):<br>
        ... <span class="tab-1">sent.append( re.sub( '[' + punc + ']+', '', full_sent[ i ] ) )</span></br>
        >>> <br>
        >>> #  Porter stem<br>
        >>> <br>
        >>> porter = nltk.stem.porter.PorterStemmer()<br>
        >>> stems = { }<br>
        >>> <br>
        >>> for i in range( 0, len( sent ) ):<br>
        ... <span class="tab-1">tok = sent[ i ].split()</span></br>
        ... <span class="tab-1">for j in range( 0, len( tok ) ):</span></br>
        ... <span class="tab-2">if tok[ j ] not in stems:</span></br>
        ... <span class="tab-3">stems[ tok[ j ] ] = porter.stem( tok[ j ] )</span></br>
        ... <span class="tab-2">tok[ j ] = stems[ tok[ j ] ]</span></br>
        ... <br>
        ... <span class="tab-1">sent[ i ] = ' '.join( tok )</span></br>
        >>><br>
        >>> #  Remove empty sentences after stop word removal<br>
        >>> <br>
        >>> i = 0<br>
        >>> while i < len( sent ):<br>
        ... <span class="tab-1">if len( sent[ i ] ) == 0:</span></br>
        ... <span class="tab-2">del sent[ i ]</span></br>
        ... <span class="tab-1">else:</span></br>
        ... <span class="tab-2">i += 1</span></br>
        >>> <br>
        >>> #  Convert frequencies to TF-IDF values, get cosine similarity<br>
        >>> #  of all pairs of documents<br>
        >>> <br>
        >>> tfidf = TfidfVectorizer( stop_words='english', max_df=0.8, max_features=1000 )<br>
        >>> term_vec = tfidf.fit_transform( sent )<br>
        >>> X = cosine_similarity( term_vec )<br>
        >>> <br>
        >>> #  Fit vectors to clusters<br>
        >>> <br>
        >>> clust = KMeans( n_clusters=5, random_state=1 ).fit( X )<br>
        >>> print( clust.labels_ )<br>
        >>> <br>
        >>> for i in range( 0, len( set( clust.labels_ ) ) ):<br>
        ... <span class="tab-1">print( f'Cluster {i}:' )</span><br>
        ...<br>
        ... <span class="tab-1">for j in range( 0, len( clust.labels_ ) ):</span><br>
        ... <span class="tab-2">if clust.labels_[ j ] == i:</span><br>
        ... <span class="tab-3">print( full_sent[ j ].replace( '"', '' ).strip() )</span><br>
        ...<br>
        ... <span class="tab-1">print()</span><br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p><i>k</i>-means requires a choice of the number of clusters to form. The common approach to choosing this number is the <i>elbow method</i>. Here, different values of <i>k</i> are chosen, starting at 1, and incrementing by 1 until the "error" in the resulting clusters begins to stabilize. Below is a function to do this, using values of distortion and inertia as metrics for error. Distortion is the average of the squared distances between the cluster centers. Inertia is the sum of squared distances of samples to their closest cluster center. <code>sklearn</code>'s <code>KMeans</code> algorithm provides inertia, and also provides cluster center positions, which allows distortion to be easily calculated.</p>

    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-07-img" class="code-div-img"></span>
        <div id="code-07">
        >>> import numpy as np<br>
        >>> from scipy.spatial.distance import cdist<br>
        >>> <br>
        >>> def elbow( X, max_clust=25 ):<br>
        ... <span class="tab-1">distort = [ ]</span><br>
        ... <span class="tab-1">inertia = [ ]</span><br>
        ... <br>
        ... <span class="tab-1">map_distort = { }</span><br>
        ... <span class="tab-1">map_inertia = { }</span><br>
        ... <br>
        ... <span class="tab-1">elbow_distort = 1</span><br>
        ... <span class="tab-1">elbow_inertia = 1</span><br>
        ... <br>
        ... <span class="tab-1">K = range( 1, max_clust )</span><br>
        ... <span class="tab-1">for k in K:</span><br>
        ... <span class="tab-2">kmean_model = KMeans( n_clusters=k )</span><br>
        ... <span class="tab-2">kmean_model.fit( X )</span><br>
        ... <br>    
        ... <span class="tab-2">distort.append( sum( np.min( cdist( X, kmean_model.cluster_centers_, 'euclidean' ), axis=1 ) ) / X.shape[ 0 ] )</span><br>
        ... <span class="tab-2">inertia.append( kmean_model.inertia_ )</span><br>
        ... <br>    
        ... <span class="tab-2">map_distort[ k ] = sum( np.min( cdist( X, kmean_model.cluster_centers_, 'euclidean' ), axis=1 ) ) / X.shape[ 0 ]</span><br>
        ... <span class="tab-2">map_inertia[ k ] = kmean_model.inertia_</span><br>
        ... <br>    
        ... <span class="tab-1">prev_k = ''</span><br>
        ... <span class="tab-1">prev_v = 0</span><br>
        ... <span class="tab-1">prev_pct = 0</span><br>
        ... <span class="tab-1">for i,(k,v) in enumerate( map_distort.items() ):</span><br>
        ... <span class="tab-2">if prev_k == '':</span><br>
        ... <span class="tab-3">print( f'{k}: {v:.4f}' )</span><br>
        ... <span class="tab-3">prev_k = str( k )</span><br>
        ... <span class="tab-3">prev_v = v</span><br>
        ... <span class="tab-3">continue</span><br>
        ... <br>        
        ... <span class="tab-2">print( f'{k}: {v:.4f}  ', end='' )</span><br>
        ... <br>    
        ... <span class="tab-2">diff_v = prev_v - v</span><br>
        ... <span class="tab-2">diff_v_pct = diff_v / prev_v * 100.0</span><br>
        ... <span class="tab-2">print( f'{diff_v:.4f}, {diff_v_pct:.2f}%' )</span><br>
        ... <br>    
        ... <span class="tab-2">if i > 2 and prev_pct - diff_v_pct < 0.5:</span><br>
        ... <span class="tab-3">elbow_distort = i + 1</span><br>
        ... <span class="tab-3">break</span><br>
        ... <br>        
        ... <span class="tab-2">prev_k = str( k )</span><br>
        ... <span class="tab-2">prev_v = v</span><br>
        ... <span class="tab-2">prev_pct = diff_v_pct</span><br>
        ... <br>    
        ... <span class="tab-1">print()</span><br>
        ... <br>
        ... <span class="tab-1">prev_k = ''</span><br>
        ... <span class="tab-1">prev_v = 0</span><br>
        ... <span class="tab-1">prev_pct = 0</span><br>
        ... <span class="tab-1">for i,(k,v) in enumerate( map_inertia.items() ):</span><br>
        ... <span class="tab-2">if prev_k == '':</span><br>
        ... <span class="tab-3">print( f'{k}: {v:.4f}' )</span><br>
        ... <span class="tab-3">prev_k = str( k )</span><br>
        ... <span class="tab-3">prev_v = v</span><br>
        ... <span class="tab-3">continue</span><br>
        ... <br>        
        ... <span class="tab-2">print( f'{k}: {v:.4f}  ', end='' )</span><br>
        ... <br>    
        ... <span class="tab-2">diff_v = prev_v - v</span><br>
        ... <span class="tab-2">diff_v_pct = diff_v / prev_v * 100.0</span><br>
        ... <span class="tab-2">print( f'{diff_v:.4f}, {diff_v_pct:.2f}%' )</span><br>
        ... <br>    
        ... <span class="tab-2">if i > 2 and prev_pct - diff_v_pct < 0.5:</span><br>
        ... <span class="tab-3">elbow_inertia = i + 1</span><br>
        ... <span class="tab-3">break</span><br>
        ... <br>        
        ... <span class="tab-2">prev_k = str( k )</span><br>
        ... <span class="tab-2">prev_v = v</span><br>
        ... <span class="tab-2">prev_pct = diff_v_pct</span><br>
        ... <br>
        ... <span class="tab-1">return max( elbow_distort, elbow_inertia )</span><br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p>Normally, we would examine a plot of the two curves to identify
    the elbow. Since this is not possible in a function, we instead
    use the following approach to identify an elbow in either
    curve.</p>

    <ul>
    <li>Compute the distortion for the current <i>k</i>.
    <li>Determine the difference in distortion \(d_{\Delta} =
    d_{k-1} - d_k\) for the previous and current values of <i>k</i>.
    <li>Convert the absolute difference to a percentage \(d_{\%,k} =
    d_{\Delta} / d_{k-1}\).
    <li>If the difference in consecutive percentages \(d_{\%,k-1} -
    d_{\%,k} < 0.5\), choose <i>k</i> as the elbow number of
    clusters.
    <li>Notice that the above test will automatically terminate if
    \(d_{\%}\) begins to increase over two consecutive values
    of <i>k</i>.
    
    </ul>

    <p>Given an elbow <i>k</i> for both distortion and inertia, return
    the larger of the two as the overall number of clusters <i>k</i>
    to generate.</p>

    <p>Finally, if you run the TF-IDF&ndash;cosine topic clustering,
    you may find the topics it generates... uninspiring. Perhaps a
    concept&ndash;document approach would produce better topics? Below
    we apply latent Dirichlet allocation (LDA) rather than TF-IDF
    alone. Note that we assume all of the variables and functions
    defined above exist for use in the following code.</p>
    
    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-08-img" class="code-div-img"></span>
        <div id="code-08">
        >>> import pandas as pd<br>
        >>> from sklearn.decomposition import LatentDirichletAllocation as LDA<br>
        >>> from sklearn.feature_extraction.text import CountVectorizer<br>
        >>> <br>
        >>> #  Count raw term frequencies<br>
        >>> <br>
        >>> count = CountVectorizer( stop_words='english' )<br>
        >>> term_vec = count.fit_transform( sent )<br>
        >>> <br>
        >>> n_topic = 10<br>
        >>> <br>
        >>> #  Build a string list of [ 'Topic 1', 'Topic 2', ..., 'Topic n' ]<br>
        >>> <br>
        >>> col_nm = [ ]<br>
        >>> for i in range( 1, n_topic + 1 ):<br>
        ... <span class="tab-1">col_nm += [ f'Topic {i}' ]</span><br>
        >>> <br>
        >>> #  Fit an LDA model to the term vectors, get cosine similarities<br>
        >>> <br>
        >>> lda_model = LDA( n_components=n_topic )<br>
        >>> concept = lda_model.fit_transform( term_vec )<br>
        >>> X = cosine_similarity( concept )<br>
        >>> <br>
        >>> #  Print top 10 terms for each topic<br>
        >>> <br>
        >>> feat = count.get_feature_names()<br>
        >>> topic_list = [ ]<br>
        >>> for i,topic in enumerate( lda_model.components_ ):<br>
        ... <span class="tab-1">top_n = [ feat[ i ] for i in topic.argsort()[ -10: ] ]</span><br>
        ... <span class="tab-1">top_feat = ' '.join( top_n )</span><br>
        ... <span class="tab-1">topic_list.append( f"topic_{'_'.join(top_n[ :3 ] ) }" )</span><br>
        ... <br>
        ... <span class="tab-1">print( f'Topic {i}: {top_feat}' )</span><br>
        >>> print()<br>
        >>><br>
        >>> #  Cluster sentences and print clusters<br>
        >>><br>
        >>> clust = KMeans( n_clusters=5 ).fit( concept )<br>
        >>><br>
        >>> for i in range( 0, len( set( clust.labels_ ) ) ):<br>
        ... <span class="tab-1">print( f'Cluster {i}:' )</span><br>
        ... <span class="tab-1">for j in range( 0, len( clust.labels_ ) ):</span><br>
        ... <span class="tab-2">if clust.labels_[ j ] != i:</span><br>
        ... <span class="tab-3">continue</span><br>
        ... <span class="tab-2">print( full_sent[ j ] )</span><br>
        ... <br>
        ... <span class="tab-1">print()</span><br>
        </div>
      </td>
      </tr>
      </table>
    </div>

    <p>One <b>very important</b> note about LDA
    in <code>sklearn</code>: TF-IDF weighting is automatically applied
    within the LDA algorithm.  This means you <b>must not</b> TF-IDF
    weight your freqencies before passing them
    into <code>sklearn</code>'s LDA model. This is why we use
    a <code>CountVectorizer</code> (compute raw frequency counts) and
    not a <code>TfidfVectorizer</code>.</p>

    <p>Also note that we use the document&ndash;concept matrix
    directly to compute cosine similarity. Unlike the example we
    showed with LSA, there's no need to convert the LDA results back
    to a document&ndash;term matrix. We did that with LSA only to make
    it easier to explain why LSA was applying non-zero weights to
    terms that did not appear in a document.</p>

    <p>In reality, LDA doesn't seem to do a much better job than
    TF-IDF. One conclusion could be that there really aren't common
    topics in the sentences we've extracted. If this is true, then
    TF-IDF and LDA are both doing the best that they can. We would
    expect more intuitive results if we ran through text that actually
    corresponded to a set of definitive topics.</p>

  </div>
</div>


<h2 id="summarize">Summarization</h2>

<p>
A common task when large document collections are analyzed is
automatically compressing their content into a condensed text
summarization that highlights the salient topics or information
contained in the document collection. Not surprisingly, this is a
difficult problem, particularly when the document collection is large.
Three standard methods have been proposed, in increasing order of
complexity: (1) representative term extraction; (2) representative
sentence extraction; and (3) representative sentence construction. We
will focus on the first two techniques, since construction of unique
sentences based on topics or information in the collection requires a
level of NLP that is beyond the scope of what we are discussing in
this module.
</p>

<p>
In an ideal situation, automatic text summarization would: (1)
recognize the content of a document collection; and (2) summarize the
collection's central ideas. This requires a system that
<i>understands</i> the semantics of the collection. Since this is
an unsolved problem, most summarization algorithms fall back to
<i>extracting</i> text from the document collection to summarize it.
This has the disadvantage of generating extractions that may not be
coherent. However, a reader can normally infer relationships between
the extractions to form a reasonable understanding of the content
document collection contains.
</p>

<h3>Term Extraction</h3>

<p>
In term extraction, we attempt to identify a small set of terms
that accurately represent the main topics, ideas, or concepts contained
in a document collection. Various simple methods exist to select
the terms to extract.</p>

<ul>

<li><b>Term Frequency.</b> Terms that occur frequently in the document
collection can be assumed to be important, and therefore can be
included in an extract term summary.

<li><b>Weighted Term Frequency.</b> A weighted term frequency like
TF-IDF can be used in place of raw term frequency, since this is
designed to take into account both the absolute frequency of a term in
a document, and the term's uniqueness across documents in the
collection.

<li><b>Sentence Position.</b> The position of a sentence within a
document can imply the sentence's importance. For example, sentences
at the beginning of a document often summarize the document, and so
are considered more important than internal sentences. Term
frequencies can be weighted by the estimated importance of their
parent sentence.

<li><b>Title Words.</b> Terms in a document's title are usually
considered critical, since by design a title is meant to summarize
the content of the document. These terms should be increased in
weight to recognize this.

<li><b>Cue Words.</b> Certain words like "important" or "relevant"
within a sentence may indicate that terms in the sentence are directly
related to topics contained in a document. Sentences with cue words
can have the weights of the terms increased to represent this.

<li><b>Concept Relevance.</b> Certain terms can be converted to
<i>concepts</i> by considering properties like synonyms or part
meronyms. For example, the term "bicycle" could be converted to a
"bicycle" concept by converting the terms "bike" (synonym) and "pedal"
(part meronym) to "bicycle".

</ul>
</p>


<h3>Sentence Extraction</h3>

<p>
A simple approach to sentence extraction is to weight terms in the
sentence, the apply an aggregation to those terms to estimate an
overall sentence score. Any of the term extraction techniques
discussed above could be used to do this. For example, TF-IDF scores
for each term in a sentence could be averaged prior to normalizing
each term vector. Other weights (position, title, cue word) could be
used to adjust the term weights up or down before averaging. Different
aggregation methods could also be applied (maximum, minimum, median,
and so on).
</p>

<p>
Another approach that has recently been suggested converts sentences
into a graph, then applies a ranking algorithm like <a target="_blank"
href="https://dl.acm.org/citation.cfm?id=324140">HITS</a> or Google's
<a target="_blank"
href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.5427">PageRank</a>
to identify high rank sentences to include in a summary. As an
example, consider the Google PageRank algorithm.
</p>

<p>
To begin, sentences in a document or document collection must be
converted into a graph structure. Each sentence is represented as a
node, and the connection between pairs of sentences are represented by
an edge between the sentences' nodes. An edge is weighted based on the
similarity between the two sentences it connects, representing the
concept overlap between the sentences. Only sentences with a
similarity above a user-chosen threshold are included in the graph.
</p>

<p>
Next, the PageRank algorithm is run on the graph. In its general form,
PageRank estimates the importance of a node by counting the number and
quality of edges connecting to that node. Theoretically, PageRank
represents the probability distribution for which node a random click
would land on. Nodes are initialized with a weight of \(\frac{1}{n}\)
for a graph containing <i>n</i> nodes. Next, each node evenly
"contributes" its weight to nodes it links to, updating the node
weights. This contribution process is run iteratively until the
weights converge such that the average PageRank over the graph is 1. A
damping factor <i>d</i> is used on each iteration to enforce
convergence.
</p>

<p>
As an example consider a four-node graph <b>A</b>, <b>B</b>, <b>C</b>,
and <b>D</b>, where <b>B</b> links to <b>A</b> and <b>C</b>, <b>C</b>
links to <b>A</b> and <b>D</b> links to <b>A</b>, <b>B</b>,
and <b>C</b>. The PageRank <i>PR</i> for <b>A</b> would be

<div style="font-size: 90%;">
\[
PR(A) = \frac{PR(B)}{2} + PR(C) + \frac{PR(D)}{3} = 0.458
\]
</div>

<p>In other words, given the number of outgoing links <i>L</i> for a
node, a target node <i>q</i>, and the child nodes <i>C<sub>q</sub></i>
that link to <i>q</i>, the PageRank of <i>q</i> is defined as
</p>

<div style="font-size: 90%;">
\[
PR(q) = \sum_{r \in C_{q}} \frac{PR(r)}{L(r)}
\]
</div>

<p>
Adding in the damping factor <i>d</i> which represents the probability
that a person will eventually stop clicking on random links, the above
formula becomes:
</p>

<div style="font-size: 90%;">
\[
PR(q) = \frac{1-d}{n} + d \sum_{r \in C_{q}} \frac{PR(r)}{L(r)}
\]
</div>

<p>
The original value for <i>d</i> was 0.85. For our example,
the PageRank of <b>A</b> would be 0.427.
</p>

<div style="font-size: 90%;">
\[
PR(A) = \frac{0.15}{4} + 0.85 \left( \frac{PR(B)}{2} + PR(C) + \frac{PR(D)}{3}\right) = 0.427
\]
</div>

<p>
Since the general PageRank algorithm does not assume any weights on the
edges, these must be added. This is done by updating the basic PageRank
algorithm as follows.
</p>

<div style="font-size: 90%;">
\[
PR^{w}(q) = \frac{1-d}{n} + d \sum_{r \in C_{q}} w_{q,r} \frac{PR^{w}(r)}{\sum_{s \in P_{r}} w_{q,s} }
\]
</div>

<p>
where <i>w<sub>q,r</sub></i> is the edge weight between nodes <i>q</i>
and <i>r</i>, and <i>P<sub>r</sub></i> are the parents of
node <i>r</i> (<i>i.e.</i>, the nodes <i>r</i> connects to), and
\(\sum_{s \in P_{r}} w_{q,s}\) is the sum of the edges leaving
node <i>r</i>. Although this will give different ranks to the nodes
compared to the original PageRank algorithm, about the same number of
iterations are needed to converge to a set of final values. Once this
is done, sentences are sorted in descending order of rank, and the
top <i>k</i> sentences are returned as a summary of the document or
document collection.
</p>

<p>
LSA can also be used to extract summary sentences from a document
collection. Recall that LSA uses SVD to factor a term&ndash;sentence
matrix <i>X</i> into three matrices <i>X</i>
= <i>U&Sigma;V<sup>T</sup></i>. This converts <i>X</i> into a latent
semantic structure where the columns of <i>U</i> form <i>concepts</i>
as linear combinations of the original terms in <i>X</i>, the rows of
<i>V<sup>T</sup></i> describe the amount of each concept contained in
individual sentences, and <i>&Sigma;</i> is a diagonal matrix that can
be seen as defining the importance of individual topics. The
factorization is normally reduced to the top <i>k</i> eigenvalues and
corresponding columns and rows in <i>U</i> and <i>V<sup>T</sup></i>.
</p>

<p>
<a target="_blank"
href="http://www.cai.sk/ojs/index.php/cai/article/viewFile/37/24">An
initial suggestion</a> for using LSA during summary sentence
extraction was to extract the sentences in <i>V<sup>T</sup></i> with
the largest value for each topic. This is done by simply collecting
sentences with the largest value in the <i>k</i>-th column
of <i>V<sup>T</sup></i>, then presenting these as the sentence
extraction summary.
</p>

<p>
The potential problem with this approach is that it treats each topic
as equally important. We know this is not true, since the eigenvalues
in &Sigma; differentiate the importance of different
topics. <a target="_blank"
href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.ijcnlp05.pdf">An
improved alternative</a> is to calculate <i>&Sigma;<sup>2</sup>V</i>,
then select sentences <i>s<sub>i</sub></i> with the greatest length
\(\sqrt{\sum_j s_{i,j} }\). The intuition is to choose sentences with
the largest combined weight across all important topics. This may
include more than one sentence discussing an important topic, or a
sentence that discusses multiple important topics. When a document
collection is summarized, the authors suggest first applying the graph
summarization algorithm to individual documents <i>D<sub>i</sub></i>,
generating document summary sentences <i>s<sub>i,j</sub></i>. Next,
the summary sentences are combined into a graph and further summarized
using exactly the same graph-based summary algorithm, producing an
overall document collection summary.
</p>

<p>
Experimental analysis suggests this does a better job of identifying
sentences that provide a good summary of the important topics and
information in a document collection.
</p>


<h2 id="sentiment">Sentiment</h2>

<p>
Sentiment is defined as "an attitude, thought, or judgment prompted by
feeling." An area of recent interest in text analytics is estimating
the sentiment contained in a block of text. This has prompted a number
of basic questions. For example, how should sentiment or emotion be
characterized so we can measure it? And how can these measurements be
extracted from text?
</p>


<h3>Emotional Models</h3>

<p>
Psychologists have proposed various models to define different
emotional states. In psychology <i>mood</i> refers to a medium or
long-term affective state, where <i>affect</i> is described using
dimensions like pleasure, arousal, and engagement.
</p>

<p>
Psychological models use emotional dimensions to position emotions on
a 2D plane. The simplest models represents pleasure along a horizontal
axis, with highly unpleasant on one end, highly pleasant on the other,
and different levels of pleasure in between. For example, when
sentiment is described as positive, negative, or neutral, it is
normally assumed that "sentiment" means pleasure.
</p>

<p>
More complex models use more than a single dimension. For example,
Russell proposed using valence (or pleasure) and arousal (or
activation) to build an emotional circumplex of affect (Russell, J. A.
<a target="_blank"
href="https://www2.bc.edu/~russeljm/publications/Russell1980.pdf">A
Circumplex Model of Affect</a>. <i>Journal of Personality and Social
Psychology 39</i>, 6, 1161&ndash;1178, 1980). Russell applied
multidimensional scaling to position 28 emotional states, producing
the model shown to the left with valence running along the horizontal
axis and arousal along the vertical axes. The intermediate terms
excited–depressed and distressed–relaxed are polar opposites formed by
intermediate states of valence and arousal.
</p>

<table class="center">
<caption style="text-align: center;">
  <a target="_blank"
  href="https://www2.bc.edu/~russeljm/publications/Russell1980.pdf">Russell's
  model of emotional affect</a>
</caption>
<tr>
  <td style="text-align: center; border: 0px; padding-bottom: 0px;">
    <img src="./fig/circumplex.png">
  </td>
</tr>
</table>

<p>
Similar models have been proposed by Watson and Tellegen (with
positive and negative valence axes), Thayer (with tension and energy
axes), and Larsen and Diener (with pleasure and activation axes
similar to Russell's). The circumplex can be further subdivided into
additional emotional regions like happy, sad, calm, and tense.
</p>


<h3>Natural Language Processing</h3>

<p>
The area of natural language processing (NLP) in computer science is
often used to analyze the structure of a text body. For example, it is
useful to subjectivity classification to remove objective, fact-based
text prior to estimating sentiment. Pang and Lee proposed a
sentence-level subjectivity detector that computes a subjectivity
weight for each sentence (Pang, B. and Lee, L. <a target="_blank"
href="http://dl.acm.org/citation.cfm?id=1218990">A Sentimental
Education. Sentiment Analysis Using Subjectivity Summarization Based
on Minimum Cuts</a>. <i>Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics (ACL '04)</i>,
271&ndash;278, 2004). Pairs of sentences are assigned an association
score based on the difference of their subjectivity scores to estimate
whether they belong to a common class. A graph is constructed with
sentences and the two classifications (subjective and objective)
forming nodes, association weights forming edges between sentences,
and subjectivity weights forming edges between each sentence and the
classification nodes. A minimum graph cut is then used to split the
sentences into subjective and objective classes.
</p>

<p>
Another common NLP method for sentiment analysis is to train a machine
learning algorithm on a set of documents with known sentiment. Naive
Bayes, maximum entropy, and support vector machine (SVM) approaches
were compared for classifying movie reviews as positive or negative.
The presence of absence of a term (unigrams) performed best, with
accuracies ranging from 80.4% for maximum entropy to 82.9% for
SVM. Interestingly, more complex inputs like bigrams, term
frequencies, part of speech tagging, and document position information
did not improve performance.
</p>

<p>
In a similar manner, semantic orientation has been used to rate online
reviews as positive or negative (Turney, P. <a target="_blank"
href="http://dl.acm.org/citation.cfm?id=1073153">Thumbs Up or Thumbs
Down? Semantic Orientation Applied to Unsupervised Classification of
Reviews</a>. <i>Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL '02)</i>,
417&ndash;424, 2002). The semantic orientation of phrases in a review
are compared to the anchor words "excellent" and "poor" by extracting
phrases containing predefined target terms, then using pointwise
mutual information (PMI) to calculate the statistical dependence
between each phrase and the anchor words. The difference PMI(phrase,
"excellent") &minus; PMI(phrase, "poor") estimates the direction and
the strength of a phrase's semantic orientation. Results for reviews
about automobiles, banks, movies, and travel destinations produced
accuracies of 84%, 80%, 65.8% and 70.5%, respectively.
</p>


<h3>Sentiment Dictionaries</h3>

<p>
An alternative to natural language approaches uses a term dictionary
to estimate sentiment, often for short text snippets like online
comments or social network conversations. Proponents argue that a
short post does not contain enough text and grammar for an NLP
algorithm to leverage, and therefore, independent word analysis may
produce results comparable to NLP.
</p>

<p>
Profile of mood states (POMS) was originally designed as a
psychometric tool to measure a person's mood state on six dimensions:
tension&ndash;anxiety, depression&ndash;dejection,
anger&ndash;hostility, fatigue&ndash;inertia, vigor&ndash;activity,
and confusion&ndash;bewilderment. Subjects rate 65 adjectives on a
five-point scale to produce a score in each dimension that can be
compared to population norms. POMS was extended by including 793
synonyms of the original 65 adjectives to form POMS-ex, a word
dictionary used to estimate sentiment.
</p>

<p>
Affective Norms for English Words (ANEW) was built to assess the
emotional affect for a set of verbal terms. Three emotional dimensions
were scored on a nine-point scale: valence (or pleasure), arousal (or
activation), and dominance. A total of 1,033 word that were previously
identified as emotion-carrying words, and that provided good coverage
of all three dimensions, were rated.
</p>

<p>
Recent lexical approaches have focused specifically on short text and
online social networks. SentiStrength was developed from manually
scoring 2,600 MySpace comments on two five-point scales representing
both the positive and the negative emotion of a comment. Analysis of
the training set produced a dictionary of 298 positive terms and 465
negative terms (Thelwall, M., Buckley, K., Paltoglou, G., Cai, D., and
Kappas, A. <a target="_blank"
href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21416/abstract">Sentiment
Strength Detection in Short Informal Text</a>. <i>Journal of the
American Society for Information Science and Technology 61</i>, 12,
2544&ndash;2558, 2010). SentiStrength is augmented to recognize
properties of social network text like emoticons, text abbreviations,
and repeated letters and punctuation, as well simple use of booster
words (somewhat, very) and negation.
</p>

<p>
WordNet is a lexical database that groups English nouns, verbs,
adjectives, and adverbs into sets of cognitive
synonyms---synsets---that represent distinct concepts. SentiWordNet
estimates the sentiment of synsets by assigning them a positive,
negative, and objective (or neutral) score on the range &minus;1 to
&plus;1 (Baccianella, S., Esuli, A., and Sebastiani,
F. <a target="_blank"
href="http://www.esuli.it/wp-content/uploads/2011/07/LREC10.pdf">SentiWordNet
3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion
Mining</a>.
<i>Proceedings of the 7th International Conference on Language
Resources and Evaluation (LREC '10)</i>, 2200&ndash;2204,
2010). Individual terms can be matched to their synset, and since
these are taken from WordNet, properties like word disambiguation are
automatically included.
</p>


<h3>Estimating Sentiment</h3>

<p>
As a practical example of estimating sentiment, consider analysis
using ANEW's independent word dictionary. ANEW has a number of
potential advantages. First, its word list is publically available.
Second, it contains multiple emotional dimensions, allowing us to
characterize sentiment in ways that more sophisticated than a simple
positive&ndash;negative rating. We apply ANEW to process a text body
as follows:
</p>

<ol>

<li>Parse the text body into terms, then identify the <i>n</i> ANEW
terms {&nbsp;<i>t</i><sub>1</sub>, &hellip;
, <i>t<sub>n</sub></i>&nbsp;} that match entries in the ANEW
dictionary.</li>

<li>For each <i>t<sub>i</sub></i>, extract the average and standard
deviation for valence
(<i>v<sub>i,</sub></i><sub>&mu;</sub>, <i>v<sub>i,</sub></i><sub>&sigma;</sub>)
and arousal
(<i>a<sub>i,</sub></i><sub>&mu;</sub>, <i>a<sub>i,</sub></i><sub>&sigma;</sub>)
from the dictionary.</li>


<li>If a text body has less than <i>n</i>=2 ANEW terms, discard it as
having insufficient measurements to estimate an overall
sentiment.</li>

<li>If <i>n</i> &ge; 2, aggregate individual term values to estimate
an overall average and standard deviation for valence
(<i>V</i><sub>&mu;</sub>, <i>V</i><sub>&sigma;</sub>) and arousal
(<i>A</i><sub>&mu;</sub>, <i>A</i><sub>&sigma;</sub>).</li>
</ol>

<p>
Calculating an overall standard deviation from a set of term average
and standard deviations pairs (&mu;<i><sub>i</sub></i>,
&sigma;<i><sub>i</sub></i>) is done using a formula for averaging
standard deviations. For example, for <i>V</i><sub>&sigma;</sub> it is
</p>


<div style="font-size: 90%;">
\[
M = \frac{1}{n} \sum_{i=1}^{n} v_{i,\mu} ,
\, \, \,
V_{\sigma}^{2} =
( \frac{1}{n} \sum_{i=1}^{n} v_{i,\sigma}^{2} + M^{2}) - M^{2}
\]
</div>


<p>
Calculating the overall average of individual term averages is more
complicated. We could use a simple unweighted average, however, this
ignores each term's standard
deviation <i>v<sub>i,</sub></i><sub>&sigma;</sub>. A large
deviation <i>v<sub>i,</sub></i><sub>&sigma;</sub> implies ambiguity in
how respondents rated a term. The term could be a homonym: lie, to not
tell the truth versus lie, to recline. The term could have been viewed
in different contexts: a sentence where the term is used directly,
"I'm happy" versus a sentence were it's negated, "I'm not happy." The
term could simply be difficult to score: what valence should we attach
to the ANEW term "bench?"

<p>
Intuitively, the larger <i>v<sub>i,</sub></i><sub>&sigma;</sub>, the
less weight we want to give <i>v<sub>i,</sub></i><sub>&mu;</sub> in
the overall average, since we are less confident about where its true
average lies. To do this, we calculate a term's cumulative
distribution function (the normal curve) <i>p</i>, then use the
probability at <i>p</i>(<i>v<sub>i,</sub></i><sub>&mu;</sub>) to
weight <i>v<sub>i,</sub></i><sub>&mu;</sub> in the overall
average. This gives a lower weights to terms with
larger <i>v<sub>i,</sub></i><sub>&sigma;</sub>.

The height of the normal curve with &mu;
= <i>v<sub>i,</sub></i><sub>&mu;</sub> and &sigma;
= <i>v<sub>i,</sub></i><sub>&sigma;</sub> at <i>x</i>
= <i>v<sub>i,</sub></i><sub>&mu;</sub> is </p>


<div style="font-size: 90%;">
\[
p_i = \frac{1}{\sqrt{2 \Pi v_{i,\sigma}^{2}}}
\]
</div>


<p>
Given <i>p<sub>i</sub></i> for each term <i>t<sub>i</sub></i>, we
normalize the <i>p<sub>i</sub></i>'s, then calculate a final weighted
average.
</p>


<div style="font-size: 90%;">
\[
V_\mu = \sum_{i=1}^{n} \frac{p_i}{\textstyle{\sum_{i=1}^{n} p_i}} v_{i,\mu}
\]
</div>


<table class="center">
<caption style="text-align: center;">
  Normal curves with <i>v<sub>i,</sub></i><sub>&mu;</sub>
  and <i>p<sub>i</sub></i> for ANEW terms <b>health</b> and <b>win</b>
</caption>
<tr>
  <td style="text-align: center; border: 0px; padding-bottom: 0px;">
    <img src="./fig/norm-curve-annotate.png">
  </td>
</tr>
</table>


<p>
Consider the tweet "Congrats to @HCP_Nevada for their <b>health</b>
care headliner <b>win</b>!" with two ANEW terms "health"
(<i>v</i><sub>&mu;</sub> = 6.81, <i>v</i><sub>&sigma;</sub> = 1.88)
and "win" (<i>v</i><sub>&mu;</sub> = 8.38, <i>v</i><sub>&sigma;</sub>
= 0.92). An unweighted average of the <i>v</i><sub>&mu;</sub>'s
produces <i>V</i><sub>&mu;</sub> = 7.56, but since the standard
deviation for health is higher than for
win, <i>v</i><sub>health,&mu;</sub> receives a weight
of <sup>&nbsp;0.21</sup>/<sub>0.64</sub> = 0.33,
while <i>v</i><sub>win,&mu;</sub> receives a weight
of <sup>&nbsp;0.43</sup>/<sub>0.64</sub> = 0.67. This produces a
weighted average <i>V</i><sub>&mu;</sub> = 7.86 that falls closer to
win's valence, exactly as we want.
</p>


<div id="anew-sentiment" class="detail">
  <h3>Term Sentiment</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>To calculate valence and arousal on your own sets of terms, we
    have implemented an extended term dictionary in Python. To use
    this dictionary, <a style="color: #e17009"
    href="./sentiment_module.zip">download
    the <code>sentiment_module.zip</code> file</a>, and unzip it to
    extract a <code>sentiment_module</code> folder. You then have two
    options for installing the module:<p>

    <ol>

    <li>Place the <code>sentiment_module</code> folder in the
    <code>.ipython</code> folder located in your home directory. This
    should allow the IPython console to see the module.</li>

    <li>Place the <code>sentiment_module</code> folder in the same
    directory where you're developing your Python program(s). This
    will allow you to run Python from the command line and load the
    term library directly.</li>

    </ol>

    <p>Here's an example of using the module, assuming you've place it
    in your <code>.ipython</code> directory and that you're running
    Python from an IPython console.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">>>> from sentiment_module import sentiment
>>> term = 'happy'
>>> sentiment.exist( term )
True
>>> sentiment.sentiment( term )
{'arousal': 6.49, 'valence': 8.21}
</code>

    </div>

    <p>The sentiment module provides a number of functions. The two
    that you are most likely to use are <code>exist</code>
    and <code>sentiment</code>:</p>

    <ul>

    <li><code>exist( term )</code>:<br>
    Returns <code>True</code> if <code>term</code> exists in the ANEW
    dictionary, <code>False</code> if it does not. <code>term</code>
    can be a string, or a list of strings.</li>

    <li><code>sentiment( term )</code>:<br> Returns
    a <code>dict</code> variable with an <code>arousal</code> field
    and a <code>valence</code> field. If <code>term</code> is a
    string, <code>sentiment</code> returns the valence and arousal for
    the given term. If <code>term</code> is a list of
    strings, <code>sentiment</code> returns the average valence and
    arousal for all recognized terms in the list.</li>

    <li><code>describe( term )</code>:<br> Returns a brief description
    of the sentmient of <code>term</code> based on the emotions around
    the circumference of Russell's emotional circumplex.</li>

    <li><code>describe( v, a )</code>:<br> Returns a brief description
    of the sentmient for a valence of <code>v</code> and an arousal
    of <code>a</code> based on the emotions around the circumference
    of Russell's emotional circumplex, 1 &leq; <code>v,a</code> &leq;
    9.</li>

    </ul>

    <p>Remember that sentiment values lie on the range 1 (minimum)
    through 9 (maximum). Below are a few examples of how to use the
    sentiment module to compute sentiment.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">>>> from sentiment_module import sentiment
>>> term = 'popsicle'
>>> sentiment.exist( term )
False
>>> term = 'enraged'
>>> sentiment.exist( term )
True
>>> sentiment.sentiment( term )
{'arousal': 7.97, 'valence': 2.46}
>>>
>>> term_list = "it was the best of times it was the worst of times".split()
>>> print term_list
['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']
>>> sentiment.exist( term_list )
[False, False, False, True, False, True, False, False, False, True, False, True]
>>> sentiment.sentiment( term_list )
{'arousal': 4.939546556471719, 'valence': 5.0307617694606375}
>>>
>>> term_list = [ 'brocolli', 'carrot', 'pea' ]
>>> sentiment.exist( term_list )
[False, False, False]
>>> sentiment.sentiment( term_list )
{'arousal': 0.0, 'valence': 0.0 }
>>> sentiment.describe( 'interesting' )
'moderately happy'
>>> sentiment.describe( 'pensive' )
'unknown'
>>> sentiment.describe( [ 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog' ] )
'moderately happy'
>>> sentiment.describe( 1, 9 )
'very nervous'
</code>

    </div>

    <p>It is also possible to add custom words to the dictionary
    using <code>add_term()</code>, as long as you can provide a
    reasonable valence and arousal for the word. Both the word and its
    stem will then be available.</p>

    <ul>

    <li><code>add_term( term, v, a [, replace] ):</code><br>
    Adds <code>term</code> to the sentiment dictionary, assigning it a
    valence of <code>v</code> and an arousal of <code>a</code>. Terms
    already in the dictionary will not be modified unless the
    optional <code>replace</code> argument is provided with a value
    of <code>True</code>.

    </ul>

    <p>Below are some examples of adding and replacing terms in the
    default sentiment dictionary.</p>

    <div class="code-div" style="background-color: #f8f8f8;">

<code style="white-space: pre-wrap;">>>>from sentiment_module import sentiment
>>> sentiment.exist( 'stunned' )
False
>>> sentiment.add_term( 'stunned', 2.0, 6.0 )
>>> sentiment.exist( 'stunned' )
True
>>> sentiment.exist( 'stun' )
True
>>> sentiment.sentiment( 'stunned' )
{'arousal': 2.0, 'valence': 6.0}
>>>
>>> sentiment.sentiment( 'happy' )
{'arousal': 6.49, 'valence': 8.21}
>>> sentiment.add_term( 'happy', 6.0, 8.0 )
>>> sentiment.sentiment( 'happy' )
{'arousal': 6.49, 'valence': 8.21}
>>> sentiment.add_term( 'happy', 6.0, 8.0, True )
>>> sentiment.sentiment( 'happy' )
{'arousal': 6.0, 'valence': 8.0}
</code>

    </div>
  </div>
</div>


<h2 id="viz">Visualization</h2>

<p>
The simplest (and most common) way to visualize text is to display it
directly to a user. This is useful, since it reveals the full detail
of a document. It also has drawbacks, however. Documents take time to
read. For a few documents, it might be possible to analyze them by
reading them. For larger document collections, however, reading every
document in its entirety is not feasible. Instead, some method is
needed to present higher-level overviews of the documents and their
relationships to one another, for example, summaries, topic clusters,
or geolocations.
</p>


<h3>Tweet Visualization</h3>


<p>
As a practical example of a number of visualization techniques,
consider <a target="_blank"
href="http://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/">an
ongoing project to analyze
<em>tweets</em></a> posted on <a target="_blank"
href="https://twitter.com">Twitter</a>, an online social network that
allows users to upload short text messages&mdash;tweets&mdash;of up to
140 characters. This restriction encourages users to construct
focused, timely updates. <a target="_blank"
href="http://abcnews.go.com/Business/twitter-ipo-filing-reveals-500-million-tweets-day/story?id=20460493">Twitter
reported in its IPO filing</a> users were posting an average
of <a target="_blank"
href="http://www.internetlivestats.com/twitter-statistics/">500
million tweets per day</a></a>. Tweets are now
being <a target="_blank"
href="http://www.loc.gov/today/pr/2010/10-081.html">archived at the
U.S. Library of Congress</a>. Twitter has also shown the potential for
societal impact, for example, in its use as <a target="_blank"
href="http://www.thewire.com/global/2011/01/the-twitter-revolution-debate-the-egyptian-test-case/21296/">a
communication and organizing tool for activists during the 2011
&quot;Arab Spring&quot; protests</a> in various Middle Eastern
countries.
</p>


<table class="center">
<caption>
  Examples of the visual features assigned to a circle to represent
  its tweet's estimated sentiment: colour&mdash;blue for unpleasant,
  green for pleasant; brightness&mdash;brighter for more aroused; size
  and transparency&mdash;larger and more opaque for more confidence in
  the sentiment estimate
</caption>
<tr>
  <td style="text-align: center; border: 0px; padding-bottom: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/tweet-viz-ex.png">
  </td>
</tr>
</table>


<p>
Collections of tweets are visualized in numerous ways: by sentiment,
by topic, by frequent terms, and so on. Individual tweets are drawn as
circles. Each circle's colour, brightness, size, and transparency
visualize different details about the sentiment of its tweet:
</p>

<ul>

  <li> <b>Colour.</b> The overall valence or pleasure of the tweet:
  pleasant tweets are green, and unpleasant tweets are blue.

  <li> <b>Brightness.</b> The overall arousal of the tweet: active
  tweets are brighter, and subdued tweets are darker.

  <li> <b>Size.</b> One measure of how confident we are about the
  estimate of the tweet's sentiment: larger tweets represent more
  confident estimates.

  <li> <b>Transparency.</b> A second measure of how confident we are
  about its estimate of the tweet's emotion: more opaque (i.e. less
  transparent) tweets represent more confident estimates.

</ul>

<p>
Tweets are presented using several different visualization
techniques. Each technique is designed to highlight different aspects
of the tweets and their sentiment.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Sentiment</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/sentiment-tab.png">
  </td>
</tr>
</table>


<p>
The estimated sentiment of each tweet defines its position in an
emotional scatterplot with pleasure and arousal on its horizontal and
vertical axes. The spatial distribution of the tweets summarizes their
overall sentiment.
</p>

<p>
Details are presented on demand. If the user hovers the mouse cursor
over a tweet, it reveals its body. Words in the sentiment dictionary
are highlighted in bold italics. Clicking on a tweet generates a
detail dialog with the overall pleasure and arousal for the tweet, as
well as each sentiment term's mean and standard deviation of pleasure,
mean and standard deviation of arousal, and frequency.
<p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Topics</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/topics-tab.png">
  </td>
</tr>
</table>


<p>
Text similarity and MST clustering are used to identify tweets that
discuss a common topic or theme. Each topic is visualized as a
rectangular group of tweets, with keywords at the top to summarize the
topic, and a number at the bottom to identify the number of tweets in
the cluster.
</p>

<p>
Tweets within each cluster are laid out so that the distance
between them shows their text similarity: closer for stronger
similarity. Topic cluster rectangles are positioned in the same way:
closer for more similar topics. Tweets that are not part of any topic
are visualized as singletons on the right.
</p>

<p>
As with the sentiment, details are available on demand by hovering the
mouse over a tweet or clicking a tweet to reveal its content and its
estimated sentiment.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Heatmap Tab</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/heatmap-tab.png">
  </td>
</tr>
</table>


<p>
A heatmap visualizes the a discretized distribution of elements in a
2D plot. Here, we use a sentiment histogram to highlight "hot" red
regions with many tweets, and "cold" blue regions with only a few
tweets.
</p>

<p>
The emotional scatterplot is subdivided into an
8&thinsp;&times;&thinsp;8 grid of bins representing one-unit steps in
pleasure and arousal. The number of tweets falling within each bin is
counted and visualized using colour: red for bins with more tweets
than average, and blue for bins with fewer tweets than average. White
bins contain no tweets. Stronger, more saturated colours lie farther
from the average.
</p>

<p>
Hovering the mouse over a heatmap bin reveals the number of tweets
that lie in the bin.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Tag Cloud</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/tag-cloud-tab.png">
  </td>
</tr>
</table>


<p>
A tag cloud visualizes a collection of text documents as a set of
frequent terms, where the size of a term represents the number of
times is occurs in the document set.
</p>

<p>
Tweets are visualized as four separate tag clouds in four emotional
regions: upset in the upper-left, happy in the upper-right, relaxed in
the lower-right, and unhappy in the lower-left. A term's size shows
how often it occurs over all the tweets in the given emotional
region. Larger terms occur more frequently.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Timeline</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/timeline-tab.png">
  </td>
</tr>
</table>


<p>
A timeline visualizes the number of tweets that occur over a given
time window using a double-ended bar graph. Pleasant tweets are shown
in green above the horizontal axis, and unpleasant tweets in blue
below the axis.</p>

<p>
The height of a bar in the graph shows the number of tweets posted
over the time range covered by the bar. Bars are split into four
segments representing the number of relaxed and happy tweets&mdash;in
dark green and light green&mdash;and the number of unhappy and upset
tweets&mdash;in dark blue and light blue.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Map</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/map-tab.png">
  </td>
</tr>
</table>

<p>
Maps are used to geolocate data. Tweets are visualized at the
latitude and longitude where they were posted. We use the same
sized, coloured circles from the sentiment and topic visualizations
to show estimated sentiment and confidence in the estimate.
</p>

<p>
Twitter presents an interesting problem for geolocation. Because it
implements an "opt-in" system for reporting location, users must
explicitly choose to allow their location to be posted before their
tweets are geotagged. Most users have not done this, so only a very
few tweets contain location data. The label in the upper-right corner
of the map is modified to show the total number of <i>geotagged</i>
tweets in parentheses, to highlight this difference relative to the
other visualizations.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Affinity</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/affinity-tab.png">
  </td>
</tr>
</table>


<p>
An affinity graph visualizes relationships between text elements. The
basis for a relationship depends on the type of data being
visualized. For tweet data, the affinity graph includes frequent
tweets, people, hashtags, and URLs, together with relationships or
affinities between these elements.
</p>

<p>
As before, blue and green nodes represent tweets. Orange nodes
represent people, yellow nodes represent hashtags, and red nodes
represent URLs. Larger nodes show more frequent elements. Links
between nodes highlight relationships, for example, tweets that are
similar to one another, or hashtags and people that occur in a set of
tweets.
</p>


<h4 style="margin-top: 1em; margin-bottom: 1em;">Tweets</h4>


<table class="center">
<tr>
  <td style="text-align: center; border: 0px; padding: 0px;">
    <img style="border: 0; width: 100%; max-width: 1097px;"
      src="fig/tweets-tab.png">
  </td>
</tr>
</table>


<p>
Even with sophisticated visualization techniques available, it is
often important to show the raw text in a document. A common analysis
strategy is to use visualizations to filter a large document
collection into a small subset of documents that are of particular
interest to an analyst. Those documents can then be read to reveal
specific details that cannot be easily captured in the visualizations.
</p>

<p>
For tweets, we show the date, author, and body of each tweet, as well
as its overall pleasure <em>v</em> and arousal <em>a</em>. Sentiment
terms in each tweet are highlighted in bold italics. This allows a
viewer to see both the raw text of the tweet, and the estimated
sentiment values we derived.</p>


<h3>Practice Problem 4</h3>

<p>Take the following except from John Steinbeck's famous novel, <i>Of
Mice and Men</i>.</p>

<div class="code-div">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  Two men, dressed in denim jackets and trousers and wearing "black,
  shapeless hats," walk single-file down a path near the pool. Both
  men carry blanket rolls — called bindles — on their shoulders. The
  smaller, wiry man is George Milton. Behind him is Lennie Small, a
  huge man with large eyes and sloping shoulders, walking at a gait
  that makes him resemble a huge bear.<br><br>

  When Lennie drops near the pool's edge and begins to drink like a
  hungry animal, George cautions him that the water may not be
  good. This advice is necessary because Lennie is retarded and
  doesn't realize the possible dangers. The two are on their way to a
  ranch where they can get temporary work, and George warns Lennie not
  to say anything when they arrive. Because Lennie forgets things very
  quickly, George must make him repeat even the simplest instructions.
  <br><br>

  Lennie also likes to pet soft things. In his pocket, he has a dead
  mouse which George confiscates and throws into the weeds beyond the
  pond. Lennie retrieves the dead mouse, and George once again catches
  him and gives Lennie a lecture about the trouble he causes when he
  wants to pet soft things (they were run out of the last town because
  Lennie touched a girl's soft dress, and she screamed). Lennie offers
  to leave and go live in a cave, causing George to soften his
  complaint and tell Lennie perhaps they can get him a puppy that can
  withstand Lennie's petting.<br><br>

  As they get ready to eat and sleep for the night, Lennie asks George
  to repeat their dream of having their own ranch where Lennie will be
  able to tend rabbits. George does so and then warns Lennie that, if
  anything bad happens, Lennie is to come back to this spot and hide
  in the brush. Before George falls asleep, Lennie tells him they must
  have many rabbits of various colors.
  </code>
  </div>
</div>

<p>Convert this text into sentences, then estimate the sentiment of
each sentence. You can use the <code>vaderSentiment</code> (Valence
Aware Dictionary and sEntiment Reasoner) package to perform
sentence-level sentiment analysis. <code>vaderSentiment</code> is
included in <code>nltk</code>, so you already have access to this
library. To use vader's sentiment capabilities,
import <code>nltk</code> and load vader's sentiment lexicon, import
the sentiment analyzer, and
<a href="https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f"
target="_blank">use it to begin analyzing sentences</a>. For more
details and documentation, read
the <a href="https://github.com/cjhutto/vaderSentiment"
target="_blank">GitHub repository's <code>README.rst</code></a>.</p>

<div class="code-div">
  <div style="margin-left: 1em; margin-bottom: 0.75em;">
  <code>
  >>> import nltk<br>
  >>> nltk.download( 'vader_lexicon' )<br><br>
  >>> from nltk.sentiment.vader import SentimentIntensityAnalyzer<br><br>

  >>> sentiment = SentimentIntensityAnalyzer()<br>
  >>> sentence = 'This phone is super cool!!!'<br>
  >>> score = sentiment.polarity_scores( sentence )<br>
  >>> print( score )<br>
  {'neg': 0.0, 'neu': 0.298, 'pos': 0.702, 'compound': 0.795}
  </code>
  </div>
</div>

<p><code>vader</code> scores each sentence in terms of its negative,
neutral, and positive components, along with a compound aggregate
score for the entire sentence. The compound score is reported on the
range [-1 &hellip; 1], representing most negative at -1 to most postive at
1.</p>

<p>Once you have sentence-level sentiment, you need to consider two
additional issues. First, how should you aggregate the senence-level
scores into a single, final score for the <i>Of Mice and Men</i>
excerpt? Second, do you want to report (and use) the compound score
for each sentence, or some combination of the scores returned? There
is no "right answer" to either question, but what you choose will
affect the results you report.</p>

<div id="prac-prob-4" class="detail">
  <h3>Practice Problem 4 Solution</h3>
  <div style="background: none; background-color: #e7eefb;">
    <p>The following snippets of Python code will use nltk's VADER
    sentiment package to report the sentiment of each sentence in
    the <i>Of Mice and Men</i> snippet.</p>
    
    <div class="code-div" style="background-color: #f8f8f8;">
      <table style="margin: 0px auto 0px auto; width: 100%;">
      <tr>
      <td style="padding: 0px 8px 0px 0px; border: 0px solid black;">
        <span title="copy code" id="code-09-img" class="code-div-img"></span>
        <div id="code-09">
        >>> import re<br>
        >>> import nltk<br>
        >>> nltk.download( 'vader_lexicon' )<br>
        >>> from nltk.sentiment.vader import SentimentIntensityAnalyzer<br>
        >>><br>
        >>> txt = 'Two men, dressed in denim jackets and trousers and wearing "black, shapeless hats," walk single-file down a path near the pool. Both men carry blanket rolls — called bindles — on their shoulders. The smaller, wiry man is George Milton. Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear. When Lennie drops near the pool\'s edge and begins to drink like a hungry animal, George cautions him that the water may not be good. This advice is necessary because Lennie is retarded and doesn\'t realize the possible dangers. The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive. Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions. Lennie also likes to pet soft things. In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond. Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl\'s soft dress, and she screamed). Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie\'s petting. As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits. George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush. Before George falls asleep, Lennie tells him they must have many rabbits of various colors.'<br>
        >>> <br>
        >>> #  Convert to sentences, create VADER sentiment analyzer<br>
        >>><br>
        >>> sentence = txt.split( '.' )<br>
        >>> sentiment = SentimentIntensityAnalyzer()<br>
        >>><br>
        >>> for i in range( 0, len( sentence ) ):<br>
        ...<br>
        ... <span class="tab-1"># Print sentence's compound sentiment score</span><br>
        ...<br>
        ... <span class="tab-1">score = sentiment.polarity_scores( sentence[ i ] )</span><br>
        ... <span class="tab-1">print( sentence[ i ] )</span><br>
        ... <span class="tab-1">print( 'Sentiment:', score[ 'compound' ] )</span><br>
        </div>
      </td>
      </tr>
      </table>
    </div>

  </div>
</div>


<!-- The mod-date span will be updated by code in mod-date.js -->

<hr class="fig_top">
<div class="footer">
  Updated <span id="mod-date">01-Jan-01</span>
</div>

</body>
</html>

<!--  LocalWords:  ui Analytics Healey preprocessing analytics i'd VC
 -->
<!--  LocalWords:  i'll i'm i've Lovins Lovins's ccc vvv Def'n ee ou
 -->
<!--  LocalWords:  oa orrery rr EMENT REPLAC PLAC CVC boolean SSES TF
 -->
<!--  LocalWords:  IES PONI EED ATIONAL ATOR OUSLI OUS ANALOGOUSLI tf
 -->
<!--  LocalWords:  PROBAT ishmael befor happi IDF idf LSA SVD juliet
 -->
<!--  LocalWords:  eigenvectors hampshire Foltz Laham Landauer ln nm
 -->
<!--  LocalWords:  Dumais Furnas Deerwester Harshman neighbour fff px
 -->
<!--  LocalWords:  agglomerative orthonormal Kruskal's labelled TFIDF
 -->
<!--  LocalWords:  subgraphs affective cimcumplex circumplex Tellegen
 -->
<!--  LocalWords:  Thayer Diener NLP Summarization nd ACL SVM bigrams
 -->
<!--  LocalWords:  unigrams accuracies Turney th pointwise PMI Cai fq
 -->
<!--  LocalWords:  SentiStrength Thelwall Paltoglou WordNet synsets
 -->
<!--  LocalWords:  SentiWordNet Baccianella Esuli Sebastiani LREC HCP
 -->
<!--  LocalWords:  synset ANEW's publically frac unweighted textstyle
 -->
<!--  LocalWords:  IPO colour scatterplot Heatmap heatmap discretized
 -->
<!--  LocalWords:  colours Timeline timeline geolocate coloured ot np
 -->
<!--  LocalWords:  geolocation geotagged hashtags dataset datasets DT
 -->
<!--  LocalWords:  Polysemy Hyponym hyponym Ngram wildcards UIUC Xie
 -->
<!--  LocalWords:  hypernym meronym holonym troponym WordNet's Fayad
 -->
<!--  LocalWords:  geolocations Grobelnik Mladenic Moodle modelling
 -->
<!--  LocalWords:  sqrt Gensim API NLTK tokenize IPython Norvig POS
 -->
<!--  LocalWords:  Mayzner Norvig's meronyms PageRank iteratively plt
 -->
<!--  LocalWords:  Bagley Villanes Scikit scikit bigram Treebank txt
 -->
<!--  LocalWords:  pyplot Jupyter matplotlib numpy bindles Lennie len
 -->
<!--  LocalWords:  doesn Args punc arange figsize ylabel xlabel loc
 -->
<!--  LocalWords:  xticks tok pos barh yticks nltk elif FW JJ JJR JJS
 -->
<!--  LocalWords:  NN NNS NNP NNPS PRP RBR RBS SYM VBD VBG VBN VBP wh
 -->
<!--  LocalWords:  VBZ WDT WRB sklearn summarization stopwords Brien
 -->
<!--  LocalWords:  telescreen thoughtcrime english Haruki Murakami's
 -->
<!--  LocalWords:  Yasunari Natsume Soseki's Kokoro Kazuo Ishiguro's
 -->
